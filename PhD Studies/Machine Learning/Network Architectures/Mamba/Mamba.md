---
aliases:
- Mamba
- Transformer
- Foundation Models
- State Space Model
---

>[!Links]-
>URL: http://arxiv.org/abs/2312.00752
>PDF: [PDF](gu2023.pdf)
>Zotero: [Zotero-Link](zotero://select/items/@gu2023)

>[!ABSTRACT]-
>Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.

>[!terminology]-
>
>>[!def]- Definition Foundation Models (FM)
>> *Foundation models* are large pre-trained models, which are fine-tuned for downstream tasks. A downstream task is a subset task for the general task a pre-trained model was trained on.
>> ^deffoundationmodel
>
>> [!def]- Definition General State Space Model
>> A general state space model is defined on the processing of sequential data. They are described as a evolution of state parameters over the input of the sequential data. They can be summarized by having
>> - **State/Hidden State** representing some latent representation of the input processing
>> - **Observation** represents some measurement our output $y_t$ at some time sequential time $t$
>> - **State Transition Equation** describing the processing of progressing through states with respect to sequential time $t$, i.e. $x_t = f(x_{t-1}, \Theta_t)$ for some model parameters $\Theta_t$ at sequential time $t$.
>> - **Observation Equation** describing how the output is calculated, i.e. $y_t = h(x_t, \Theta_t)$ for some model parameters $\Theta_t$ at sequential time $t$
>> - **Initial State** representing the hidden state without it being influenced by any input
>>
>> Examples for these general state space models are recurrent neural networks, hidden Markov models, Kalman filters, linear dynamical systems etc.
>>
>> See [Pattern Recognition and Machine Learning by Bishop](../../../../Sources/bishop2006.pdf) page 605 ff.

>[!observation]
>Transformers are mostly used for creating complex sequence based models. They mostly utilize some *attention mechanic (self-attention)*. Attention models suffer from 
>- *finite sequence windows* over which attention is performed
>- quadratic time and space complexity $\mathcal{O}(n^2)$ if $n$ is the sequence length over the sequence window

>[!def] Definition Structured State Space Models (S4)
> Let 
> - $x$ denote a **continuous input** $x(t) \in \mathbb{R}$ *(continuous)*
> - $y$ denote a **continuous output** $y(t) \in \mathbb{R}$ *(continuous)*
> - $h$ denote a**continuous hidden state** $h(t) \in \mathbb{R}$ *(continuous)*
> - $({x}_t)_{t=1}^N$ denote an arbitrary **input sequence** *(discrete)*
> - $({y}_t)_{t=1}^N$ denote the **output sequence** generated by the input sequence $(x_t)_{t=1}^N$ *(discrete)*
> - $({h}_t)_{t=1}^N$ denote the **hidden state sequence** generated by the input sequence $(x_t)_{t=1}^N$ *(discrete)*
> - $t$ denote a **specific point of time** in the sequence, i.e. $\mathbf{x}_t$, or of the function, i.e. $x(t)$. 
> - matrices *without an overline* denote **continuous parameter**, i.e. $\mathbf{A}, \mathbf{B}, \mathbf{C}, \mathbf{K}$
> - matrices *with an overline* denote **discretized parameter**, i.e. $\overline{\mathbf{A}},\overline{\mathbf{B}},\overline{\mathbf{C}},\overline{\mathbf{K}}$
> 
> We define the $4$-tuple $(\Delta, \mathbf{A}, \mathbf{B}, \mathbf{C})$ sequence-to-sequence transformation with
> $$ \begin{align} 
> 	h(t)' &= \mathbf{A}h(t) + \mathbf{B}x(t) \tag{Continuous}\\
> 	y(t) &= \mathbf{C}h(t)  \tag{Continuous}\\
> 	h_t &= \overline{\mathbf{A}}h_{t-1} + \overline{\mathbf{B}}x_t \tag{Discrete}\\
> 	y_t &= \mathbf{C}h_t \tag{Discrete} \\
> 	\overline{\mathbf{K}} &= (C\overline{\mathbf{B}}, C\overline{\mathbf{AB}}, \ldots, C\overline{\mathbf{A}}^k\overline{\mathbf{B}}, \ldots) \\
> 	y&= x * \overline{K}
> \end{align}$$
>>[!pic]- Figure 
>>![[figures/selective_state_space_model.png|900]]

>[!note]
>The continuous

>[!def] Definition Discretization
> Let $(f_A, f_B)$ be discretization rules, such that continuous parameters $A, B$ are mapped to their discrete counterparts such that,
> $$ \begin{align}
> 	\overline{\mathbf{A}} &= f_A(\Delta, \mathbf{A}) \\
> 	\overline{\mathbf{B}} &= f_B(\Delta, \mathbf{A}, \mathbf{B})
> 	\end{align}
> $$

---
title: Quantization based Fast Inner Product Search
authors: Ruiqi Guo, Sanjiv Kumar, Krzysztof Choromanski, David Simcha
year: 2015
DOI: 10.48550/arXiv.1509.01469
aliases:
  - QIPS Paper
---

>[!Links]-
>URL: http://arxiv.org/abs/1509.01469
>PDF: [PDF](../PDFs/guo2015.pdf)
>Zotero: [Zotero-Link](zotero://select/items/@guo2015)

>[!ABSTRACT]-
>We propose a quantization based approach for fast approximate Maximum Inner Product Search ([[../PhD Studies/Transformer Vector Quantization/1 - Maximum Inner Product Search|MIPS]]). Each database vector is quantized in multiple subspaces via a set of codebooks, learned directly by minimizing the inner product quantization error. Then, the inner product of a query to a database vector is approximated as the sum of [[../PhD Studies/Mathematic Basics/Linear Algebra/Inner Product/Inner Products|inner products]] with the subspace quantizers. Different from recently proposed LSH approaches to [[../PhD Studies/Transformer Vector Quantization/1 - Maximum Inner Product Search|MIPS]], the database vectors and queries do not need to be augmented in a higher dimensional feature space. We also provide a theoretical analysis of the proposed approach, consisting of the concentration results under mild assumptions. Furthermore, if a small sample of example queries is given at the training time, we propose a modified codebook learning procedure which further improves the accuracy. Experimental results on a variety of datasets including those arising from deep neural networks show that the proposed approach significantly outperforms the existing state-of-the-art.


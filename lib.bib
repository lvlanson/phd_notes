@article{albertsson2018,
  title = {Machine {{Learning}} in {{High Energy Physics Community White Paper}}},
  author = {Albertsson, Kim and Altoe, Piero and Anderson, Dustin and Andrews, Michael and Espinosa, Juan Pedro Araque and Aurisano, Adam and Basara, Laurent and Bevan, Adrian and Bhimji, Wahid and Bonacorsi, Daniele and Calafiura, Paolo and Campanelli, Mario and Capps, Louis and Carminati, Federico and Carrazza, Stefano and Childers, Taylor and Coniavitis, Elias and Cranmer, Kyle and David, Claire and Davis, Douglas and Duarte, Javier and Erdmann, Martin and Eschle, Jonas and Farbin, Amir and Feickert, Matthew and Castro, Nuno Filipe and Fitzpatrick, Conor and Floris, Michele and Forti, Alessandra and Garra-Tico, Jordi and Gemmler, Jochen and Girone, Maria and Glaysher, Paul and Gleyzer, Sergei and Gligorov, Vladimir and Golling, Tobias and Graw, Jonas and Gray, Lindsey and Greenwood, Dick and Hacker, Thomas and Harvey, John and Hegner, Benedikt and Heinrich, Lukas and Hooberman, Ben and Junggeburth, Johannes and Kagan, Michael and Kane, Meghan and Kanishchev, Konstantin and Karpiński, Przemysław and Kassabov, Zahari and Kaul, Gaurav and Kcira, Dorian and Keck, Thomas and Klimentov, Alexei and Kowalkowski, Jim and Kreczko, Luke and Kurepin, Alexander and Kutschke, Rob and Kuznetsov, Valentin and Köhler, Nicolas and Lakomov, Igor and Lannon, Kevin and Lassnig, Mario and Limosani, Antonio and Louppe, Gilles and Mangu, Aashrita and Mato, Pere and Meinhard, Helge and Menasce, Dario and Moneta, Lorenzo and Moortgat, Seth and Narain, Meenakshi and Neubauer, Mark and Newman, Harvey and Pabst, Hans and Paganini, Michela and Paulini, Manfred and Perdue, Gabriel and Perez, Uzziel and Picazio, Attilio and Pivarski, Jim and Prosper, Harrison and Psihas, Fernanda and Radovic, Alexander and Reece, Ryan and Rinkevicius, Aurelius and Rodrigues, Eduardo and Rorie, Jamal and Rousseau, David and Sauers, Aaron and Schramm, Steven and Schwartzman, Ariel and Severini, Horst and Seyfert, Paul and Siroky, Filip and Skazytkin, Konstantin and Sokoloff, Mike and Stewart, Graeme and Stienen, Bob and Stockdale, Ian and Strong, Giles and Thais, Savannah and Tomko, Karen and Upfal, Eli and Usai, Emanuele and Ustyuzhanin, Andrey and Vala, Martin and Vallecorsa, Sofia and Vasel, Justin and Verzetti, Mauro and Vilasís-Cardona, Xavier and Vlimant, Jean-Roch and Vukotic, Ilija and Wang, Sean-Jiun and Watts, Gordon and Williams, Michael and Wu, Wenjing and Wunsch, Stefan and Zapata, Omar},
  date = {2018-09},
  journaltitle = {J. Phys.: Conf. Ser.},
  volume = {1085},
  number = {2},
  pages = {022008},
  publisher = {IOP Publishing},
  issn = {1742-6596},
  doi = {10.1088/1742-6596/1085/2/022008},
  url = {https://dx.doi.org/10.1088/1742-6596/1085/2/022008},
  urldate = {2024-01-05},
  abstract = {Machine learning is an important applied research area in particle physics, beginning with applications to high-level physics analysis in the 1990s and 2000s, followed by an explosion of applications in particle and event identification and reconstruction in the 2010s. In this document we discuss promising future research and development areas in machine learning in particle physics with a roadmap for their implementation, software and hardware resource requirements, collaborative initiatives with the data science community, academia and industry, and training the particle physics community in data science. The main objective of the document is to connect and motivate these areas of research and development with the physics drivers of the High-Luminosity Large Hadron Collider and future neutrino experiments and identify the resource needs for their implementation. Additionally we identify areas where collaboration with external communities will be of great benefit.},
  langid = {english},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/albertsson2018.pdf}
}

@book{apostol1967,
  title = {Calculus, Vol. 2: {{Multi-variable}} Calculus and Linear Algebra with Applications to Differential Equations and Probability},
  author = {Apostol, Tom M.},
  date = {1967},
  publisher = {J. Wiley},
  location = {New York},
  isbn = {0-471-00005-1 978-0-471-00005-1 0-471-00007-8 978-0-471-00007-5},
  keywords = {calculus textbook}
}

@online{ben-baruch2021,
  title = {Asymmetric {{Loss For Multi-Label Classification}}},
  author = {Ben-Baruch, Emanuel and Ridnik, Tal and Zamir, Nadav and Noy, Asaf and Friedman, Itamar and Protter, Matan and Zelnik-Manor, Lihi},
  date = {2021-07-29},
  eprint = {2009.14119},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2009.14119},
  url = {http://arxiv.org/abs/2009.14119},
  urldate = {2023-12-27},
  abstract = {In a typical multi-label setting, a picture contains on average few positive labels, and many negative ones. This positive-negative imbalance dominates the optimization process, and can lead to under-emphasizing gradients from positive labels during training, resulting in poor accuracy. In this paper, we introduce a novel asymmetric loss ("ASL"), which operates differently on positive and negative samples. The loss enables to dynamically down-weights and hard-thresholds easy negative samples, while also discarding possibly mislabeled samples. We demonstrate how ASL can balance the probabilities of different samples, and how this balancing is translated to better mAP scores. With ASL, we reach state-of-the-art results on multiple popular multi-label datasets: MS-COCO, Pascal-VOC, NUS-WIDE and Open Images. We also demonstrate ASL applicability for other tasks, such as single-label classification and object detection. ASL is effective, easy to implement, and does not increase the training time or complexity. Implementation is available at: https://github.com/Alibaba-MIIL/ASL.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,I.0,I.2.10,I.2.6,I.4.0},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/ben-baruch2021.pdf;/home/thomas/Zotero/storage/QGNK38MC/2009.html}
}

@book{bishop2006,
  title = {Pattern {{Recognition}} and {{Machine Learning}}},
  author = {Bishop, Christopher M.},
  date = {2006-08-17},
  eprint = {kTNoQgAACAAJ},
  eprinttype = {googlebooks},
  publisher = {Springer},
  abstract = {This is the first textbook on pattern recognition to present the Bayesian viewpoint. The book presents approximate inference algorithms that permit fast approximate answers in situations where exact answers are not feasible. It uses graphical models to describe probability distributions when no other books apply graphical models to machine learning. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory.},
  isbn = {978-0-387-31073-2},
  langid = {english},
  pagetotal = {738},
  keywords = {Computers / Computer Graphics,Computers / Computer Vision & Pattern Recognition,Computers / Intelligence (AI) & Semantics,Mathematics / Probability & Statistics / General}
}

@book{briggs2019,
  title = {Calculus},
  author = {Briggs, William L. and Cochran, Lyle and Gillett, Bernard and Schulz, Eric P.},
  date = {2019},
  eprint = {DNUGtAEACAAJ},
  eprinttype = {googlebooks},
  publisher = {Pearson},
  abstract = {For 3- to 4-semester courses covering single-variable and multivariable calculus, taken by students of mathematics, engineering, natural sciences, or economics.  T he most successful new calculus text in the last two decades  The much-anticipated 3rd Edition of Briggs' Calculus Series  retains its hallmark features while introducing important advances and refinements. Briggs, Cochran, Gillett, and Schulz build from a foundation of meticulously crafted exercise sets, then draw students into the narrative through writing that reflects the voice of the instructor. Examples are stepped out and thoughtfully annotated, and figures are designed to teach rather than simply supplement the narrative. The groundbreaking eBook contains approximately 700 Interactive Figures that can be manipulated to shed light on key concepts.  For the 3rd Edition, the authors synthesized feedback on the text and MyLab(TM) Math content from over 140 instructors and an Engineering Review Panel. This thorough and extensive review process, paired with the authors' own teaching experiences, helped create a text that was designed for today's calculus instructors and students.  Also available with MyLab Math  MyLab Math is the teaching and learning platform that empowers instructors to reach every student. By combining trusted author content with digital tools and a flexible platform, MyLab Math personalizes the learning experience and improves results for each student.    Note: You are purchasing a standalone product; MyLab Math does not come packaged with this content. Students, if interested in purchasing this title with MyLab Math, ask your instructor to confirm the correct package ISBN and Course ID. Instructors, contact your Pearson representative for more information.    If you would like to purchase both the physical text and MyLab Math, search for:  0134996720 / 9780134996721 Calculus and MyLab Math with Pearson eText - Title-Specific Access Card Package, 3/e Package consists of:  013476563X / 9780134765631 Calculus 013485683X / 9780134856834 MyLab Math with Pearson eText - Standalone Access Card - for Calculus},
  isbn = {978-0-13-476563-1},
  langid = {english},
  pagetotal = {1344}
}

@book{brogan1991,
  title = {Modern Control Theory (3rd Ed.)},
  author = {Brogan, William L.},
  date = {1991-02},
  publisher = {Prentice-Hall, Inc.},
  location = {USA},
  isbn = {978-0-13-589763-8},
  pagetotal = {653}
}

@book{brualdi2004,
  title = {Introductory {{Combinatorics}}},
  author = {Brualdi, Richard A.},
  date = {2004-01-01},
  edition = {Subsequent edition},
  publisher = {Pearson College Div},
  location = {Upper Saddle River, N.J},
  abstract = {This book emphasizes combinatorial ideas including the pigeon-hole principle, counting techniques, permutations and combinations, Pólya counting, binomial coefficients, inclusion-exclusion principle, generating functions and recurrence relations, and combinatortial structures (matchings, designs, graphs). The volume provides a complete examination of combinatorial ideas and techniques. For individuals interested in combinatorial concepts.},
  isbn = {978-0-13-100119-0},
  langid = {english},
  pagetotal = {608}
}

@online{conci2018,
  title = {Distance {{Between Sets}} - {{A}} Survey},
  author = {Conci, A. and Kubrusly, C. S.},
  date = {2018-08-07},
  eprint = {1808.02574},
  eprinttype = {arxiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.1808.02574},
  url = {http://arxiv.org/abs/1808.02574},
  urldate = {2024-02-21},
  abstract = {The purpose of this paper is to give a survey on the notions of distance between subsets either of a metric space or of a measure space, including definitions, a classification, and a discussion of the best-known distance functions, which is followed by a review on applications used in many areas of knowledge, ranging from theoretical to practical applications.},
  pubstate = {preprint},
  keywords = {28A78 54E35,Mathematics - Functional Analysis},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/conci2018.pdf;/home/thomas/Zotero/storage/TTV8T363/1808.html}
}

@online{devlin2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  date = {2019-05-24},
  eprint = {1810.04805},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1810.04805},
  url = {http://arxiv.org/abs/1810.04805},
  urldate = {2023-12-05},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  pubstate = {preprint},
  version = {2},
  keywords = {Computer Science - Computation and Language},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/devlin2019.pdf;/home/thomas/Zotero/storage/KEMTIW53/1810.html}
}

@book{folland2009,
  title = {Fourier {{Analysis}} and {{Its Applications}}},
  author = {Folland, Gerald B.},
  date = {2009-01-13},
  publisher = {American Mathematical Society},
  location = {Providence, RI},
  abstract = {This book presents the theory and applications of Fourier series and integrals, eigenfunction expansions, and related topics, on a level suitable for advanced undergraduates. It includes material on Bessel functions, orthogonal polynomials, and Laplace transforms, and it concludes with chapters on generalized functions and Green's functions for ordinary and partial differential equations. The book deals almost exclusively with aspects of these subjects that are useful in physics and engineering, and includes a wide variety of applications. On the theoretical side, it uses ideas from modern analysis to develop the concepts and reasoning behind the techniques without getting bogged down in the technicalities of rigorous proofs.},
  isbn = {978-0-8218-4790-9},
  langid = {english},
  pagetotal = {433}
}

@article{ge2014,
  title = {Optimized {{Product Quantization}}},
  author = {Ge, Tiezheng and He, Kaiming and Ke, Qifa and Sun, Jian},
  date = {2014-04},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {36},
  number = {4},
  pages = {744--755},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2013.240},
  url = {https://ieeexplore.ieee.org/document/6678503},
  urldate = {2023-11-13},
  abstract = {Product quantization (PQ) is an effective vector quantization method. A product quantizer can generate an exponentially large codebook at very low memory/time cost. The essence of PQ is to decompose the high-dimensional vector space into the Cartesian product of subspaces and then quantize these subspaces separately. The optimal space decomposition is important for the PQ performance, but still remains an unaddressed issue. In this paper, we optimize PQ by minimizing quantization distortions w.r.t the space decomposition and the quantization codebooks. We present two novel solutions to this challenging optimization problem. The first solution iteratively solves two simpler sub-problems. The second solution is based on a Gaussian assumption and provides theoretical analysis of the optimality. We evaluate our optimized product quantizers in three applications: (i) compact encoding for exhaustive ranking [1], (ii) building inverted multi-indexing for non-exhaustive search [2], and (iii) compacting image representations for image retrieval [3]. In all applications our optimized product quantizers outperform existing solutions.},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/ge2014.pdf;/home/thomas/Zotero/storage/3HGULA2E/6678503.html}
}

@online{greene2019,
  title = {Econometric {{Analysis}}},
  author = {Greene, William},
  date = {2019},
  publisher = {Pearson Deutschland},
  url = {https://elibrary.pearson.de/book/99.150005/9781292231150},
  urldate = {2023-12-23},
  abstract = {{$<$}br{$><$}h4{$>$}For first-year graduate courses in Econometrics for Social Scientists.{$<$}/h4{$>$} {$<$}br{$><$}p{$>$}Bridging the gap between social science studies and econometric analysis Designed to bridge the gap between social science studies and field-econometrics, Econometric Analysis, 8th Edition, Global Edition, presents this ever-growing area at an accessible graduate level. The book first introduces students to basic techniques, a rich variety of models, and underlying theory that is easy to put into practice. It then presents students with a sufficient theoretical background to understand advanced techniques and to recognise new variants of established models. This focus, along with hundreds of worked numerical examples, ensures that students can apply the theory to real-world application and are prepared to be successful economists in the field.{$<$}/p{$>$} {$<$}br{$><$}br{$>$}},
  isbn = {9781292231150},
  langid = {english},
  file = {/home/thomas/Zotero/storage/NE6XIYDM/9781292231150.html}
}

@book{grimmett1986,
  title = {Probability: {{An Introduction}}},
  shorttitle = {Probability},
  author = {Grimmett, Geoffrey and Welsh, D. J. A.},
  date = {1986},
  eprint = {DyifaCLXxkIC},
  eprinttype = {googlebooks},
  publisher = {Clarendon Press},
  abstract = {This new undergraduate text offers a concise introduction to probability and random processes. Exercises and problems range from simple to difficult, and the overall treatment, though elementary, includes rigorous mathematical arguments. Chapters contain core material for a beginning course in probability, a treatment of joint distributions leading to accounts of moment-generating functions, the law of large numbers and the central limit theorem, and basic random processes.},
  isbn = {978-0-19-853264-4},
  langid = {english},
  pagetotal = {230}
}

@online{gu2022,
  title = {Efficiently {{Modeling Long Sequences}} with {{Structured State Spaces}}},
  author = {Gu, Albert and Goel, Karan and Ré, Christopher},
  date = {2022-08-05},
  eprint = {2111.00396},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2111.00396},
  url = {http://arxiv.org/abs/2111.00396},
  urldate = {2024-01-01},
  abstract = {A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of \$10000\$ or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) \textbackslash ( x'(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t) \textbackslash ), and showed that for appropriate choices of the state matrix \textbackslash ( A \textbackslash ), this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning \textbackslash ( A \textbackslash ) with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91\textbackslash\% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation \$60\textbackslash times\$ faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/gu2022.pdf;/home/thomas/Zotero/storage/LGUIB4RK/2111.html}
}

@online{gu2023,
  title = {Mamba: {{Linear-Time Sequence Modeling}} with {{Selective State Spaces}}},
  shorttitle = {Mamba},
  author = {Gu, Albert and Dao, Tri},
  date = {2023-12-01},
  eprint = {2312.00752},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.00752},
  url = {http://arxiv.org/abs/2312.00752},
  urldate = {2023-12-31},
  abstract = {Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5\$\textbackslash times\$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/gu2023.pdf;/home/thomas/Zotero/storage/3SUGQ2QC/2312.html}
}

@online{guo2015,
  title = {Quantization Based {{Fast Inner Product Search}}},
  author = {Guo, Ruiqi and Kumar, Sanjiv and Choromanski, Krzysztof and Simcha, David},
  date = {2015-09-04},
  eprint = {1509.01469},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1509.01469},
  url = {http://arxiv.org/abs/1509.01469},
  urldate = {2023-11-13},
  abstract = {We propose a quantization based approach for fast approximate Maximum Inner Product Search (MIPS). Each database vector is quantized in multiple subspaces via a set of codebooks, learned directly by minimizing the inner product quantization error. Then, the inner product of a query to a database vector is approximated as the sum of inner products with the subspace quantizers. Different from recently proposed LSH approaches to MIPS, the database vectors and queries do not need to be augmented in a higher dimensional feature space. We also provide a theoretical analysis of the proposed approach, consisting of the concentration results under mild assumptions. Furthermore, if a small sample of example queries is given at the training time, we propose a modified codebook learning procedure which further improves the accuracy. Experimental results on a variety of datasets including those arising from deep neural networks show that the proposed approach significantly outperforms the existing state-of-the-art.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/guo2015.pdf;/home/thomas/Zotero/storage/PUC7YB59/1509.html}
}

@online{guo2020,
  title = {Accelerating {{Large-Scale Inference}} with {{Anisotropic Vector Quantization}}},
  author = {Guo, Ruiqi and Sun, Philip and Lindgren, Erik and Geng, Quan and Simcha, David and Chern, Felix and Kumar, Sanjiv},
  date = {2020-12-04},
  eprint = {1908.10396},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1908.10396},
  url = {http://arxiv.org/abs/1908.10396},
  urldate = {2023-11-13},
  abstract = {Quantization based techniques are the current state-of-the-art for scaling maximum inner product search to massive databases. Traditional approaches to quantization aim to minimize the reconstruction error of the database points. Based on the observation that for a given query, the database points that have the largest inner products are more relevant, we develop a family of anisotropic quantization loss functions. Under natural statistical assumptions, we show that quantization with these loss functions leads to a new variant of vector quantization that more greatly penalizes the parallel component of a datapoint's residual relative to its orthogonal component. The proposed approach achieves state-of-the-art results on the public benchmarks available at \textbackslash url\{ann-benchmarks.com\}.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/guo2020.pdf;/home/thomas/Zotero/storage/E6AVEC3F/1908.html}
}

@book{hall2015,
  title = {Lie {{Groups}}, {{Lie Algebras}}, and {{Representations}}: {{An Elementary Introduction}}},
  shorttitle = {Lie {{Groups}}, {{Lie Algebras}}, and {{Representations}}},
  author = {Hall, Brian C.},
  date = {2015},
  series = {Graduate {{Texts}} in {{Mathematics}}},
  volume = {222},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-13467-3},
  url = {https://link.springer.com/10.1007/978-3-319-13467-3},
  urldate = {2024-03-15},
  isbn = {978-3-319-13466-6 978-3-319-13467-3},
  langid = {english},
  keywords = {Baker-Campbell-Hausdorff formula,Cartan-Weyl theory,Lie algebras,Lie groups,representation theory},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/hall2015.pdf}
}

@online{hornrogara,
  title = {Matrix Analysis 2nd Edition | {{Algebra}} | {{Cambridge University Press}}},
  author = {Horn, Rogar A},
  url = {https://www.cambridge.org/de/universitypress/subjects/mathematics/algebra/matrix-analysis-2nd-edition?format=PB&isbn=9780521548236},
  urldate = {2023-11-29},
  abstract = {Linear algebra and matrix theory are fundamental tools in mathematical and physical science, as well as fertile fields for research. This second edition of this acclaimed text presents results of both classic and recent matrix analysis using canonical forms as a unifying theme and demonstrates their importance in a variety of applications. This thoroughly revised and updated second edition is a text for a second course on linear algebra and has more than 1,100 problems and exercises, new sections on the singular value and CS decompositions and the Weyr canonical form, expanded treatments of inverse problems and of block matrices, and much more.},
  file = {/home/thomas/Downloads/Roger A. Horn, Charles R. Johnson - Matrix Analysis-Cambridge University Press (2013).pdf;/home/thomas/Zotero/storage/S4UW9BI7/matrix-analysis-2nd-edition.html}
}

@book{howell2016,
  title = {Principles of {{Fourier Analysis}}},
  author = {Howell, Kenneth B.},
  date = {2016-12-12},
  edition = {2nd edition},
  publisher = {CRC Press},
  abstract = {Fourier analysis is one of the most useful and widely employed sets of tools for the engineer, the scientist, and the applied mathematician. As such, students and practitioners in these disciplines need a practical and mathematically solid introduction to its principles. They need straightforward verifications of its results and formulas, and they need clear indications of the limitations of those results and formulas.Principles of Fourier Analysis furnishes all this and more. It provides a comprehensive overview of the mathematical theory of Fourier analysis, including the development of Fourier series, "classical" Fourier transforms, generalized Fourier transforms and analysis, and the discrete theory. Much of the author's development is strikingly different from typical presentations. His approach to defining the classical Fourier transform results in a much cleaner, more coherent theory that leads naturally to a starting point for the generalized theory. He also introduces a new generalized theory based on the use of Gaussian test functions that yields an even more general -yet simpler -theory than usually presented.Principles of Fourier Analysis stimulates the appreciation and understanding of the fundamental concepts and serves both beginning students who have seen little or no Fourier analysis as well as the more advanced students who need a deeper understanding. Insightful, non-rigorous derivations motivate much of the material, and thought-provoking examples illustrate what can go wrong when formulas are misused. With clear, engaging exposition, readers develop the ability to intelligently handle the more sophisticated mathematics that Fourier analysis ultimately requires.},
  langid = {english},
  pagetotal = {791}
}

@book{ibe2014,
  title = {Fundamentals of {{Applied Probability}} and {{Random Processes}}},
  author = {Ibe, Oliver},
  date = {2014-07-28},
  edition = {2nd edition},
  publisher = {Academic Press},
  location = {Amsterdam Boston},
  isbn = {978-0-12-800852-2},
  langid = {english},
  pagetotal = {456}
}

@inproceedings{jadon2020,
  title = {A Survey of Loss Functions for Semantic Segmentation},
  booktitle = {2020 {{IEEE Conference}} on {{Computational Intelligence}} in {{Bioinformatics}} and {{Computational Biology}} ({{CIBCB}})},
  author = {Jadon, Shruti},
  date = {2020-10},
  pages = {1--7},
  doi = {10.1109/CIBCB48159.2020.9277638},
  url = {https://ieeexplore.ieee.org/abstract/document/9277638},
  urldate = {2023-12-16},
  abstract = {Image Segmentation has been an active field of research as it has a wide range of applications, ranging from automated disease detection to self driving cars. In the past five years, various papers came up with different objective loss functions used in different cases such as biased data, sparse segmentation, etc. In this paper, we have summarized some of the well-known loss functions widely used for Image Segmentation and listed out the cases where their usage can help in fast and better convergence of a model. Furthermore, we have also introduced a new log-cosh dice loss function and compared its performance on NBFS skull-segmentation open source data-set with widely used loss functions. We also showcased that certain loss functions perform well across all data-sets and can be taken as a good baseline choice in unknown data distribution scenarios.},
  eventtitle = {2020 {{IEEE Conference}} on {{Computational Intelligence}} in {{Bioinformatics}} and {{Computational Biology}} ({{CIBCB}})},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/jadon2020.pdf;/home/thomas/Zotero/storage/S689HGHR/9277638.html}
}

@article{jegou2011,
  title = {Product {{Quantization}} for {{Nearest Neighbor Search}}},
  author = {Jégou, Herve and Douze, Matthijs and Schmid, Cordelia},
  date = {2011-01},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {33},
  number = {1},
  pages = {117--128},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2010.57},
  url = {https://ieeexplore.ieee.org/document/5432202},
  urldate = {2023-11-13},
  abstract = {This paper introduces a product quantization-based approach for approximate nearest neighbor search. The idea is to decompose the space into a Cartesian product of low-dimensional subspaces and to quantize each subspace separately. A vector is represented by a short code composed of its subspace quantization indices. The euclidean distance between two vectors can be efficiently estimated from their codes. An asymmetric version increases precision, as it computes the approximate distance between a vector and a code. Experimental results show that our approach searches for nearest neighbors efficiently, in particular in combination with an inverted file system. Results for SIFT and GIST image descriptors show excellent search accuracy, outperforming three state-of-the-art approaches. The scalability of our approach is validated on a data set of two billion vectors.},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/jegou2011.pdf;/home/thomas/Zotero/storage/7T72YLS5/5432202.html}
}

@article{kalman1960,
  title = {A {{New Approach}} to {{Linear Filtering}} and {{Prediction Problems}}},
  author = {Kalman, R. E.},
  date = {1960-03-01},
  journaltitle = {Journal of Basic Engineering},
  doi = {10.1115/1.3662552},
  url = {https://www.scinapse.io/papers/2105934661},
  urldate = {2023-12-31},
  abstract = {R. E. Kalman},
  langid = {english}
}

@book{kreyszig1978,
  title = {Introductory Functional Analysis with Applications},
  author = {Kreyszig, Erwin},
  date = {1978},
  series = {Wiley Classics Library},
  edition = {15. print},
  publisher = {Wiley},
  location = {New York, N.Y.},
  abstract = {Provides avenues for applying functional analysis to the practical study of natural sciences as well as mathematics. Contains worked problems on Hilbert space theory and on Banach spaces and emphasizes concepts, principles, methods and major applications of functional analysis.},
  isbn = {978-0-471-50459-7},
  langid = {english},
  pagetotal = {688},
  keywords = {Funktionalanalysis},
  annotation = {OCLC: 474285534}
}

@book{kreyszig2007,
  title = {Introductory Functional Analysis with Applications},
  author = {Kreyszig},
  date = {2007},
  series = {Wiley Classics Library},
  publisher = {Wiley India Pvt. Limited},
  url = {https://books.google.de/books?id=osXw-pRsptoC},
  isbn = {978-81-265-1191-4}
}

@book{kwak2004,
  title = {Linear {{Algebra}}},
  author = {Kwak, Jin Ho and Hong, Sungpyo},
  date = {2004},
  publisher = {Birkhäuser},
  location = {Boston, MA},
  doi = {10.1007/978-0-8176-8194-4},
  url = {http://link.springer.com/10.1007/978-0-8176-8194-4},
  urldate = {2023-11-28},
  isbn = {978-0-8176-4294-5 978-0-8176-8194-4},
  langid = {english},
  keywords = {algebra,computer,computer science,Eigenvalue,Eigenvector,linear algebra,Matrix,matrix theory,Transformation},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/kwak2004.pdf}
}

@book{lang1999,
  title = {Complex {{Analysis}}},
  author = {Lang, Serge},
  date = {1999},
  series = {Graduate {{Texts}} in {{Mathematics}}},
  volume = {103},
  publisher = {Springer},
  location = {New York, NY},
  doi = {10.1007/978-1-4757-3083-8},
  url = {http://link.springer.com/10.1007/978-1-4757-3083-8},
  urldate = {2024-03-13},
  isbn = {978-1-4419-3135-1 978-1-4757-3083-8},
  keywords = {calculus,Cauchy's integral formula,Complex analysis,differential equation,gamma function,Jensen's formula,maximum,Meromorphic function},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/lang1999.pdf}
}

@book{lange2019,
  title = {Hebbian Learning Approaches Based on General Inner Products and Distance Measures in Non-{{Euclidean}} Spaces},
  author = {Lange, Mandy},
  date = {2019},
  publisher = {University of Groningen},
  location = {[Groningen]},
  abstract = {The topic of this thesis is to define a unified and generalized scheme for Hebbian approaches in non-Euclidean spaces for unsupervised and supervised learning. This can be realized in different ways. One possibility is the replacement of the inner product by a semi-inner product (SIP). A SIP relaxes the strict properties of an inner product but preserves the linear aspect in the first argument. Thus, these SIPs are natural equivalents of inner products generating Banach spaces instead of Hilbert spaces for inner products. In this work SIPs for Banach spaces are considered for unsupervised Hebbian like learning approaches. Further, the learning scheme of the supervised Learning Vector Quantization (LVQ) network, which is originally designed for applications in Euclidean data space, can be interpreted under specific circumstances as a Hebbian like learning, too. It is shown that, non-Euclidean metrics applied in LVQ can improve the performance of classification learning compared to Euclidean variants.The previously addressed Hebbian learning methods are vectorial approaches. However, if the data space is a vector space of matrices equipped with a respective matrix norm, then matrix approaches for Hebbian like learning methods become of interest. The extension of these methods in non-Euclidean spaces of matrices to process matrix data is the last main point of this thesis.},
  isbn = {978-94-034-1470-6},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/lange2019.pdf}
}

@online{lin2018,
  title = {Focal {{Loss}} for {{Dense Object Detection}}},
  author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr},
  date = {2018-02-07},
  eprint = {1708.02002},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1708.02002},
  url = {http://arxiv.org/abs/1708.02002},
  urldate = {2023-12-16},
  abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/lin2018.pdf;/home/thomas/Zotero/storage/4YJ9LBZT/1708.html}
}

@online{lingle2023,
  title = {Transformer-{{VQ}}: {{Linear-Time Transformers}} via {{Vector Quantization}}},
  shorttitle = {Transformer-{{VQ}}},
  author = {Lingle, Lucas D.},
  date = {2023-09-28},
  eprint = {2309.16354},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2309.16354},
  url = {http://arxiv.org/abs/2309.16354},
  urldate = {2023-11-13},
  abstract = {We introduce Transformer-VQ, a decoder-only transformer computing softmax-based dense self-attention in linear time. Transformer-VQ's efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In large-scale experiments, Transformer-VQ is shown highly competitive in quality, with strong results on Enwik8 (0.99 bpb), PG-19 (26.6 ppl), and ImageNet64 (3.16 bpb). Code: https://github.com/transformer-vq/transformer\_vq},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/lingle2023.pdf;/home/thomas/Zotero/storage/AMZDGPVR/2309.html}
}

@online{liu2024,
  title = {{{LongVQ}}: {{Long Sequence Modeling}} with {{Vector Quantization}} on {{Structured Memory}}},
  shorttitle = {{{LongVQ}}},
  author = {Liu, Zicheng and Wang, Li and Li, Siyuan and Wang, Zedong and Lin, Haitao and Li, Stan Z.},
  date = {2024-04-18},
  eprint = {2404.11163},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2404.11163},
  url = {http://arxiv.org/abs/2404.11163},
  urldate = {2024-04-21},
  abstract = {Transformer models have been successful in various sequence processing tasks, but the self-attention mechanism's computational cost limits its practicality for long sequences. Although there are existing attention variants that improve computational efficiency, they have a limited ability to abstract global information effectively based on their hand-crafted mixing strategies. On the other hand, state-space models (SSMs) are tailored for long sequences but cannot capture complicated local information. Therefore, the combination of them as a unified token mixer is a trend in recent long-sequence models. However, the linearized attention degrades performance significantly even when equipped with SSMs. To address the issue, we propose a new method called LongVQ. LongVQ uses the vector quantization (VQ) technique to compress the global abstraction as a length-fixed codebook, enabling the linear-time computation of the attention matrix. This technique effectively maintains dynamic global and local patterns, which helps to complement the lack of long-range dependency issues. Our experiments on the Long Range Arena benchmark, autoregressive language modeling, and image and speech classification demonstrate the effectiveness of LongVQ. Our model achieves significant improvements over other sequence models, including variants of Transformers, Convolutions, and recent State Space Models.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/liu2024.pdf;/home/thomas/Zotero/storage/GL4588R4/2404.html}
}

@article{lumer1961,
  title = {Semi-Inner-Product Spaces},
  author = {Lumer, G.},
  date = {1961},
  journaltitle = {Trans. Amer. Math. Soc.},
  volume = {100},
  number = {1},
  pages = {29--43},
  issn = {0002-9947, 1088-6850},
  doi = {10.1090/S0002-9947-1961-0133024-2},
  url = {https://www.ams.org/tran/1961-100-01/S0002-9947-1961-0133024-2/},
  urldate = {2023-11-15},
  abstract = {Advancing research. Creating connections.},
  langid = {english},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/lumer1961.pdf}
}

@online{millar2011,
  title = {Maximum {{Likelihood Estimation}} and {{Inference}}: {{With Examples}} in {{R}}, {{SAS}} and {{ADMB}} | {{Wiley}}},
  shorttitle = {Maximum {{Likelihood Estimation}} and {{Inference}}},
  author = {{millar}},
  date = {2011},
  url = {https://www.wiley.com/en-us/Maximum+Likelihood+Estimation+and+Inference%3A+With+Examples+in+R%2C+SAS+and+ADMB-p-9780470094822},
  urldate = {2023-12-23},
  abstract = {This book takes a fresh look at the popular and well-established method of maximum likelihood for statistical estimation and inference. It begins with an intuitive introduction to the concepts and background of likelihood, and moves through to the latest developments in maximum likelihood methodology, including general latent variable models and new material for the practical implementation of integrated likelihood using the free ADMB software. Fundamental issues of statistical inference are also examined, with a presentation of some of the philosophical debates underlying the choice of statistical paradigm. Key features: Provides an accessible introduction to pragmatic maximum likelihood modelling. Covers more advanced topics, including general forms of latent variable models (including non-linear and non-normal mixed-effects and state-space models) and the use of maximum likelihood variants, such as estimating equations, conditional likelihood, restricted likelihood and integrated likelihood. Adopts a practical approach, with a focus on providing the relevant tools required by researchers and practitioners who collect and analyze real data. Presents numerous examples and case studies across a wide range of applications including medicine, biology and ecology. Features applications from a range of disciplines, with implementation in R, SAS and/or ADMB. Provides all program code and software extensions on a supporting website. Confines supporting theory to the final chapters to maintain a readable and pragmatic focus of the preceding chapters. This book is not just an accessible and practical text about maximum likelihood, it is a comprehensive guide to modern maximum likelihood estimation and inference. It will be of interest to readers of all levels, from novice to expert. It will be of great benefit to researchers, and to students of statistics from senior undergraduate to graduate level. For use as a course text, exercises are provided at the end of each chapter.},
  langid = {american},
  organization = {Wiley.com},
  file = {/home/thomas/Zotero/storage/ASYFM2KI/Maximum+Likelihood+Estimation+and+Inference+With+Examples+in+R,+SAS+and+ADMB-p-9780470094822.html}
}

@article{nagy,
  title = {Ordinary {{Diﬀerential Equations}}},
  author = {Nagy, Gabriel},
  langid = {english},
  file = {/home/thomas/Zotero/storage/DMSY57RB/Nagy - Ordinary Diﬀerential Equations.pdf}
}

@inproceedings{nam2014,
  title = {Large-{{Scale Multi-label Text Classification}} — {{Revisiting Neural Networks}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Nam, Jinseok and Kim, Jungi and Loza Mencía, Eneldo and Gurevych, Iryna and Fürnkranz, Johannes},
  editor = {Calders, Toon and Esposito, Floriana and Hüllermeier, Eyke and Meo, Rosa},
  date = {2014},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {437--452},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-662-44851-9_28},
  abstract = {Neural networks have recently been proposed for multi-label classification because they are able to capture and model label dependencies in the output layer. In this work, we investigate limitations of BP-MLL, a neural network (NN) architecture that aims at minimizing pairwise ranking error. Instead, we propose to use a comparably simple NN approach with recently proposed learning techniques for large-scale multi-label text classification tasks. In particular, we show that BP-MLL’s ranking loss minimization can be efficiently and effectively replaced with the commonly used cross entropy error function, and demonstrate that several advances in neural network training that have been developed in the realm of deep learning can be effectively employed in this setting. Our experimental results show that simple NN models equipped with advanced techniques such as rectified linear units, dropout, and AdaGrad perform as well as or even outperform state-of-the-art approaches on six large-scale textual datasets with diverse characteristics.},
  isbn = {978-3-662-44851-9},
  langid = {english},
  keywords = {Cross Entropy,Hide Layer,Hide Unit,Single Hide Layer,Stochastic Gradient Descent},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/nam2014.pdf}
}

@book{needham1999,
  title = {Visual {{Complex Analysis}}},
  author = {Needham, Tristan},
  date = {1999-02-18},
  edition = {Reprint edition},
  publisher = {Oxford University Press, USA},
  location = {Oxford},
  abstract = {This radical first course on complex analysis brings a beautiful and powerful subject to life by consistently using geometry (not calculation) as the means of explanation. Aimed at undergraduate students in mathematics, physics, and engineering, the book's intuitive explanations, lack of advanced prerequisites, and consciously user-friendly prose style will help students to master the subject more readily than was previously possible. The key to this is the book's use of new geometric arguments in place of the standard calculational ones. These geometric arguments are communicated with the aid of hundreds of diagrams of a standard seldom encountered in mathematical works. A new approach to a classical topic, this work will be of interest to students in mathematics, physics, and engineering, as well as to professionals in these fields.},
  isbn = {978-0-19-853446-4},
  langid = {english},
  pagetotal = {616}
}

@article{nelder1972,
  title = {Generalized {{Linear Models}}},
  author = {Nelder, J. A. and Wedderburn, R. W. M.},
  date = {1972},
  journaltitle = {Journal of the Royal Statistical Society. Series A (General)},
  volume = {135},
  number = {3},
  eprint = {2344614},
  eprinttype = {jstor},
  pages = {370--384},
  publisher = {[Royal Statistical Society, Wiley]},
  issn = {0035-9238},
  doi = {10.2307/2344614},
  url = {https://www.jstor.org/stable/2344614},
  urldate = {2023-12-23},
  abstract = {The technique of iterative weighted linear regression can be used to obtain maximum likelihood estimates of the parameters with observations distributed according to some exponential family and systematic effects that can be made linear by a suitable transformation. A generalization of the analysis of variance is given for these models using log-likelihoods. These generalized linear models are illustrated by examples relating to four distributions; the Normal, Binomial (probit analysis, etc.), Poisson (contingency tables) and gamma (variance components). The implications of the approach in designing statistics courses are discussed.}
}

@book{nita2023,
  title = {Advances to {{Homomorphic}} and {{Searchable Encryption}}},
  author = {Nita, Stefania Loredana and Mihailescu, Marius Iulian},
  date = {2023},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-43214-9},
  url = {https://link.springer.com/10.1007/978-3-031-43214-9},
  urldate = {2024-05-21},
  isbn = {978-3-031-43213-2 978-3-031-43214-9},
  langid = {english},
  keywords = {big data security,cloud computing security,data science,homomorphic encryption,information security,lattice-based cryptography,multivariate cryptography,quantum cryptography,searchable encryption}
}

@book{olson2017,
  title = {Applied {{Fourier Analysis}}},
  author = {Olson, Tim},
  date = {2017},
  publisher = {Springer},
  location = {New York, NY},
  doi = {10.1007/978-1-4939-7393-4},
  url = {http://link.springer.com/10.1007/978-1-4939-7393-4},
  urldate = {2024-02-18},
  isbn = {978-1-4939-7391-0 978-1-4939-7393-4},
  langid = {english},
  keywords = {analysis,communications,Fourier applications,medical imaging,partial differential equations,sampling},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/olson2017.pdf}
}

@article{ramos2018,
  title = {Deconstructing {{Cross-Entropy}} for {{Probabilistic Binary Classifiers}}},
  author = {Ramos, Daniel and Franco-Pedroso, Javier and Lozano-Diez, Alicia and Gonzalez-Rodriguez, Joaquin},
  date = {2018-03},
  journaltitle = {Entropy},
  volume = {20},
  number = {3},
  pages = {208},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1099-4300},
  doi = {10.3390/e20030208},
  url = {https://www.mdpi.com/1099-4300/20/3/208},
  urldate = {2023-12-15},
  abstract = {In this work, we analyze the cross-entropy function, widely used in classifiers both as a performance measure and as an optimization objective. We contextualize cross-entropy in the light of Bayesian decision theory, the formal probabilistic framework for making decisions, and we thoroughly analyze its motivation, meaning and interpretation from an information-theoretical point of view. In this sense, this article presents several contributions: First, we explicitly analyze the contribution to cross-entropy of (i) prior knowledge; and (ii) the value of the features in the form of a likelihood ratio. Second, we introduce a decomposition of cross-entropy into two components: discrimination and calibration. This decomposition enables the measurement of different performance aspects of a classifier in a more precise way; and justifies previously reported strategies to obtain reliable probabilities by means of the calibration of the output of a discriminating classifier. Third, we give different information-theoretical interpretations of cross-entropy, which can be useful in different application scenarios, and which are related to the concept of reference probabilities. Fourth, we present an analysis tool, the Empirical Cross-Entropy (ECE) plot, a compact representation of cross-entropy and its aforementioned decomposition. We show the power of ECE plots, as compared to other classical performance representations, in two diverse experimental examples: a speaker verification system, and a forensic case where some glass findings are present.},
  issue = {3},
  langid = {english},
  keywords = {Bayesian,calibration,classifier,cross-entropy,discrimination,ECE plot,probabilistic},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/ramos2018.pdf}
}

@online{reddi2018,
  title = {Stochastic {{Negative Mining}} for {{Learning}} with {{Large Output Spaces}}},
  author = {Reddi, Sashank J. and Kale, Satyen and Yu, Felix and Holtmann-Rice, Dan and Chen, Jiecao and Kumar, Sanjiv},
  date = {2018-10-16},
  eprint = {1810.07076},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1810.07076},
  url = {http://arxiv.org/abs/1810.07076},
  urldate = {2023-11-13},
  abstract = {We consider the problem of retrieving the most relevant labels for a given input when the size of the output space is very large. Retrieval methods are modeled as set-valued classifiers which output a small set of classes for each input, and a mistake is made if the label is not in the output set. Despite its practical importance, a statistically principled, yet practical solution to this problem is largely missing. To this end, we first define a family of surrogate losses and show that they are calibrated and convex under certain conditions on the loss parameters and data distribution, thereby establishing a statistical and analytical basis for using these losses. Furthermore, we identify a particularly intuitive class of loss functions in the aforementioned family and show that they are amenable to practical implementation in the large output space setting (i.e. computation is possible without evaluating scores of all labels) by developing a technique called Stochastic Negative Mining. We also provide generalization error bounds for the losses in the family. Finally, we conduct experiments which demonstrate that Stochastic Negative Mining yields benefits over commonly used negative sampling approaches.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/reddi2018.pdf;/home/thomas/Zotero/storage/W6JVNWF3/1810.html}
}

@inproceedings{rivest1978,
  title = {{{ON DATA BANKS AND PRIVACY HOMOMORPHISMS}}},
  author = {Rivest, Ronald L. and Dertouzos, M.},
  date = {1978},
  url = {https://www.semanticscholar.org/paper/ON-DATA-BANKS-AND-PRIVACY-HOMOMORPHISMS-Rivest-Dertouzos/c365f01d330b2211e74069120e88cff37eacbcf5},
  urldate = {2024-05-21},
  abstract = {Encryption is a well—known technique for preserving the privacy of sensitive information. One of the basic, apparently inherent, limitations of this technique is that an information system working with encrypted data can at most store or retrieve the data for the user; any more complicated operations seem to require that the data be decrypted before being operated on. This limitation follows from the choice of encryption functions used, however, and although there are some truly inherent limitations on what can be accomplished, we shall see that it appears likely that there exist encryption functions which permit encrypted data to be operated on without preliminary decryption of the operands, for many sets of interesting operations. These special encryption functions we call “privacy homomorphisms”; they form an interesting subset of arbitrary encryption schemes (called “privacy transformations”).}
}

@book{rudin1976,
  title = {Principles of {{Mathematical Analysis}}},
  author = {Rudin, Walter},
  date = {1976},
  eprint = {kwqzPAAACAAJ},
  eprinttype = {googlebooks},
  publisher = {McGraw-Hill},
  abstract = {The third edition of this well known text continues to provide a solid foundation in mathematical analysis for undergraduate and first-year graduate students. The text begins with a discussion of the real number system as a complete ordered field. (Dedekind's construction is now treated in an appendix to Chapter I.) The topological background needed for the development of convergence, continuity, differentiation and integration is provided in Chapter 2. There is a new section on the gamma function, and many new and interesting exercises are included. This text is part of the Walter Rudin Student Series in Advanced Mathematics.},
  isbn = {978-0-07-085613-4},
  langid = {english},
  pagetotal = {342},
  keywords = {Mathematics / Calculus}
}

@inproceedings{sato1995,
  title = {Generalized {{Learning Vector Quantization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Sato, Atsushi and Yamada, Keiji},
  date = {1995},
  volume = {8},
  publisher = {MIT Press},
  url = {https://papers.nips.cc/paper_files/paper/1995/hash/9c3b1830513cc3b8fc4b76635d32e692-Abstract.html},
  urldate = {2024-05-12},
  abstract = {We  propose  a  new  learning  method,  "Generalized  Learning  Vec(cid:173) tor Quantization (GLVQ),"  in which reference vectors are updated  based on the steepest descent method in order to minimize the cost  function .  The  cost  function  is  determined  so  that  the  obtained  learning  rule  satisfies  the  convergence  condition.  We  prove  that  Kohonen's  rule  as  used  in  LVQ  does  not  satisfy  the  convergence  condition  and  thus  degrades  recognition  ability.  Experimental re(cid:173) sults  for  printed  Chinese  character recognition  reveal  that  GLVQ  is  superior to  LVQ  in  recognition  ability.},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/sato1995.pdf}
}

@book{shao2003,
  title = {Mathematical {{Statistics}}},
  author = {Shao, Jun},
  editor = {Casella, G. and Fienberg, S. and Olkin, I.},
  editortype = {redactor},
  date = {2003},
  series = {Springer {{Texts}} in {{Statistics}}},
  publisher = {Springer},
  location = {New York, NY},
  doi = {10.1007/b97553},
  url = {http://link.springer.com/10.1007/b97553},
  urldate = {2023-12-26},
  isbn = {978-0-387-95382-3 978-0-387-21718-5},
  keywords = {likelihood,Markov chain,Mathematica,mathematical statistics,Mathematical Statistics,probability,probability theory,statistical theory,Statistical Theory,statistics},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/shao2003.pdf}
}

@book{shifrin2005,
  title = {Multivariable Mathematics: {{Linear}} Algebra, Multivariable Calculus, and Manifolds},
  author = {Shifrin, Theodore},
  date = {2005},
  publisher = {John Wiley \& Sons, Inc.},
  isbn = {978-0-471-52638-4 0-471-52638-X}
}

@book{stanley2012,
  title = {Enumerative {{Combinatorics}}: {{Volume}} 1},
  shorttitle = {Enumerative {{Combinatorics}}},
  author = {Stanley, Richard P.},
  date = {2012-02-23},
  edition = {2. edition},
  publisher = {Cambridge University Press},
  location = {Cambridge, NY},
  abstract = {Richard Stanley's two-volume basic introduction to enumerative combinatorics has become the standard guide to the topic for students and experts alike. This thoroughly revised second edition of Volume 1 includes ten new sections and more than 300 new exercises, most with solutions, reflecting numerous new developments since the publication of the first edition in 1986. The material in Volume 1 was chosen to cover those parts of enumerative combinatorics of greatest applicability and with the most important connections with other areas of mathematics. The four chapters are devoted to an introduction to enumeration (suitable for advanced undergraduates), sieve methods, partially ordered sets, and rational generating functions. Much of the material is related to generating functions, a fundamental tool in enumerative combinatorics. In this new edition, the author brings the coverage up to date and includes a wide variety of additional applications and examples, as well as updated and expanded chapter bibliographies. Many of the less difficult new exercises have no solutions so that they can more easily be assigned to students. The material on P-partitions has been rearranged and generalized; the treatment of permutation statistics has been greatly enlarged; and there are also new sections on q-analogues of permutations, hyperplane arrangements, the cd-index, promotion and evacuation, and differential posets.},
  isbn = {978-1-107-01542-5},
  langid = {english},
  pagetotal = {632}
}

@book{vasudeva2017,
  title = {Elements of {{Hilbert Spaces}} and {{Operator Theory}}},
  author = {Vasudeva, Harkrishan Lal},
  date = {2017},
  publisher = {Springer},
  location = {Singapore},
  doi = {10.1007/978-981-10-3020-8},
  url = {http://link.springer.com/10.1007/978-981-10-3020-8},
  urldate = {2023-11-14},
  isbn = {978-981-10-3019-2 978-981-10-3020-8},
  langid = {english},
  keywords = {Banach Spaces,Finite Dimensional Spaces,Functional analysis,Linear operators,Operator theory,Riesz Lemma,Special theory},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/vasudeva2017.pdf}
}

@online{vaswani2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2023-08-01},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2023-11-17},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/vaswani2023.pdf;/home/thomas/Zotero/storage/SCJWFGAA/1706.html}
}

@online{xie2014,
  title = {Crypto-{{Nets}}: {{Neural Networks}} over {{Encrypted Data}}},
  shorttitle = {Crypto-{{Nets}}},
  author = {Xie, Pengtao and Bilenko, Misha and Finley, Tom and Gilad-Bachrach, Ran and Lauter, Kristin and Naehrig, Michael},
  date = {2014-12-24},
  eprint = {1412.6181},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1412.6181},
  url = {http://arxiv.org/abs/1412.6181},
  urldate = {2024-05-21},
  abstract = {The problem we address is the following: how can a user employ a predictive model that is held by a third party, without compromising private information. For example, a hospital may wish to use a cloud service to predict the readmission risk of a patient. However, due to regulations, the patient's medical files cannot be revealed. The goal is to make an inference using the model, without jeopardizing the accuracy of the prediction or the privacy of the data. To achieve high accuracy, we use neural networks, which have been shown to outperform other learning models for many tasks. To achieve the privacy requirements, we use homomorphic encryption in the following protocol: the data owner encrypts the data and sends the ciphertexts to the third party to obtain a prediction from a trained model. The model operates on these ciphertexts and sends back the encrypted prediction. In this protocol, not only the data remains private, even the values predicted are available only to the data owner. Using homomorphic encryption and modifications to the activation functions and training algorithms of neural networks, we show that it is protocol is possible and may be feasible. This method paves the way to build a secure cloud-based neural network prediction services without invading users' privacy.},
  pubstate = {preprint},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/xie2014.pdf;/home/thomas/Zotero/storage/NE4WJDV2/1412.html}
}

@book{yi2014,
  title = {Homomorphic {{Encryption}} and {{Applications}}},
  author = {Yi, Xun and Paulet, Russell and Bertino, Elisa},
  date = {2014},
  series = {{{SpringerBriefs}} in {{Computer Science}}},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-12229-8},
  url = {https://link.springer.com/10.1007/978-3-319-12229-8},
  urldate = {2024-05-21},
  isbn = {978-3-319-12228-1 978-3-319-12229-8},
  langid = {english},
  keywords = {Critical Infrastructure Outsourcing,Cryptographic Assumption,Encrypted Email,Homomorphic Cryptography,Homomorphic Encryption,Homomorphic Encryption Application,Homomorphic Spam Filtering,Lattice Based Cryptography,Privacy Preserving Data Mining,Privacy Preserving Email},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/yi2014.pdf}
}

@book{young1988,
  title = {An {{Introduction}} to {{Hilbert Space}}},
  author = {Young, N.},
  date = {1988},
  publisher = {Cambridge University Press},
  location = {Cambridge},
  doi = {10.1017/CBO9781139172011},
  url = {https://www.cambridge.org/core/books/an-introduction-to-hilbert-space/41D80B163B8E186820B5A48F3665EC22},
  urldate = {2023-11-15},
  abstract = {This textbook is an introduction to the theory of Hilbert space and its applications. The notion of Hilbert space is central in functional analysis and is used in numerous branches of pure and applied mathematics. Dr Young has stressed applications of the theory, particularly to the solution of partial differential equations in mathematical physics and to the approximation of functions in complex analysis. Some basic familiarity with real analysis, linear algebra and metric spaces is assumed, but otherwise the book is self-contained. It is based on courses given at the University of Glasgow and contains numerous examples and exercises (many with solutions). Thus it will make an excellent first course in Hilbert space theory at either undergraduate or graduate level and will also be of interest to electrical engineers and physicists, particularly those involved in control theory and filter design.},
  isbn = {978-0-521-33717-5},
  file = {/home/thomas/Zotero/storage/Q72XTLTI/41D80B163B8E186820B5A48F3665EC22.html}
}

@book{zabczyk2008,
  title = {Mathematical {{Control Theory}}},
  author = {Zabczyk, Jerzy},
  date = {2008},
  publisher = {Birkhäuser},
  location = {Boston, MA},
  doi = {10.1007/978-0-8176-4733-9},
  url = {http://link.springer.com/10.1007/978-0-8176-4733-9},
  urldate = {2024-01-01},
  isbn = {978-0-8176-4732-2 978-0-8176-4733-9},
  langid = {english},
  keywords = {control,control system,control theory,dynamic programming,infinite dimensional linear systems,linear systems,mathematical control theory,nonlinear control,nonlinear system,observability,optimal control,programming,stability,stabilization,sys},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/zabczyk2008.pdf}
}

@online{zotero-182,
  title = {{{OpenStax}} | {{Calculus Volume}} 2},
  url = {https://openstax.org/},
  abstract = {OpenStax offers free college textbooks for all types of students, making education accessible \& affordable for everyone. Browse our list of available subjects!},
  langid = {american},
  file = {/home/thomas/Zotero/storage/8HXCFBAT/calculus-volume-2.html}
}

@online{zotero-211,
  title = {Amazon.de:{{Customer Reviews}}: {{Fourier Analysis}} and {{Its Applications}} ({{Pure}} and {{Applied Undergraduate Texts}}, 4, {{Band}} 4)},
  url = {https://www.amazon.de/-/en/Gerald-B-Folland/product-reviews/0821847902/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews},
  urldate = {2024-02-26},
  file = {/home/thomas/Zotero/storage/6UVGAD4G/ref=cm_cr_dp_d_show_all_btm.html}
}

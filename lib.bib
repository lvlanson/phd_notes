@incollection{2011,
  title = {Front {{Matter}}},
  booktitle = {Maximum {{Likelihood Estimation}} and {{Inference}}},
  date = {2011},
  pages = {i-xvi},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9780470094846.fmatter},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470094846.fmatter},
  urldate = {2023-12-23},
  abstract = {The prelims comprise: Half-Title Page Wiley Series Page Title Page Copyright Page Table of Contents Preface},
  isbn = {978-0-470-09484-6},
  langid = {english},
  file = {/home/thomas/Zotero/storage/UMM6EW4N/2011 - Front Matter.pdf;/home/thomas/Zotero/storage/69NKTQNW/9780470094846.html}
}

@incollection{2011a,
  title = {A {{Taste}} of {{Likelihood}}},
  booktitle = {Maximum {{Likelihood Estimation}} and {{Inference}}},
  date = {2011},
  pages = {1--17},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9780470094846.ch1},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470094846.ch1},
  urldate = {2023-12-23},
  abstract = {In this book, likelihood is used within the traditional framework of frequentist statistics, and maximum likelihood (ML) is presented as a general-purpose tool for inference, including the evaluation of statistical significance, calculation of confidence intervals (CIs), model assessment, and prediction. ML estimators (MLEs) have optimal properties for sufficiently large sample sizes. The pragmatic use of ML inference is the primary focus of the book. Likelihood is also a fundamental concept underlying other statistical paradigms, especially the Bayesian approach. A simple binomial example is used to motivate and demonstrate many of the essential properties of likelihood that are developed in later chapters of the book. The code used in the chapter demonstrates how an explicit log-likelihood function is maximized within each of SAS, R and ADMB, and the calculation of the Wald and likelihood-ratio CIs. Controlled Vocabulary Terms Bayesian inference; confidence interval; frequentist; likelihood ratio; statistics},
  isbn = {978-0-470-09484-6},
  langid = {english},
  keywords = {ADMB,Bayesian approach,binomial model,frequentist statistics,likelihood estimators (MLEs),maximum,R functions,SAS,Wald confidence interval},
  file = {/home/thomas/Zotero/storage/9MIF9F3F/9780470094846.html}
}

@incollection{2011b,
  title = {Essential {{Concepts}} and {{Iid Examples}}},
  booktitle = {Maximum {{Likelihood Estimation}} and {{Inference}}},
  date = {2011},
  pages = {18--35},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9780470094846.ch2},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470094846.ch2},
  urldate = {2023-12-23},
  abstract = {For continuous data, the likelihood is defined to be the density function evaluated at the observed data, and is regarded as a function of the unknown parameter. This chapter takes a quick look at a long-established interpretation of likelihood that it provides a measure of the relative support for different values of θ. The examples presented in the chapter look at maximum likelihood estimation (MLE) for models where the data y consist of n iid observations. For example, small-sample bias of MLEs is noted in the normal and uniform examples, and it is seen that the likelihood equation for the uniform model has no solution because the likelihood is not differentiable at the MLE. In the Cauchy example, it is seen that the likelihood can have multiple maxima. In the final example, the binormal mixture model is shown to have a non-identifiable parameterization. Controlled Vocabulary Terms Bernoulli distribution; Cauchy distribution; density function; likelihood},
  isbn = {978-0-470-09484-6},
  langid = {english},
  keywords = {density function,iid Bernoulli,iid binormal mixture model,iid Cauchy,iid normal,iid uniform,maximum likelihood estimate (MLE)},
  file = {/home/thomas/Zotero/storage/77NIJUZH/9780470094846.html}
}

@incollection{2011c,
  title = {Hypothesis {{Tests}} and {{Confidence Intervals}} or {{Regions}}},
  booktitle = {Maximum {{Likelihood Estimation}} and {{Inference}}},
  date = {2011},
  pages = {37--63},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9780470094846.ch3},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470094846.ch3},
  urldate = {2023-12-23},
  abstract = {This chapter demonstrates how R and SAS can be used to perform hypothesis tests and to construct confidence intervals or regions. Inference is demonstrated using the large-sample approximate normality of maximum likelihood estimators (MLEs) (the Wald approach), and using the approximate χ\textsuperscript{2} distribution of likelihood ratio statistics. Hypothesis tests and confidence intervals/regions are considered for a single element θ\textsubscript{k} of the parameter vector, θ, and for a subset of (or all) elements of θ. There is undeniable virtue in the simplicity of the Wald approach, and in many cases it will make little difference compared to using the likelihood ratio. To encourage use of the likelihood ratio, the chapter includes demonstration of the R function Plkhci and SAS macro of the same name for construction of likelihood ratio confidence intervals. Controlled Vocabulary Terms confidence interval; hypothesis testing; likelihood ratio test; maximum likelihood estimator; profile likelihood; Wald test},
  isbn = {978-0-470-09484-6},
  langid = {english},
  keywords = {confidence intervals,hypothesis tests,likelihood ratio tests,maximum likelihood estimator (MLE),profile likelihood,R function,SAS macro,Wald tests},
  file = {/home/thomas/Zotero/storage/L5E3F4HB/9780470094846.html}
}

@incollection{2011d,
  title = {What You {{Really}} Need to {{Know}}},
  booktitle = {Maximum {{Likelihood Estimation}} and {{Inference}}},
  date = {2011},
  pages = {64--100},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9780470094846.ch4},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470094846.ch4},
  urldate = {2023-12-23},
  abstract = {This chapter delves into some of the important issues that are encountered in practice of estimation and inference. It commences with the delta method used for determining the approximating normal distribution of functions of the maximum likelihood estimator (MLE). Then, the chapter takes a closer look at ML inference based on approximate normality, and compares it to the use of likelihood ratio. It presents an example to look at the fallibility of the Wald statistics. The chapter illustrates that inference based on the likelihood ratio is generally more reliable, but comes at greater computational expense. Next, the chapter discusses model selection criteria, presenting the bootstrap as a computationally intensive method useful for inference in data-poor and nonstandard situations. The bootstrap also provides one of several practical methodologies for prediction. The chapter ends with a brief look at a variety of situations under which ML inference can go astray. Controlled Vocabulary Terms bootstrap methods; delta method; likelihood ratio test; maximum likelihood estimator; Wald test},
  isbn = {978-0-470-09484-6},
  langid = {english},
  keywords = {Bootstrap prediction,convergence,delta method,likelihood ratio,maximum likelihood estimator (MLE),ML inference,multiple maxima,Wald statistics},
  file = {/home/thomas/Zotero/storage/HRDBBFXR/9780470094846.html}
}

@incollection{2011e,
  title = {Maximizing the {{Likelihood}}},
  booktitle = {Maximum {{Likelihood Estimation}} and {{Inference}}},
  date = {2011},
  pages = {101--120},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9780470094846.ch5},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470094846.ch5},
  urldate = {2023-12-23},
  abstract = {In the general case, finding a maximum likelihood estimator (MLE) will require numerical maximization of the log-likelihood. The general-purpose optimizers are unlikely to be successful when applied to mixture models with many component distributions. For this purpose the expectation-maximization (EM) algorithm is better suited. When using Newton-Raphson type algorithms, most optimizers are able to approximate the required first and second derivatives numerically. This chapter looks at maximizing the likelihood in stages, and it is seen that the use of profile likelihood can sometimes be an extremely efficient method for reducing the dimensionality of a numerical optimization. In high-dimensional problems it can be a challenge to obtain good starting values for an optimizer, and it is seen that one way to overcome this is to build up to maximization of the log-likelihood in steps. The chapter concludes with a simple example showing the functionality of ADMB for implementing such multi-stage optimization. Controlled Vocabulary Terms estimation-maximization algorithm; maximum likelihood estimator; multi-stage estimation; Newton-Raphson method},
  isbn = {978-0-470-09484-6},
  langid = {english},
  keywords = {ADMB software,expectation-maximization (EM) algorithm,maximum likelihood estimator (MLE),multi-stage maximization,multi-stage optimization,Newton-Raphson algorithm},
  file = {/home/thomas/Zotero/storage/TJTZLNMA/9780470094846.html}
}

@incollection{2011f,
  title = {Some {{Widely Used Applications}} of {{Maximum Likelihood}}},
  booktitle = {Maximum {{Likelihood Estimation}} and {{Inference}}},
  date = {2011},
  pages = {121--142},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9780470094846.ch6},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470094846.ch6},
  urldate = {2023-12-23},
  abstract = {This chapter presents three examples chosen from the diverse myriad of applications of maximum likelihood inference. The aim is to give a flavour for the nature of the likelihoods used in each of these applications. The chapter presents the Box-Cox transformation as a very elegant application of profile likelihood. The total number of model parameters may be large, but numerical optimization is required over only the sole parameter that determines the optimal power transformation. The chapter looks very briefly at survival analysis. To be generally applicable, methods of analysis for survival-time data must be able to accommodate censored data. Such data routinely arise due to an individual being lost from the study prior to failure. The chapter presents some simple forms of mark-recapture models. These models have the unusual feature that the parameter of primary interest is the unknown number of animals in a population, and therefore is integer valued. Controlled Vocabulary Terms censored data; numerical method; population; power transformation; profile likelihood; survival analysis},
  isbn = {978-0-470-09484-6},
  langid = {english},
  keywords = {Box-Cox transformations,censored data,mark-recapture models,maximum likelihood inference,numerical optimization,population,power transformation,profile likelihood,survival analysis,survival-time data},
  file = {/home/thomas/Zotero/storage/C3LQ4PGR/9780470094846.html}
}

@incollection{2011g,
  title = {Generalized {{Linear Models}} and {{Extensions}}},
  booktitle = {Maximum {{Likelihood Estimation}} and {{Inference}}},
  date = {2011},
  pages = {143--174},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9780470094846.ch7},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470094846.ch7},
  urldate = {2023-12-23},
  abstract = {From the likelihood point of view, generalized linear models (GLMs) can simply be regarded as a flexible class of models, all of which share a convenient form of likelihood function. This chapter presents the general form of the likelihood equations that underlie the maximum likelihood modelling of exponential family data. It presents methods for model evaluation and comparison. The model that is used in the first case study in the chapter is a standard logistic regression on proportion data, and no obvious problems with the model fit are found. Often proportion data and count data will not be well modelled by a binomial distribution and a Poisson distribution. The chapter presents practical strategies for this situation, including the use of natural generalizations of the binomial and Poisson distributions, and the use of quasi-likelihood. It ends with a second case study that employs these strategies in an analysis of count data. Controlled Vocabulary Terms generalised additive model; generalized binomial distribution; likelihood ratio test; logistic regression; Poisson distribution; quasi-likelihood},
  isbn = {978-0-470-09484-6},
  langid = {english},
  keywords = {additive models,binomial distributions,generalized linear models (GLMs),inverse prediction,likelihood ratio tests,logistic regression,model evaluation tools,multiplicative models,Poisson distributions},
  file = {/home/thomas/Zotero/storage/4GYEKHRH/9780470094846.html}
}

@incollection{2011h,
  title = {Quasi-{{Likelihood}} and {{Generalized Estimating Equations}}},
  booktitle = {Maximum {{Likelihood Estimation}} and {{Inference}}},
  date = {2011},
  pages = {175--187},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9780470094846.ch8},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470094846.ch8},
  urldate = {2023-12-23},
  abstract = {This chapter presents the more general form of quasi-likelihood that was popularized by Wedderburn. Liang and Zeger showed that Wedderburn’s quasi-likelihoood approach could be extended to data that are correlated, and in particular, to the analysis of grouped data. Such data could be obtained from repeated measurements on randomly chosen experimental subjects. This approach is commonly known as the method of generalized estimating equations (GEEs), and is demonstrated in an analysis of drug efficacy data in which observations are grouped within clinics. Quasi-likelihood and GEEs enable the modeler to make inference about θ from specification of only the mean and variance (matrix) of the data. The analysis of leaf blotch data, in the chapter, demonstrates a situation where it is unclear how to specify a likelihood for proportion data. A binomial model does not suit these data, because the proportion is measured by visual inspection. Controlled Vocabulary Terms generalized estimating equations; quasi-likelihood},
  isbn = {978-0-470-09484-6},
  langid = {english},
  keywords = {barley blotch data,generalized estimating equations (GEEs),Wedderburn’s quasi-likelihood approach},
  file = {/home/thomas/Zotero/storage/UWNVTBWC/9780470094846.html}
}

@incollection{2011i,
  title = {{{ML Inference}} in the {{Presence}} of {{Incidental Parameters}}},
  booktitle = {Maximum {{Likelihood Estimation}} and {{Inference}}},
  date = {2011},
  pages = {188--201},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9780470094846.ch9},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470094846.ch9},
  urldate = {2023-12-23},
  abstract = {This chapter presents methodology to protect the maximum likelihood (ML) estimators of specified parameters from undesirable consequences that might arise from the estimation of other parameters. The mixed-effects analysis includes a demonstration of the integrated likelihood. The approach taken in the chapter is to use a modified form of the likelihood function for inference about ψ. It is desired to obtain a form of likelihood that is a function of ψ alone, and which encapsulates the information about ψ that is present in the (standard) likelihood L(θ). The focus in the chapter is on conditional likelihood, due to its theoretical underpinning and its wide use in mixed-effects modelling where it is more commonly known as restricted maximum likelihood (REML). The chapter presents the paired t-test in the context of modelling paired normally distributed data, and shows that it is an application of conditional inference. Controlled Vocabulary Terms conditional likelihood; incidental parameters; maximum likelihood estimator; REML},
  isbn = {978-0-470-09484-6},
  langid = {english},
  keywords = {conditional likelihood,incidental parameters,integrated likelihood,maximum likelihood (ML) inference,mixed-effects modelling,restricted maximum likelihood (REML)},
  file = {/home/thomas/Zotero/storage/HA6CGFT6/9780470094846.html}
}

@incollection{2011j,
  title = {Latent {{Variable Models}}},
  booktitle = {Maximum {{Likelihood Estimation}} and {{Inference}}},
  date = {2011},
  pages = {202--232},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9780470094846.ch10},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470094846.ch10},
  urldate = {2023-12-23},
  abstract = {Latent variable models provide a rich and widely-used collection of models that are especially suited for inference when the data are observed from experiments where more than one source of variability is present. Latent variable models include semi-parametric regression models, by virtue of the mixed-effects model formulation of penalized smoothing splines. They also include mixture models, and the binormal mixture model can be used to develop the notation for likelihood-based inference from latent variable models. This chapter focuses on demonstration of the current capabilities of R, SAS and ADMB for maximum likelihood inference from a likelihood. It presents examples of linear mixed models, nonlinear mixed models, generalized linear mixed models (GLMMs), and Poisson state-space models, respectively. The accuracy of Laplace approximation can be improved by the use of importance sampling. In addition, for low-dimensional integrals, Gauss-Hermite quadrature can be used to provide a higher order of approximation. Controlled Vocabulary Terms Laplace approximation; latent variable; random effects model},
  isbn = {978-0-470-09484-6},
  langid = {english},
  keywords = {Gauss-Hermite quadrature,generalized linear mixed-effects model (GLMM),Laplace approximation,latent variable models,likelihood-based inference,nonlinear mixed-effects model,one-way linear random-effects model,state-space model},
  file = {/home/thomas/Zotero/storage/H354L7NY/9780470094846.html}
}

@incollection{2011k,
  title = {Cramér-{{Rao Inequality}} and {{Fisher Information}}},
  booktitle = {Maximum {{Likelihood Estimation}} and {{Inference}}},
  date = {2011},
  pages = {233--255},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9780470094846.ch11},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470094846.ch11},
  urldate = {2023-12-23},
  abstract = {This chapter develops the theory under which maximum likelihood can be regarded as an optimal methodology. It is the Cramér-Rao (CR) inequality that establishes the minimum variance that can be achieved by unbiased estimators, and the key concept in the statement of this inequality is the expected Fisher information. Fisher information is a central concept in the theory of maximum likelihood (ML) inference, and much of the chapter is aimed at providing familiarity with Fisher information and its properties. The CR inequality and (expected) Fisher information are initially presented in the one-dimensional case, in which case the Fisher information is a scalar quantity. The chapter extends the inequality to unbiased estimators, and provides alternative formulae for calculation of Fisher information. The alternative formulae for the calculation of expected Fisher information extend to the multi-parameter case in a natural way, and are stated in the chapter. Controlled Vocabulary Terms maximum likelihood estimator; minimum variance; unbiased estimator},
  isbn = {978-0-470-09484-6},
  langid = {english},
  keywords = {Fisher information,maximum likelihood (ML) inference,minimum variance,multiparameter Cramér-Rao (CR) inequality,unbiased estimators},
  file = {/home/thomas/Zotero/storage/DBRTJGQA/9780470094846.html}
}

@incollection{2011l,
  title = {Asymptotic {{Theory}} and {{Approximate Normality}}},
  booktitle = {Maximum {{Likelihood Estimation}} and {{Inference}}},
  date = {2011},
  pages = {256--285},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9780470094846.ch12},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470094846.ch12},
  urldate = {2023-12-23},
  abstract = {The central-limit theorem for maximum likelihood estimators is derived in this chapter for the case of iid data. This is obtained in three steps, via two Lemmas and a theorem. The theorem uses a Taylor series expansion of the derivative of the log-likelihood function (score function) to obtain the asymptotic normality of a√n-standardized sequence of maximum likelihood estimators. The chapter provides a translation of the central-limit result into a more pragmatic interpretation based on approximate normality, and shows how this can be put to good use. Generalized estimating equations (GEEs) are a form of M-estimator, and their approximate normality is considered in the chapter. The chapter looks at the construction of asymptotically correct hypothesis tests and confidence intervals/regions. It presents the theory of likelihood ratio tests and confidence intervals, and provides brief coverage of the lesser-used Rao-score statistic. Controlled Vocabulary Terms asymptotic theory; central-limit theorem; confidence interval; estimator; generalized estimating equations; likelihood ratio; statistics; Taylor series expansion},
  isbn = {978-0-470-09484-6},
  langid = {english},
  keywords = {approximate normality,asymptotic normality,asymptotic theory,confidence interval,generalized estimating equations,likelihood ratio test statistic,maximum likelihood estimators (MLEs),Rao-score test statistic,Taylor series expansion},
  file = {/home/thomas/Zotero/storage/TI3RKPR8/9780470094846.html}
}

@incollection{2011m,
  title = {Tools of the {{Trade}}},
  booktitle = {Maximum {{Likelihood Estimation}} and {{Inference}}},
  date = {2011},
  pages = {286--298},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9780470094846.ch13},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470094846.ch13},
  urldate = {2023-12-23},
  abstract = {Test statistics can be used to construct confidence intervals, and vice-versa. This chapter formalizes the equivalency between hypothesis tests and confidence intervals. The mean and variance identities given in the chapter are very useful if it is difficult to obtain these moments directly from the distribution of Y, but the distribution of X, and of Y given X, are of convenient form. Two inequalities, Jensen’s inequality for convex functions and the Cauchy-Schwarz inequality, are explained in the chapter. Indeed, the Cramér-Rao inequality reduces to an application of the Cauchy-Schwarz inequality. The chapter discusses the asymptotic probability theory, and focuses on the concept of convergence for sequences of random variables. It provides the tools to derive the behaviour of this sequence of random variables (after suitable standardization) as n tends to infinity. Controlled Vocabulary Terms asymptotic normality; Cauchy-Schwarz inequality; confidence interval; equivalence testing; hypothesis testing; Jensen`s inequality; mean; variance},
  isbn = {978-0-470-09484-6},
  langid = {english},
  keywords = {asymptotic probability theory,Cauchy-Schwarz inequality,confidence intervals,equivalency,hypothesis tests,Jensen’s inequality,mean identities,variance conditional identities},
  file = {/home/thomas/Zotero/storage/DVI2DLAH/9780470094846.html}
}

@incollection{2011n,
  title = {Fundamental {{Paradigms}} and {{Principles}} of {{Inference}}},
  booktitle = {Maximum {{Likelihood Estimation}} and {{Inference}}},
  date = {2011},
  pages = {299--312},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9780470094846.ch14},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470094846.ch14},
  urldate = {2023-12-23},
  abstract = {The Bayesian approach departs markedly from the frequentist and fiducial approaches, by viewing probability as an expression of belief. This chapter outlines some of the considerations behind the justifications of an appropriate paradigm for statistic inference. It takes a look at three principles, namely, the sufficiency, conditionality and likelihood principles. Birnbaum caused a statistical furor between frequentist and Bayesian statisticians with his ground-breaking work on the interplay between these relationships. It is the flavour of this controversy that the chapter attempts to capture. Royall provides a more in-depth coverage of the relationship between statistical significance and statistical evidence, and Lehmann considers other strategies for circumventing some of the shortcomings of Neyman-Pearson tests of hypotheses. Controlled Vocabulary Terms Bayesian inference; conditionality principle; frequentist approach; likelihood principle; Neyman-Pearson lemma; significance; sufficiency},
  isbn = {978-0-470-09484-6},
  langid = {english},
  keywords = {Bayesian approach,conditionality principle,fiducial approach,frequentist approach,likelihood principle,Neyman-Pearson tests,statistical evidence,statistical significance,sufficiency principle},
  file = {/home/thomas/Zotero/storage/RANHA37F/9780470094846.html}
}

@incollection{2011o,
  title = {Miscellanea},
  booktitle = {Maximum {{Likelihood Estimation}} and {{Inference}}},
  date = {2011},
  pages = {313--324},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9780470094846.ch15},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470094846.ch15},
  urldate = {2023-12-23},
  abstract = {This chapter begins with a notation section that gives basic information of all the parameters used in the book; a table of Greek symbols used in the book is also provided. Then, a list of acronyms is given. Most of the distributions used in the book are listed in the chapter. The description of discrete distributions and continuous distributions includes the specification of its density function, its mean and variance, and a very brief explanation of its common usage. One of the recurring themes in the book is that it is preferable, where practicably possible, to work directly with the likelihood function for the purpose of conducting hypothesis tests and calculating confidence intervals. The software utilities described in chapter are provided for this purpose. The chapter ends with a brief note on automatic differentiation. Controlled Vocabulary Terms Bayesian inference; confidence interval; differentiation; discrete distribution; frequentist approach; hypothesis testing},
  isbn = {978-0-470-09484-6},
  langid = {english},
  keywords = {automatic differentiation,Bayesian approach,confidence intervals,continuous distributions,discrete distributions,frequentist approach,hypothesis tests},
  file = {/home/thomas/Zotero/storage/6MS5J356/9780470094846.html}
}

@incollection{2011p,
  title = {Appendix: {{Partial Solutions}} to {{Selected Exercises}}},
  shorttitle = {Appendix},
  booktitle = {Maximum {{Likelihood Estimation}} and {{Inference}}},
  date = {2011},
  pages = {325--336},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9780470094846.app1},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470094846.app1},
  urldate = {2023-12-23},
  abstract = {This appendix contains sections titled: Chapter 1 Chapter 2 Chapter 3 Chapter 4 Chapter 5 Chapter 6 Chapter 7 Chapter 8 Chapter 11 Chapter 12 Chapter 13 Chapter 14 Chapter 15},
  isbn = {978-0-470-09484-6},
  langid = {english},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/2011p.pdf;/home/thomas/Zotero/storage/HM594NYR/9780470094846.html}
}

@incollection{2011q,
  title = {Bibliography},
  booktitle = {Maximum {{Likelihood Estimation}} and {{Inference}}},
  date = {2011},
  pages = {337--344},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9780470094846.biblio},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470094846.biblio},
  urldate = {2023-12-23},
  isbn = {978-0-470-09484-6},
  langid = {english},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/2011q.pdf;/home/thomas/Zotero/storage/HARQQMSZ/9780470094846.html}
}

@incollection{2011r,
  title = {Index},
  booktitle = {Maximum {{Likelihood Estimation}} and {{Inference}}},
  date = {2011},
  pages = {345--357},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9780470094846.index},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470094846.index},
  urldate = {2023-12-23},
  isbn = {978-0-470-09484-6},
  langid = {english},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/2011r.pdf;/home/thomas/Zotero/storage/9Z28FN4K/9780470094846.html}
}

@incollection{2011s,
  title = {Statistics in {{Practice}}},
  booktitle = {Maximum {{Likelihood Estimation}} and {{Inference}}},
  date = {2011},
  pages = {358--359},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9780470094846.scard},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470094846.scard},
  urldate = {2023-12-23},
  isbn = {978-0-470-09484-6},
  langid = {english},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/2011s.pdf;/home/thomas/Zotero/storage/D6BR5PGH/9780470094846.html}
}

@book{bishop2006,
  title = {Pattern {{Recognition}} and {{Machine Learning}}},
  author = {Bishop, Christopher M.},
  date = {2006-08-17},
  eprint = {kTNoQgAACAAJ},
  eprinttype = {googlebooks},
  publisher = {{Springer}},
  abstract = {This is the first textbook on pattern recognition to present the Bayesian viewpoint. The book presents approximate inference algorithms that permit fast approximate answers in situations where exact answers are not feasible. It uses graphical models to describe probability distributions when no other books apply graphical models to machine learning. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory.},
  isbn = {978-0-387-31073-2},
  langid = {english},
  pagetotal = {738},
  keywords = {Computers / Computer Graphics,Computers / Computer Vision \& Pattern Recognition,Computers / Intelligence (AI) \& Semantics,Mathematics / Probability \& Statistics / General}
}

@online{devlin2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  date = {2019-05-24},
  eprint = {1810.04805},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1810.04805},
  url = {http://arxiv.org/abs/1810.04805},
  urldate = {2023-12-05},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  pubstate = {preprint},
  version = {2},
  keywords = {Computer Science - Computation and Language},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/devlin2019.pdf;/home/thomas/Zotero/storage/KEMTIW53/1810.html}
}

@article{ge2014,
  title = {Optimized {{Product Quantization}}},
  author = {Ge, Tiezheng and He, Kaiming and Ke, Qifa and Sun, Jian},
  date = {2014-04},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {36},
  number = {4},
  pages = {744--755},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2013.240},
  url = {https://ieeexplore.ieee.org/document/6678503},
  urldate = {2023-11-13},
  abstract = {Product quantization (PQ) is an effective vector quantization method. A product quantizer can generate an exponentially large codebook at very low memory/time cost. The essence of PQ is to decompose the high-dimensional vector space into the Cartesian product of subspaces and then quantize these subspaces separately. The optimal space decomposition is important for the PQ performance, but still remains an unaddressed issue. In this paper, we optimize PQ by minimizing quantization distortions w.r.t the space decomposition and the quantization codebooks. We present two novel solutions to this challenging optimization problem. The first solution iteratively solves two simpler sub-problems. The second solution is based on a Gaussian assumption and provides theoretical analysis of the optimality. We evaluate our optimized product quantizers in three applications: (i) compact encoding for exhaustive ranking [1], (ii) building inverted multi-indexing for non-exhaustive search [2], and (iii) compacting image representations for image retrieval [3]. In all applications our optimized product quantizers outperform existing solutions.},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/ge2014.pdf;/home/thomas/Zotero/storage/3HGULA2E/6678503.html}
}

@online{greene2019,
  title = {Econometric {{Analysis}}},
  author = {Greene, William},
  date = {2019},
  publisher = {{Pearson Deutschland}},
  url = {https://elibrary.pearson.de/book/99.150005/9781292231150},
  urldate = {2023-12-23},
  abstract = {{$<$}br{$><$}h4{$>$}For first-year graduate courses in Econometrics for Social Scientists.{$<$}/h4{$>$} {$<$}br{$><$}p{$>$}Bridging the gap between social science studies and econometric analysis Designed to bridge the gap between social science studies and field-econometrics, Econometric Analysis, 8th Edition, Global Edition, presents this ever-growing area at an accessible graduate level. The book first introduces students to basic techniques, a rich variety of models, and underlying theory that is easy to put into practice. It then presents students with a sufficient theoretical background to understand advanced techniques and to recognise new variants of established models. This focus, along with hundreds of worked numerical examples, ensures that students can apply the theory to real-world application and are prepared to be successful economists in the field.{$<$}/p{$>$} {$<$}br{$><$}br{$>$}},
  isbn = {9781292231150},
  langid = {english},
  file = {/home/thomas/Zotero/storage/NE6XIYDM/9781292231150.html}
}

@book{grimmett1986,
  title = {Probability: {{An Introduction}}},
  shorttitle = {Probability},
  author = {Grimmett, Geoffrey and Welsh, D. J. A.},
  date = {1986},
  eprint = {DyifaCLXxkIC},
  eprinttype = {googlebooks},
  publisher = {{Clarendon Press}},
  abstract = {This new undergraduate text offers a concise introduction to probability and random processes. Exercises and problems range from simple to difficult, and the overall treatment, though elementary, includes rigorous mathematical arguments. Chapters contain core material for a beginning course in probability, a treatment of joint distributions leading to accounts of moment-generating functions, the law of large numbers and the central limit theorem, and basic random processes.},
  isbn = {978-0-19-853264-4},
  langid = {english},
  pagetotal = {230}
}

@online{guo2015,
  title = {Quantization Based {{Fast Inner Product Search}}},
  author = {Guo, Ruiqi and Kumar, Sanjiv and Choromanski, Krzysztof and Simcha, David},
  date = {2015-09-04},
  eprint = {1509.01469},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1509.01469},
  url = {http://arxiv.org/abs/1509.01469},
  urldate = {2023-11-13},
  abstract = {We propose a quantization based approach for fast approximate Maximum Inner Product Search (MIPS). Each database vector is quantized in multiple subspaces via a set of codebooks, learned directly by minimizing the inner product quantization error. Then, the inner product of a query to a database vector is approximated as the sum of inner products with the subspace quantizers. Different from recently proposed LSH approaches to MIPS, the database vectors and queries do not need to be augmented in a higher dimensional feature space. We also provide a theoretical analysis of the proposed approach, consisting of the concentration results under mild assumptions. Furthermore, if a small sample of example queries is given at the training time, we propose a modified codebook learning procedure which further improves the accuracy. Experimental results on a variety of datasets including those arising from deep neural networks show that the proposed approach significantly outperforms the existing state-of-the-art.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/guo2015.pdf;/home/thomas/Zotero/storage/PUC7YB59/1509.html}
}

@online{guo2020,
  title = {Accelerating {{Large-Scale Inference}} with {{Anisotropic Vector Quantization}}},
  author = {Guo, Ruiqi and Sun, Philip and Lindgren, Erik and Geng, Quan and Simcha, David and Chern, Felix and Kumar, Sanjiv},
  date = {2020-12-04},
  eprint = {1908.10396},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1908.10396},
  url = {http://arxiv.org/abs/1908.10396},
  urldate = {2023-11-13},
  abstract = {Quantization based techniques are the current state-of-the-art for scaling maximum inner product search to massive databases. Traditional approaches to quantization aim to minimize the reconstruction error of the database points. Based on the observation that for a given query, the database points that have the largest inner products are more relevant, we develop a family of anisotropic quantization loss functions. Under natural statistical assumptions, we show that quantization with these loss functions leads to a new variant of vector quantization that more greatly penalizes the parallel component of a datapoint's residual relative to its orthogonal component. The proposed approach achieves state-of-the-art results on the public benchmarks available at \textbackslash url\{ann-benchmarks.com\}.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/guo2020.pdf;/home/thomas/Zotero/storage/E6AVEC3F/1908.html}
}

@online{hornrogara,
  title = {Matrix Analysis 2nd Edition | {{Algebra}} | {{Cambridge University Press}}},
  author = {Horn, Rogar A},
  url = {https://www.cambridge.org/de/universitypress/subjects/mathematics/algebra/matrix-analysis-2nd-edition?format=PB&isbn=9780521548236},
  urldate = {2023-11-29},
  abstract = {Linear algebra and matrix theory are fundamental tools in mathematical and physical science, as well as fertile fields for research. This second edition of this acclaimed text presents results of both classic and recent matrix analysis using canonical forms as a unifying theme and demonstrates their importance in a variety of applications. This thoroughly revised and updated second edition is a text for a second course on linear algebra and has more than 1,100 problems and exercises, new sections on the singular value and CS decompositions and the Weyr canonical form, expanded treatments of inverse problems and of block matrices, and much more.},
  file = {/home/thomas/Downloads/Roger A. Horn, Charles R. Johnson - Matrix Analysis-Cambridge University Press (2013).pdf;/home/thomas/Zotero/storage/S4UW9BI7/matrix-analysis-2nd-edition.html}
}

@inproceedings{jadon2020,
  title = {A Survey of Loss Functions for Semantic Segmentation},
  booktitle = {2020 {{IEEE Conference}} on {{Computational Intelligence}} in {{Bioinformatics}} and {{Computational Biology}} ({{CIBCB}})},
  author = {Jadon, Shruti},
  date = {2020-10},
  pages = {1--7},
  doi = {10.1109/CIBCB48159.2020.9277638},
  url = {https://ieeexplore.ieee.org/abstract/document/9277638},
  urldate = {2023-12-16},
  abstract = {Image Segmentation has been an active field of research as it has a wide range of applications, ranging from automated disease detection to self driving cars. In the past five years, various papers came up with different objective loss functions used in different cases such as biased data, sparse segmentation, etc. In this paper, we have summarized some of the well-known loss functions widely used for Image Segmentation and listed out the cases where their usage can help in fast and better convergence of a model. Furthermore, we have also introduced a new log-cosh dice loss function and compared its performance on NBFS skull-segmentation open source data-set with widely used loss functions. We also showcased that certain loss functions perform well across all data-sets and can be taken as a good baseline choice in unknown data distribution scenarios.},
  eventtitle = {2020 {{IEEE Conference}} on {{Computational Intelligence}} in {{Bioinformatics}} and {{Computational Biology}} ({{CIBCB}})},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/jadon2020.pdf;/home/thomas/Zotero/storage/S689HGHR/9277638.html}
}

@inproceedings{jadon2020a,
  title = {A Survey of Loss Functions for Semantic Segmentation},
  booktitle = {2020 {{IEEE Conference}} on {{Computational Intelligence}} in {{Bioinformatics}} and {{Computational Biology}} ({{CIBCB}})},
  author = {Jadon, Shruti},
  date = {2020-10},
  pages = {1--7},
  doi = {10.1109/CIBCB48159.2020.9277638},
  url = {https://ieeexplore.ieee.org/abstract/document/9277638},
  urldate = {2023-12-18},
  abstract = {Image Segmentation has been an active field of research as it has a wide range of applications, ranging from automated disease detection to self driving cars. In the past five years, various papers came up with different objective loss functions used in different cases such as biased data, sparse segmentation, etc. In this paper, we have summarized some of the well-known loss functions widely used for Image Segmentation and listed out the cases where their usage can help in fast and better convergence of a model. Furthermore, we have also introduced a new log-cosh dice loss function and compared its performance on NBFS skull-segmentation open source data-set with widely used loss functions. We also showcased that certain loss functions perform well across all data-sets and can be taken as a good baseline choice in unknown data distribution scenarios.},
  eventtitle = {2020 {{IEEE Conference}} on {{Computational Intelligence}} in {{Bioinformatics}} and {{Computational Biology}} ({{CIBCB}})},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/jadon2020a.pdf;/home/thomas/Zotero/storage/6J6QWMHQ/9277638.html}
}

@article{jegou2011,
  title = {Product {{Quantization}} for {{Nearest Neighbor Search}}},
  author = {Jégou, Herve and Douze, Matthijs and Schmid, Cordelia},
  date = {2011-01},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {33},
  number = {1},
  pages = {117--128},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2010.57},
  url = {https://ieeexplore.ieee.org/document/5432202},
  urldate = {2023-11-13},
  abstract = {This paper introduces a product quantization-based approach for approximate nearest neighbor search. The idea is to decompose the space into a Cartesian product of low-dimensional subspaces and to quantize each subspace separately. A vector is represented by a short code composed of its subspace quantization indices. The euclidean distance between two vectors can be efficiently estimated from their codes. An asymmetric version increases precision, as it computes the approximate distance between a vector and a code. Experimental results show that our approach searches for nearest neighbors efficiently, in particular in combination with an inverted file system. Results for SIFT and GIST image descriptors show excellent search accuracy, outperforming three state-of-the-art approaches. The scalability of our approach is validated on a data set of two billion vectors.},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/jegou2011.pdf;/home/thomas/Zotero/storage/7T72YLS5/5432202.html}
}

@book{kwak2004,
  title = {Linear {{Algebra}}},
  author = {Kwak, Jin Ho and Hong, Sungpyo},
  date = {2004},
  publisher = {{Birkhäuser}},
  location = {{Boston, MA}},
  doi = {10.1007/978-0-8176-8194-4},
  url = {http://link.springer.com/10.1007/978-0-8176-8194-4},
  urldate = {2023-11-28},
  isbn = {978-0-8176-4294-5 978-0-8176-8194-4},
  langid = {english},
  keywords = {algebra,computer,computer science,Eigenvalue,Eigenvector,linear algebra,Matrix,matrix theory,Transformation},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/kwak2004.pdf}
}

@book{lange2019,
  title = {Hebbian Learning Approaches Based on General Inner Products and Distance Measures in Non-{{Euclidean}} Spaces},
  author = {Lange, Mandy},
  date = {2019},
  publisher = {{University of Groningen}},
  location = {{[Groningen]}},
  abstract = {The topic of this thesis is to define a unified and generalized scheme for Hebbian approaches in non-Euclidean spaces for unsupervised and supervised learning. This can be realized in different ways. One possibility is the replacement of the inner product by a semi-inner product (SIP). A SIP relaxes the strict properties of an inner product but preserves the linear aspect in the first argument. Thus, these SIPs are natural equivalents of inner products generating Banach spaces instead of Hilbert spaces for inner products. In this work SIPs for Banach spaces are considered for unsupervised Hebbian like learning approaches. Further, the learning scheme of the supervised Learning Vector Quantization (LVQ) network, which is originally designed for applications in Euclidean data space, can be interpreted under specific circumstances as a Hebbian like learning, too. It is shown that, non-Euclidean metrics applied in LVQ can improve the performance of classification learning compared to Euclidean variants.The previously addressed Hebbian learning methods are vectorial approaches. However, if the data space is a vector space of matrices equipped with a respective matrix norm, then matrix approaches for Hebbian like learning methods become of interest. The extension of these methods in non-Euclidean spaces of matrices to process matrix data is the last main point of this thesis.},
  isbn = {978-94-034-1470-6},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/lange2019.pdf}
}

@online{lin2018,
  title = {Focal {{Loss}} for {{Dense Object Detection}}},
  author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr},
  date = {2018-02-07},
  eprint = {1708.02002},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1708.02002},
  url = {http://arxiv.org/abs/1708.02002},
  urldate = {2023-12-16},
  abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/lin2018.pdf;/home/thomas/Zotero/storage/4YJ9LBZT/1708.html}
}

@online{lingle2023,
  title = {Transformer-{{VQ}}: {{Linear-Time Transformers}} via {{Vector Quantization}}},
  shorttitle = {Transformer-{{VQ}}},
  author = {Lingle, Lucas D.},
  date = {2023-09-28},
  eprint = {2309.16354},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2309.16354},
  url = {http://arxiv.org/abs/2309.16354},
  urldate = {2023-11-13},
  abstract = {We introduce Transformer-VQ, a decoder-only transformer computing softmax-based dense self-attention in linear time. Transformer-VQ's efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In large-scale experiments, Transformer-VQ is shown highly competitive in quality, with strong results on Enwik8 (0.99 bpb), PG-19 (26.6 ppl), and ImageNet64 (3.16 bpb). Code: https://github.com/transformer-vq/transformer\_vq},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/lingle2023.pdf;/home/thomas/Zotero/storage/AMZDGPVR/2309.html}
}

@article{lumer1961,
  title = {Semi-Inner-Product Spaces},
  author = {Lumer, G.},
  date = {1961},
  journaltitle = {Transactions of the American Mathematical Society},
  shortjournal = {Trans. Amer. Math. Soc.},
  volume = {100},
  number = {1},
  pages = {29--43},
  issn = {0002-9947, 1088-6850},
  doi = {10.1090/S0002-9947-1961-0133024-2},
  url = {https://www.ams.org/tran/1961-100-01/S0002-9947-1961-0133024-2/},
  urldate = {2023-11-15},
  abstract = {Advancing research. Creating connections.},
  langid = {english},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/lumer1961.pdf}
}

@online{millar2011,
  title = {Maximum {{Likelihood Estimation}} and {{Inference}}: {{With Examples}} in {{R}}, {{SAS}} and {{ADMB}} | {{Wiley}}},
  shorttitle = {Maximum {{Likelihood Estimation}} and {{Inference}}},
  author = {{millar}},
  date = {2011},
  url = {https://www.wiley.com/en-us/Maximum+Likelihood+Estimation+and+Inference%3A+With+Examples+in+R%2C+SAS+and+ADMB-p-9780470094822},
  urldate = {2023-12-23},
  abstract = {This book takes a fresh look at the popular and well-established method of maximum likelihood for statistical estimation and inference. It begins with an intuitive introduction to the concepts and background of likelihood, and moves through to the latest developments in maximum likelihood methodology, including general latent variable models and new material for the practical implementation of integrated likelihood using the free ADMB software. Fundamental issues of statistical inference are also examined, with a presentation of some of the philosophical debates underlying the choice of statistical paradigm. Key features: Provides an accessible introduction to pragmatic maximum likelihood modelling. Covers more advanced topics, including general forms of latent variable models (including non-linear and non-normal mixed-effects and state-space models) and the use of maximum likelihood variants, such as estimating equations, conditional likelihood, restricted likelihood and integrated likelihood. Adopts a practical approach, with a focus on providing the relevant tools required by researchers and practitioners who collect and analyze real data. Presents numerous examples and case studies across a wide range of applications including medicine, biology and ecology. Features applications from a range of disciplines, with implementation in R, SAS and/or ADMB. Provides all program code and software extensions on a supporting website. Confines supporting theory to the final chapters to maintain a readable and pragmatic focus of the preceding chapters. This book is not just an accessible and practical text about maximum likelihood, it is a comprehensive guide to modern maximum likelihood estimation and inference. It will be of interest to readers of all levels, from novice to expert. It will be of great benefit to researchers, and to students of statistics from senior undergraduate to graduate level. For use as a course text, exercises are provided at the end of each chapter.},
  langid = {american},
  organization = {{Wiley.com}},
  file = {/home/thomas/Zotero/storage/ASYFM2KI/Maximum+Likelihood+Estimation+and+Inference+With+Examples+in+R,+SAS+and+ADMB-p-9780470094822.html}
}

@article{miura2011,
  title = {An {{Introduction}} to {{Maximum Likelihood Estimation}} and {{Information Geometry}}},
  author = {Miura, Keiji},
  date = {2011-11-30},
  journaltitle = {Interdisciplinary Information Sciences (IIS)},
  shortjournal = {Interdisciplinary Information Sciences (IIS)},
  volume = {17},
  doi = {10.4036/iis.2011.155},
  abstract = {In this paper, we review the maximum likelihood method for estimating the statistical parameters which specify a probabilistic model and show that it generally gives an optimal estimator with minimum mean square error asymptotically. Thus, for most applications in information sciences, the maximum likelihood estimation suffices. Fisher information matrix, which defines the orthogonality between parameters in a probabilistic model, naturally arises from the maximum likelihood estimation. As the inverse of the Fisher information matrix gives the covariance matrix for the estimation errors of the parameters, the orthogonalization of the parameters guarantees that the estimates of the parameters distribute independently from each other. The theory of information geometry provides procedures to diagonalize parameters globally or at all parameter values at least for the exponential and mixture families of distributions. The global orthogonalization gives a simplified and better view for statistical inference and, for example, makes it possible to perform a statistical test for each unknown parameter separately. Therefore, for practical applications, a good start is to examine if the probabilistic model under study belongs to these families.},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/miura2011.pdf}
}

@inproceedings{nam2014,
  title = {Large-{{Scale Multi-label Text Classification}} — {{Revisiting Neural Networks}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Nam, Jinseok and Kim, Jungi and Loza Mencía, Eneldo and Gurevych, Iryna and Fürnkranz, Johannes},
  editor = {Calders, Toon and Esposito, Floriana and Hüllermeier, Eyke and Meo, Rosa},
  date = {2014},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {437--452},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-44851-9_28},
  abstract = {Neural networks have recently been proposed for multi-label classification because they are able to capture and model label dependencies in the output layer. In this work, we investigate limitations of BP-MLL, a neural network (NN) architecture that aims at minimizing pairwise ranking error. Instead, we propose to use a comparably simple NN approach with recently proposed learning techniques for large-scale multi-label text classification tasks. In particular, we show that BP-MLL’s ranking loss minimization can be efficiently and effectively replaced with the commonly used cross entropy error function, and demonstrate that several advances in neural network training that have been developed in the realm of deep learning can be effectively employed in this setting. Our experimental results show that simple NN models equipped with advanced techniques such as rectified linear units, dropout, and AdaGrad perform as well as or even outperform state-of-the-art approaches on six large-scale textual datasets with diverse characteristics.},
  isbn = {978-3-662-44851-9},
  langid = {english},
  keywords = {Cross Entropy,Hide Layer,Hide Unit,Single Hide Layer,Stochastic Gradient Descent},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/nam2014.pdf}
}

@article{nelder1972,
  title = {Generalized {{Linear Models}}},
  author = {Nelder, J. A. and Wedderburn, R. W. M.},
  date = {1972},
  journaltitle = {Journal of the Royal Statistical Society. Series A (General)},
  volume = {135},
  number = {3},
  eprint = {2344614},
  eprinttype = {jstor},
  pages = {370--384},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0035-9238},
  doi = {10.2307/2344614},
  url = {https://www.jstor.org/stable/2344614},
  urldate = {2023-12-23},
  abstract = {The technique of iterative weighted linear regression can be used to obtain maximum likelihood estimates of the parameters with observations distributed according to some exponential family and systematic effects that can be made linear by a suitable transformation. A generalization of the analysis of variance is given for these models using log-likelihoods. These generalized linear models are illustrated by examples relating to four distributions; the Normal, Binomial (probit analysis, etc.), Poisson (contingency tables) and gamma (variance components). The implications of the approach in designing statistics courses are discussed.}
}

@article{ramos2018,
  title = {Deconstructing {{Cross-Entropy}} for {{Probabilistic Binary Classifiers}}},
  author = {Ramos, Daniel and Franco-Pedroso, Javier and Lozano-Diez, Alicia and Gonzalez-Rodriguez, Joaquin},
  date = {2018-03},
  journaltitle = {Entropy},
  volume = {20},
  number = {3},
  pages = {208},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {1099-4300},
  doi = {10.3390/e20030208},
  url = {https://www.mdpi.com/1099-4300/20/3/208},
  urldate = {2023-12-15},
  abstract = {In this work, we analyze the cross-entropy function, widely used in classifiers both as a performance measure and as an optimization objective. We contextualize cross-entropy in the light of Bayesian decision theory, the formal probabilistic framework for making decisions, and we thoroughly analyze its motivation, meaning and interpretation from an information-theoretical point of view. In this sense, this article presents several contributions: First, we explicitly analyze the contribution to cross-entropy of (i) prior knowledge; and (ii) the value of the features in the form of a likelihood ratio. Second, we introduce a decomposition of cross-entropy into two components: discrimination and calibration. This decomposition enables the measurement of different performance aspects of a classifier in a more precise way; and justifies previously reported strategies to obtain reliable probabilities by means of the calibration of the output of a discriminating classifier. Third, we give different information-theoretical interpretations of cross-entropy, which can be useful in different application scenarios, and which are related to the concept of reference probabilities. Fourth, we present an analysis tool, the Empirical Cross-Entropy (ECE) plot, a compact representation of cross-entropy and its aforementioned decomposition. We show the power of ECE plots, as compared to other classical performance representations, in two diverse experimental examples: a speaker verification system, and a forensic case where some glass findings are present.},
  issue = {3},
  langid = {english},
  keywords = {Bayesian,calibration,classifier,cross-entropy,discrimination,ECE plot,probabilistic},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/ramos2018.pdf}
}

@online{reddi2018,
  title = {Stochastic {{Negative Mining}} for {{Learning}} with {{Large Output Spaces}}},
  author = {Reddi, Sashank J. and Kale, Satyen and Yu, Felix and Holtmann-Rice, Dan and Chen, Jiecao and Kumar, Sanjiv},
  date = {2018-10-16},
  eprint = {1810.07076},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1810.07076},
  url = {http://arxiv.org/abs/1810.07076},
  urldate = {2023-11-13},
  abstract = {We consider the problem of retrieving the most relevant labels for a given input when the size of the output space is very large. Retrieval methods are modeled as set-valued classifiers which output a small set of classes for each input, and a mistake is made if the label is not in the output set. Despite its practical importance, a statistically principled, yet practical solution to this problem is largely missing. To this end, we first define a family of surrogate losses and show that they are calibrated and convex under certain conditions on the loss parameters and data distribution, thereby establishing a statistical and analytical basis for using these losses. Furthermore, we identify a particularly intuitive class of loss functions in the aforementioned family and show that they are amenable to practical implementation in the large output space setting (i.e. computation is possible without evaluating scores of all labels) by developing a technique called Stochastic Negative Mining. We also provide generalization error bounds for the losses in the family. Finally, we conduct experiments which demonstrate that Stochastic Negative Mining yields benefits over commonly used negative sampling approaches.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/reddi2018.pdf;/home/thomas/Zotero/storage/W6JVNWF3/1810.html}
}

@book{vasudeva2017,
  title = {Elements of {{Hilbert Spaces}} and {{Operator Theory}}},
  author = {Vasudeva, Harkrishan Lal},
  date = {2017},
  publisher = {{Springer}},
  location = {{Singapore}},
  doi = {10.1007/978-981-10-3020-8},
  url = {http://link.springer.com/10.1007/978-981-10-3020-8},
  urldate = {2023-11-14},
  isbn = {978-981-10-3019-2 978-981-10-3020-8},
  langid = {english},
  keywords = {Banach Spaces,Finite Dimensional Spaces,Functional analysis,Linear operators,Operator theory,Riesz Lemma,Special theory},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/vasudeva2017.pdf}
}

@online{vaswani2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2023-08-01},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2023-11-17},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/vaswani2023.pdf;/home/thomas/Zotero/storage/SCJWFGAA/1706.html}
}

@online{vaswani2023a,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2023-08-01},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2023-12-18},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/vaswani2023a.pdf;/home/thomas/Zotero/storage/METTBEQP/1706.html}
}

@book{young1988,
  title = {An {{Introduction}} to {{Hilbert Space}}},
  author = {Young, N.},
  date = {1988},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge}},
  doi = {10.1017/CBO9781139172011},
  url = {https://www.cambridge.org/core/books/an-introduction-to-hilbert-space/41D80B163B8E186820B5A48F3665EC22},
  urldate = {2023-11-15},
  abstract = {This textbook is an introduction to the theory of Hilbert space and its applications. The notion of Hilbert space is central in functional analysis and is used in numerous branches of pure and applied mathematics. Dr Young has stressed applications of the theory, particularly to the solution of partial differential equations in mathematical physics and to the approximation of functions in complex analysis. Some basic familiarity with real analysis, linear algebra and metric spaces is assumed, but otherwise the book is self-contained. It is based on courses given at the University of Glasgow and contains numerous examples and exercises (many with solutions). Thus it will make an excellent first course in Hilbert space theory at either undergraduate or graduate level and will also be of interest to electrical engineers and physicists, particularly those involved in control theory and filter design.},
  isbn = {978-0-521-33717-5},
  file = {/home/thomas/Zotero/storage/Q72XTLTI/41D80B163B8E186820B5A48F3665EC22.html}
}

@online{zotero-55,
  title = {Zotero | {{Your}} Personal Research Assistant},
  url = {https://www.zotero.org/},
  urldate = {2023-12-15}
}

@book{zotero-76,
  title = {Pattern {{Recognition}} and {{Machine Learning}}},
  url = {https://link.springer.com/book/9780387310732},
  urldate = {2023-12-18},
  langid = {english},
  file = {/home/thomas/Zotero/storage/UNPPN5J7/9780387310732.html}
}

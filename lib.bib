@article{albertsson2018,
  title = {Machine {{Learning}} in {{High Energy Physics Community White Paper}}},
  author = {Albertsson, Kim and Altoe, Piero and Anderson, Dustin and Andrews, Michael and Espinosa, Juan Pedro Araque and Aurisano, Adam and Basara, Laurent and Bevan, Adrian and Bhimji, Wahid and Bonacorsi, Daniele and Calafiura, Paolo and Campanelli, Mario and Capps, Louis and Carminati, Federico and Carrazza, Stefano and Childers, Taylor and Coniavitis, Elias and Cranmer, Kyle and David, Claire and Davis, Douglas and Duarte, Javier and Erdmann, Martin and Eschle, Jonas and Farbin, Amir and Feickert, Matthew and Castro, Nuno Filipe and Fitzpatrick, Conor and Floris, Michele and Forti, Alessandra and Garra-Tico, Jordi and Gemmler, Jochen and Girone, Maria and Glaysher, Paul and Gleyzer, Sergei and Gligorov, Vladimir and Golling, Tobias and Graw, Jonas and Gray, Lindsey and Greenwood, Dick and Hacker, Thomas and Harvey, John and Hegner, Benedikt and Heinrich, Lukas and Hooberman, Ben and Junggeburth, Johannes and Kagan, Michael and Kane, Meghan and Kanishchev, Konstantin and Karpiński, Przemysław and Kassabov, Zahari and Kaul, Gaurav and Kcira, Dorian and Keck, Thomas and Klimentov, Alexei and Kowalkowski, Jim and Kreczko, Luke and Kurepin, Alexander and Kutschke, Rob and Kuznetsov, Valentin and Köhler, Nicolas and Lakomov, Igor and Lannon, Kevin and Lassnig, Mario and Limosani, Antonio and Louppe, Gilles and Mangu, Aashrita and Mato, Pere and Meinhard, Helge and Menasce, Dario and Moneta, Lorenzo and Moortgat, Seth and Narain, Meenakshi and Neubauer, Mark and Newman, Harvey and Pabst, Hans and Paganini, Michela and Paulini, Manfred and Perdue, Gabriel and Perez, Uzziel and Picazio, Attilio and Pivarski, Jim and Prosper, Harrison and Psihas, Fernanda and Radovic, Alexander and Reece, Ryan and Rinkevicius, Aurelius and Rodrigues, Eduardo and Rorie, Jamal and Rousseau, David and Sauers, Aaron and Schramm, Steven and Schwartzman, Ariel and Severini, Horst and Seyfert, Paul and Siroky, Filip and Skazytkin, Konstantin and Sokoloff, Mike and Stewart, Graeme and Stienen, Bob and Stockdale, Ian and Strong, Giles and Thais, Savannah and Tomko, Karen and Upfal, Eli and Usai, Emanuele and Ustyuzhanin, Andrey and Vala, Martin and Vallecorsa, Sofia and Vasel, Justin and Verzetti, Mauro and Vilasís-Cardona, Xavier and Vlimant, Jean-Roch and Vukotic, Ilija and Wang, Sean-Jiun and Watts, Gordon and Williams, Michael and Wu, Wenjing and Wunsch, Stefan and Zapata, Omar},
  date = {2018-09},
  journaltitle = {J. Phys.: Conf. Ser.},
  volume = {1085},
  number = {2},
  pages = {022008},
  publisher = {IOP Publishing},
  issn = {1742-6596},
  doi = {10.1088/1742-6596/1085/2/022008},
  url = {https://dx.doi.org/10.1088/1742-6596/1085/2/022008},
  urldate = {2024-01-05},
  abstract = {Machine learning is an important applied research area in particle physics, beginning with applications to high-level physics analysis in the 1990s and 2000s, followed by an explosion of applications in particle and event identification and reconstruction in the 2010s. In this document we discuss promising future research and development areas in machine learning in particle physics with a roadmap for their implementation, software and hardware resource requirements, collaborative initiatives with the data science community, academia and industry, and training the particle physics community in data science. The main objective of the document is to connect and motivate these areas of research and development with the physics drivers of the High-Luminosity Large Hadron Collider and future neutrino experiments and identify the resource needs for their implementation. Additionally we identify areas where collaboration with external communities will be of great benefit.},
  langid = {english},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/albertsson2018.pdf}
}

@inproceedings{anderson1977,
  title = {Distinctive Features, Categorical Perception, and Probability Learning: {{Some}} Applications of a Neural Model.},
  shorttitle = {Distinctive Features, Categorical Perception, and Probability Learning},
  booktitle = {Psychological {{Review}}},
  author = {Anderson, James A. and Silverstein, Jack W. and Ritz, Stephen A. and Jones, Randall S.},
  date = {1977-09},
  volume = {84},
  number = {5},
  pages = {413--451},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/0033-295X.84.5.413},
  url = {https://doi.apa.org/doi/10.1037/0033-295X.84.5.413},
  urldate = {2024-07-27},
  abstract = {A previously proposed model for memory based on neurophysiolo gical considerations is reviewed. We assume that (a) nervous system activity is usefully represented as the set of simultaneous individual neuron activities in a group of neurons; (b) different memory traces make use of the same synapses; and (c) synapses associate two patterns of neural activity by incrementing synaptic connectivity proportionally to the product of pre- and postsynaptic activity, forming a matrix of synaptic connectivities. We extend this model by (a) introducing positive feedback of a set of neurons onto itself and (b) allowing the individual neurons to saturate. A hybrid model, partly analog and partly binary, arises. The system has certain characteristics reminiscent of analysis by distinctive features. Next, we apply the model to "categorical perception." Finally, we discuss probability learning. The model can predict overshooting, recency data, and probabilities occurring in systems with more than two events with reasonably good accuracy.},
  langid = {english}
}

@book{apostol1967,
  title = {Calculus, Vol. 2: {{Multi-variable}} Calculus and Linear Algebra with Applications to Differential Equations and Probability},
  author = {Apostol, Tom M.},
  date = {1967},
  publisher = {J. Wiley},
  location = {New York},
  isbn = {0-471-00005-1 978-0-471-00005-1 0-471-00007-8 978-0-471-00007-5},
  keywords = {calculus textbook}
}

@online{beck2024,
  title = {{{xLSTM}}: {{Extended Long Short-Term Memory}}},
  shorttitle = {{{xLSTM}}},
  author = {Beck, Maximilian and Pöppel, Korbinian and Spanring, Markus and Auer, Andreas and Prudnikova, Oleksandra and Kopp, Michael and Klambauer, Günter and Brandstetter, Johannes and Hochreiter, Sepp},
  date = {2024-05-07},
  eprint = {2405.04517},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2405.04517},
  url = {http://arxiv.org/abs/2405.04517},
  urldate = {2024-07-15},
  abstract = {In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/beck2024.pdf;/home/thomas/Zotero/storage/PNS37KYI/2405.html}
}

@online{ben-baruch2021,
  title = {Asymmetric {{Loss For Multi-Label Classification}}},
  author = {Ben-Baruch, Emanuel and Ridnik, Tal and Zamir, Nadav and Noy, Asaf and Friedman, Itamar and Protter, Matan and Zelnik-Manor, Lihi},
  date = {2021-07-29},
  eprint = {2009.14119},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2009.14119},
  url = {http://arxiv.org/abs/2009.14119},
  urldate = {2023-12-27},
  abstract = {In a typical multi-label setting, a picture contains on average few positive labels, and many negative ones. This positive-negative imbalance dominates the optimization process, and can lead to under-emphasizing gradients from positive labels during training, resulting in poor accuracy. In this paper, we introduce a novel asymmetric loss ("ASL"), which operates differently on positive and negative samples. The loss enables to dynamically down-weights and hard-thresholds easy negative samples, while also discarding possibly mislabeled samples. We demonstrate how ASL can balance the probabilities of different samples, and how this balancing is translated to better mAP scores. With ASL, we reach state-of-the-art results on multiple popular multi-label datasets: MS-COCO, Pascal-VOC, NUS-WIDE and Open Images. We also demonstrate ASL applicability for other tasks, such as single-label classification and object detection. ASL is effective, easy to implement, and does not increase the training time or complexity. Implementation is available at: https://github.com/Alibaba-MIIL/ASL.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,I.0,I.2.10,I.2.6,I.4.0},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/ben-baruch2021.pdf;/home/thomas/Zotero/storage/QGNK38MC/2009.html}
}

@book{bishop2006,
  title = {Pattern {{Recognition}} and {{Machine Learning}}},
  author = {Bishop, Christopher M.},
  date = {2006-08-17},
  eprint = {kTNoQgAACAAJ},
  eprinttype = {googlebooks},
  publisher = {Springer},
  abstract = {This is the first textbook on pattern recognition to present the Bayesian viewpoint. The book presents approximate inference algorithms that permit fast approximate answers in situations where exact answers are not feasible. It uses graphical models to describe probability distributions when no other books apply graphical models to machine learning. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory.},
  isbn = {978-0-387-31073-2},
  langid = {english},
  pagetotal = {738},
  keywords = {Computers / Computer Graphics,Computers / Computer Vision & Pattern Recognition,Computers / Intelligence (AI) & Semantics,Mathematics / Probability & Statistics / General}
}

@inproceedings{brakerski2012,
  title = {Fully {{Homomorphic Encryption}} without {{Modulus Switching}} from {{Classical GapSVP}}},
  booktitle = {Advances in {{Cryptology}} – {{CRYPTO}} 2012},
  author = {Brakerski, Zvika},
  editor = {Safavi-Naini, Reihaneh and Canetti, Ran},
  date = {2012},
  volume = {7417},
  pages = {868--886},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-32009-5_50},
  url = {http://link.springer.com/10.1007/978-3-642-32009-5_50},
  urldate = {2024-06-27},
  abstract = {We present a new tensoring technique for LWE-based fully homomorphic encryption. While in all previous works, the ciphertext noise grows quadratically \$\$B \textbackslash rightarrow B\textasciicircum 2\textbackslash cdot \textbackslash text \{poly\}n\$\$ with every multiplication before "refreshing", our noise only grows linearly \$\$B \textbackslash rightarrow B\textbackslash cdot \textbackslash text \{poly\}n\$\$.    We use this technique to construct a scale-invariant fully homomorphic encryption scheme, whose properties only depend on the ratio between the modulus q and the initial noise level B, and not on their absolute values.    Our scheme has a number of advantages over previous candidates: It uses the same modulus throughout the evaluation process no need for "modulus switching", and this modulus can take arbitrary form. In addition, security can be classically reduced from the worst-case hardness of the GapSVP problem with quasi-polynomial approximation factor, whereas previous constructions could only exhibit a quantum reduction from GapSVP.},
  isbn = {978-3-642-32008-8 978-3-642-32009-5},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/brakerski2012.pdf}
}

@book{briggs2019,
  title = {Calculus},
  author = {Briggs, William L. and Cochran, Lyle and Gillett, Bernard and Schulz, Eric P.},
  date = {2019},
  eprint = {DNUGtAEACAAJ},
  eprinttype = {googlebooks},
  publisher = {Pearson},
  abstract = {For 3- to 4-semester courses covering single-variable and multivariable calculus, taken by students of mathematics, engineering, natural sciences, or economics.  T he most successful new calculus text in the last two decades  The much-anticipated 3rd Edition of Briggs' Calculus Series  retains its hallmark features while introducing important advances and refinements. Briggs, Cochran, Gillett, and Schulz build from a foundation of meticulously crafted exercise sets, then draw students into the narrative through writing that reflects the voice of the instructor. Examples are stepped out and thoughtfully annotated, and figures are designed to teach rather than simply supplement the narrative. The groundbreaking eBook contains approximately 700 Interactive Figures that can be manipulated to shed light on key concepts.  For the 3rd Edition, the authors synthesized feedback on the text and MyLab(TM) Math content from over 140 instructors and an Engineering Review Panel. This thorough and extensive review process, paired with the authors' own teaching experiences, helped create a text that was designed for today's calculus instructors and students.  Also available with MyLab Math  MyLab Math is the teaching and learning platform that empowers instructors to reach every student. By combining trusted author content with digital tools and a flexible platform, MyLab Math personalizes the learning experience and improves results for each student.    Note: You are purchasing a standalone product; MyLab Math does not come packaged with this content. Students, if interested in purchasing this title with MyLab Math, ask your instructor to confirm the correct package ISBN and Course ID. Instructors, contact your Pearson representative for more information.    If you would like to purchase both the physical text and MyLab Math, search for:  0134996720 / 9780134996721 Calculus and MyLab Math with Pearson eText - Title-Specific Access Card Package, 3/e Package consists of:  013476563X / 9780134765631 Calculus 013485683X / 9780134856834 MyLab Math with Pearson eText - Standalone Access Card - for Calculus},
  isbn = {978-0-13-476563-1},
  langid = {english},
  pagetotal = {1344}
}

@book{brogan1991,
  title = {Modern Control Theory (3rd Ed.)},
  author = {Brogan, William L.},
  date = {1991-02},
  publisher = {Prentice-Hall, Inc.},
  location = {USA},
  isbn = {978-0-13-589763-8},
  pagetotal = {653}
}

@book{brualdi2004,
  title = {Introductory {{Combinatorics}}},
  author = {Brualdi, Richard A.},
  date = {2004-01-01},
  edition = {Subsequent edition},
  publisher = {Pearson College Div},
  location = {Upper Saddle River, N.J},
  abstract = {This book emphasizes combinatorial ideas including the pigeon-hole principle, counting techniques, permutations and combinations, Pólya counting, binomial coefficients, inclusion-exclusion principle, generating functions and recurrence relations, and combinatortial structures (matchings, designs, graphs). The volume provides a complete examination of combinatorial ideas and techniques. For individuals interested in combinatorial concepts.},
  isbn = {978-0-13-100119-0},
  langid = {english},
  pagetotal = {608}
}

@online{conci2018,
  title = {Distance {{Between Sets}} - {{A}} Survey},
  author = {Conci, A. and Kubrusly, C. S.},
  date = {2018-08-07},
  eprint = {1808.02574},
  eprinttype = {arXiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.1808.02574},
  url = {http://arxiv.org/abs/1808.02574},
  urldate = {2024-02-21},
  abstract = {The purpose of this paper is to give a survey on the notions of distance between subsets either of a metric space or of a measure space, including definitions, a classification, and a discussion of the best-known distance functions, which is followed by a review on applications used in many areas of knowledge, ranging from theoretical to practical applications.},
  pubstate = {prepublished},
  keywords = {28A78 54E35,Mathematics - Functional Analysis},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/conci2018.pdf;/home/thomas/Zotero/storage/TTV8T363/1808.html}
}

@online{devlin2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  date = {2019-05-24},
  eprint = {1810.04805},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1810.04805},
  url = {http://arxiv.org/abs/1810.04805},
  urldate = {2023-12-05},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  pubstate = {prepublished},
  version = {2},
  keywords = {Computer Science - Computation and Language},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/devlin2019.pdf;/home/thomas/Zotero/storage/KEMTIW53/1810.html}
}

@book{folland2009,
  title = {Fourier {{Analysis}} and {{Its Applications}}},
  author = {Folland, Gerald B.},
  date = {2009-01-13},
  publisher = {American Mathematical Society},
  location = {Providence, RI},
  abstract = {This book presents the theory and applications of Fourier series and integrals, eigenfunction expansions, and related topics, on a level suitable for advanced undergraduates. It includes material on Bessel functions, orthogonal polynomials, and Laplace transforms, and it concludes with chapters on generalized functions and Green's functions for ordinary and partial differential equations. The book deals almost exclusively with aspects of these subjects that are useful in physics and engineering, and includes a wide variety of applications. On the theoretical side, it uses ideas from modern analysis to develop the concepts and reasoning behind the techniques without getting bogged down in the technicalities of rigorous proofs.},
  isbn = {978-0-8218-4790-9},
  langid = {english},
  pagetotal = {433}
}

@article{ge2014,
  title = {Optimized {{Product Quantization}}},
  author = {Ge, Tiezheng and He, Kaiming and Ke, Qifa and Sun, Jian},
  date = {2014-04},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {36},
  number = {4},
  pages = {744--755},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2013.240},
  url = {https://ieeexplore.ieee.org/document/6678503},
  urldate = {2023-11-13},
  abstract = {Product quantization (PQ) is an effective vector quantization method. A product quantizer can generate an exponentially large codebook at very low memory/time cost. The essence of PQ is to decompose the high-dimensional vector space into the Cartesian product of subspaces and then quantize these subspaces separately. The optimal space decomposition is important for the PQ performance, but still remains an unaddressed issue. In this paper, we optimize PQ by minimizing quantization distortions w.r.t the space decomposition and the quantization codebooks. We present two novel solutions to this challenging optimization problem. The first solution iteratively solves two simpler sub-problems. The second solution is based on a Gaussian assumption and provides theoretical analysis of the optimality. We evaluate our optimized product quantizers in three applications: (i) compact encoding for exhaustive ranking [1], (ii) building inverted multi-indexing for non-exhaustive search [2], and (iii) compacting image representations for image retrieval [3]. In all applications our optimized product quantizers outperform existing solutions.},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/ge2014.pdf;/home/thomas/Zotero/storage/3HGULA2E/6678503.html}
}

@misc{gentry2009,
  title = {Fully Homomorphic Encryption Using Ideal Lattices.},
  author = {Gentry, Craig},
  date = {2009},
  url = {crypto.stanford.edu/craig},
  organization = {Stanford University}
}

@inproceedings{gentry2013,
  title = {Homomorphic {{Encryption}} from {{Learning}} with {{Errors}}: {{Conceptually-Simpler}}, {{Asymptotically-Faster}}, {{Attribute-Based}}},
  shorttitle = {Homomorphic {{Encryption}} from {{Learning}} with {{Errors}}},
  booktitle = {Advances in {{Cryptology}} – {{CRYPTO}} 2013},
  author = {Gentry, Craig and Sahai, Amit and Waters, Brent},
  editor = {Canetti, Ran and Garay, Juan A.},
  date = {2013},
  pages = {75--92},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-40041-4_5},
  abstract = {We describe a comparatively simple fully homomorphic encryption (FHE) scheme based on the learning with errors (LWE) problem. In previous LWE-based FHE schemes, multiplication is a complicated and expensive step involving “relinearization”. In this work, we propose a new technique for building FHE schemes that we call the approximate eigenvector method. In our scheme, for the most part, homomorphic addition and multiplication are just matrix addition and multiplication. This makes our scheme both asymptotically faster and (we believe) easier to understand.},
  isbn = {978-3-642-40041-4},
  langid = {english},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/gentry2013.pdf}
}

@online{greene2019,
  title = {Econometric {{Analysis}}},
  author = {Greene, William},
  date = {2019},
  publisher = {Pearson Deutschland},
  url = {https://elibrary.pearson.de/book/99.150005/9781292231150},
  urldate = {2023-12-23},
  abstract = {{$<$}br{$><$}h4{$>$}For first-year graduate courses in Econometrics for Social Scientists.{$<$}/h4{$>$} {$<$}br{$><$}p{$>$}Bridging the gap between social science studies and econometric analysis Designed to bridge the gap between social science studies and field-econometrics, Econometric Analysis, 8th Edition, Global Edition, presents this ever-growing area at an accessible graduate level. The book first introduces students to basic techniques, a rich variety of models, and underlying theory that is easy to put into practice. It then presents students with a sufficient theoretical background to understand advanced techniques and to recognise new variants of established models. This focus, along with hundreds of worked numerical examples, ensures that students can apply the theory to real-world application and are prepared to be successful economists in the field.{$<$}/p{$>$} {$<$}br{$><$}br{$>$}},
  isbn = {9781292231150},
  langid = {english},
  file = {/home/thomas/Zotero/storage/NE6XIYDM/9781292231150.html}
}

@book{grimmett1986,
  title = {Probability: {{An Introduction}}},
  shorttitle = {Probability},
  author = {Grimmett, Geoffrey and Welsh, D. J. A.},
  date = {1986},
  eprint = {DyifaCLXxkIC},
  eprinttype = {googlebooks},
  publisher = {Clarendon Press},
  abstract = {This new undergraduate text offers a concise introduction to probability and random processes. Exercises and problems range from simple to difficult, and the overall treatment, though elementary, includes rigorous mathematical arguments. Chapters contain core material for a beginning course in probability, a treatment of joint distributions leading to accounts of moment-generating functions, the law of large numbers and the central limit theorem, and basic random processes.},
  isbn = {978-0-19-853264-4},
  langid = {english},
  pagetotal = {230}
}

@article{gritsenko2017,
  title = {Neural {{Distributed Autoassociative Memories}}: {{A Survey}}},
  shorttitle = {Neural {{Distributed Autoassociative Memories}}},
  author = {Gritsenko, V. I. and Rachkovskij, D. A. and Frolov, A. A. and Gayler, R. and Kleyko, D. and Osipov, E.},
  date = {2017-06-12},
  journaltitle = {Kibern. vyčisl. teh.},
  volume = {2017},
  eprint = {1709.00848},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {5--35},
  issn = {04549910, 25192205},
  doi = {10.15407/kvt188.02.005},
  url = {http://arxiv.org/abs/1709.00848},
  urldate = {2024-07-27},
  abstract = {Introduction. Neural network models of autoassociative, distributed memory allow storage and retrieval of many items (vectors) where the number of stored items can exceed the vector dimension (the number of neurons in the network). This opens the possibility of a sublinear time search (in the number of stored items) for approximate nearest neighbors among vectors of high dimension. The purpose of this paper is to review models of autoassociative, distributed memory that can be naturally implemented by neural networks (mainly with local learning rules and iterative dynamics based on information locally available to neurons). Scope. The survey is focused mainly on the networks of Hopfield, Willshaw and Potts, that have connections between pairs of neurons and operate on sparse binary vectors. We discuss not only autoassociative memory, but also the generalization properties of these networks. We also consider neural networks with higher-order connections and networks with a bipartite graph structure for non-binary data with linear constraints. Conclusions. In conclusion we discuss the relations to similarity search, advantages and drawbacks of these techniques, and topics for further research. An interesting and still not completely resolved question is whether neural autoassociative memories can search for approximate nearest neighbors faster than other index structures for similarity search, in particular for the case of very high dimensional vectors.},
  issue = {2(188)},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/gritsenko2017.pdf;/home/thomas/Zotero/storage/T5IK487F/1709.html}
}

@online{gu2022,
  title = {Efficiently {{Modeling Long Sequences}} with {{Structured State Spaces}}},
  author = {Gu, Albert and Goel, Karan and Ré, Christopher},
  date = {2022-08-05},
  eprint = {2111.00396},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2111.00396},
  url = {http://arxiv.org/abs/2111.00396},
  urldate = {2024-01-01},
  abstract = {A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of \$10000\$ or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) \textbackslash ( x'(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t) \textbackslash ), and showed that for appropriate choices of the state matrix \textbackslash ( A \textbackslash ), this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning \textbackslash ( A \textbackslash ) with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91\textbackslash\% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation \$60\textbackslash times\$ faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/gu2022.pdf;/home/thomas/Zotero/storage/LGUIB4RK/2111.html}
}

@online{gu2023,
  title = {Mamba: {{Linear-Time Sequence Modeling}} with {{Selective State Spaces}}},
  shorttitle = {Mamba},
  author = {Gu, Albert and Dao, Tri},
  date = {2023-12-01},
  eprint = {2312.00752},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.00752},
  url = {http://arxiv.org/abs/2312.00752},
  urldate = {2023-12-31},
  abstract = {Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5\$\textbackslash times\$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/gu2023.pdf;/home/thomas/Zotero/storage/3SUGQ2QC/2312.html}
}

@online{guo2015,
  title = {Quantization Based {{Fast Inner Product Search}}},
  author = {Guo, Ruiqi and Kumar, Sanjiv and Choromanski, Krzysztof and Simcha, David},
  date = {2015-09-04},
  eprint = {1509.01469},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1509.01469},
  url = {http://arxiv.org/abs/1509.01469},
  urldate = {2023-11-13},
  abstract = {We propose a quantization based approach for fast approximate Maximum Inner Product Search (MIPS). Each database vector is quantized in multiple subspaces via a set of codebooks, learned directly by minimizing the inner product quantization error. Then, the inner product of a query to a database vector is approximated as the sum of inner products with the subspace quantizers. Different from recently proposed LSH approaches to MIPS, the database vectors and queries do not need to be augmented in a higher dimensional feature space. We also provide a theoretical analysis of the proposed approach, consisting of the concentration results under mild assumptions. Furthermore, if a small sample of example queries is given at the training time, we propose a modified codebook learning procedure which further improves the accuracy. Experimental results on a variety of datasets including those arising from deep neural networks show that the proposed approach significantly outperforms the existing state-of-the-art.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/guo2015.pdf;/home/thomas/Zotero/storage/PUC7YB59/1509.html}
}

@online{guo2020,
  title = {Accelerating {{Large-Scale Inference}} with {{Anisotropic Vector Quantization}}},
  author = {Guo, Ruiqi and Sun, Philip and Lindgren, Erik and Geng, Quan and Simcha, David and Chern, Felix and Kumar, Sanjiv},
  date = {2020-12-04},
  eprint = {1908.10396},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1908.10396},
  url = {http://arxiv.org/abs/1908.10396},
  urldate = {2023-11-13},
  abstract = {Quantization based techniques are the current state-of-the-art for scaling maximum inner product search to massive databases. Traditional approaches to quantization aim to minimize the reconstruction error of the database points. Based on the observation that for a given query, the database points that have the largest inner products are more relevant, we develop a family of anisotropic quantization loss functions. Under natural statistical assumptions, we show that quantization with these loss functions leads to a new variant of vector quantization that more greatly penalizes the parallel component of a datapoint's residual relative to its orthogonal component. The proposed approach achieves state-of-the-art results on the public benchmarks available at \textbackslash url\{ann-benchmarks.com\}.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/guo2020.pdf;/home/thomas/Zotero/storage/E6AVEC3F/1908.html}
}

@book{hall2015,
  title = {Lie {{Groups}}, {{Lie Algebras}}, and {{Representations}}: {{An Elementary Introduction}}},
  shorttitle = {Lie {{Groups}}, {{Lie Algebras}}, and {{Representations}}},
  author = {Hall, Brian C.},
  date = {2015},
  series = {Graduate {{Texts}} in {{Mathematics}}},
  volume = {222},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-13467-3},
  url = {https://link.springer.com/10.1007/978-3-319-13467-3},
  urldate = {2024-03-15},
  isbn = {978-3-319-13466-6 978-3-319-13467-3},
  langid = {english},
  keywords = {Baker-Campbell-Hausdorff formula,Cartan-Weyl theory,Lie algebras,Lie groups,representation theory},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/hall2015.pdf}
}

@online{hornrogara,
  title = {Matrix Analysis 2nd Edition | {{Algebra}} | {{Cambridge University Press}}},
  author = {Horn, Rogar A},
  url = {https://www.cambridge.org/de/universitypress/subjects/mathematics/algebra/matrix-analysis-2nd-edition?format=PB&isbn=9780521548236},
  urldate = {2023-11-29},
  abstract = {Linear algebra and matrix theory are fundamental tools in mathematical and physical science, as well as fertile fields for research. This second edition of this acclaimed text presents results of both classic and recent matrix analysis using canonical forms as a unifying theme and demonstrates their importance in a variety of applications. This thoroughly revised and updated second edition is a text for a second course on linear algebra and has more than 1,100 problems and exercises, new sections on the singular value and CS decompositions and the Weyr canonical form, expanded treatments of inverse problems and of block matrices, and much more.},
  file = {/home/thomas/Downloads/Roger A. Horn, Charles R. Johnson - Matrix Analysis-Cambridge University Press (2013).pdf;/home/thomas/Zotero/storage/S4UW9BI7/matrix-analysis-2nd-edition.html}
}

@book{howell2016,
  title = {Principles of {{Fourier Analysis}}},
  author = {Howell, Kenneth B.},
  date = {2016-12-12},
  edition = {2nd edition},
  publisher = {CRC Press},
  abstract = {Fourier analysis is one of the most useful and widely employed sets of tools for the engineer, the scientist, and the applied mathematician. As such, students and practitioners in these disciplines need a practical and mathematically solid introduction to its principles. They need straightforward verifications of its results and formulas, and they need clear indications of the limitations of those results and formulas.Principles of Fourier Analysis furnishes all this and more. It provides a comprehensive overview of the mathematical theory of Fourier analysis, including the development of Fourier series, "classical" Fourier transforms, generalized Fourier transforms and analysis, and the discrete theory. Much of the author's development is strikingly different from typical presentations. His approach to defining the classical Fourier transform results in a much cleaner, more coherent theory that leads naturally to a starting point for the generalized theory. He also introduces a new generalized theory based on the use of Gaussian test functions that yields an even more general -yet simpler -theory than usually presented.Principles of Fourier Analysis stimulates the appreciation and understanding of the fundamental concepts and serves both beginning students who have seen little or no Fourier analysis as well as the more advanced students who need a deeper understanding. Insightful, non-rigorous derivations motivate much of the material, and thought-provoking examples illustrate what can go wrong when formulas are misused. With clear, engaging exposition, readers develop the ability to intelligently handle the more sophisticated mathematics that Fourier analysis ultimately requires.},
  langid = {english},
  pagetotal = {791}
}

@book{ibe2014,
  title = {Fundamentals of {{Applied Probability}} and {{Random Processes}}},
  author = {Ibe, Oliver},
  date = {2014-07-28},
  edition = {2nd edition},
  publisher = {Academic Press},
  location = {Amsterdam Boston},
  isbn = {978-0-12-800852-2},
  langid = {english},
  pagetotal = {456}
}

@inproceedings{jadon2020,
  title = {A Survey of Loss Functions for Semantic Segmentation},
  booktitle = {2020 {{IEEE Conference}} on {{Computational Intelligence}} in {{Bioinformatics}} and {{Computational Biology}} ({{CIBCB}})},
  author = {Jadon, Shruti},
  date = {2020-10},
  pages = {1--7},
  doi = {10.1109/CIBCB48159.2020.9277638},
  url = {https://ieeexplore.ieee.org/abstract/document/9277638},
  urldate = {2023-12-16},
  abstract = {Image Segmentation has been an active field of research as it has a wide range of applications, ranging from automated disease detection to self driving cars. In the past five years, various papers came up with different objective loss functions used in different cases such as biased data, sparse segmentation, etc. In this paper, we have summarized some of the well-known loss functions widely used for Image Segmentation and listed out the cases where their usage can help in fast and better convergence of a model. Furthermore, we have also introduced a new log-cosh dice loss function and compared its performance on NBFS skull-segmentation open source data-set with widely used loss functions. We also showcased that certain loss functions perform well across all data-sets and can be taken as a good baseline choice in unknown data distribution scenarios.},
  eventtitle = {2020 {{IEEE Conference}} on {{Computational Intelligence}} in {{Bioinformatics}} and {{Computational Biology}} ({{CIBCB}})},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/jadon2020.pdf;/home/thomas/Zotero/storage/S689HGHR/9277638.html}
}

@book{jammalamadaka2001,
  title = {Topics in {{Circular Statistics}}},
  author = {Jammalamadaka, S. Rao and Sengupta, Ambar and Sengupta, Ashis},
  date = {2001},
  eprint = {sKqWMGqQXQkC},
  eprinttype = {googlebooks},
  publisher = {World Scientific},
  abstract = {This research monograph on circular data analysis covers some recent advances in the field, besides providing a brief introduction to, and a review of, existing methods and models. The primary focus is on recent research into topics such as change-point problems, predictive distributions, circular correlation and regression, etc. An important feature of this work is the S-plus subroutines provided for analyzing actual data sets. Coupled with the discussion of new theoretical research, the book should benefit both the researcher and the practitioner. Contents: Circular Probability Distributions; Some Sampling Distributions; Estimation of Parameters; Tests for Mean Direction and Concentration; Tests for Uniformity; Nonparametric Testing Procedures; Circular Correlation and Regression; Predictive Inference for Directional Data; Outliers and Related Problems; Change-Point Problems; Miscellaneous Topics; Some Facts on Bessel Functions; How to Use the CircStats Package. Readership: Researchers and practitioners dealing with circular data.},
  isbn = {978-981-277-926-7},
  langid = {english},
  pagetotal = {348},
  keywords = {Mathematics / Differential Equations / General,Mathematics / Probability & Statistics / General}
}

@article{jegou2011,
  title = {Product {{Quantization}} for {{Nearest Neighbor Search}}},
  author = {Jégou, Herve and Douze, Matthijs and Schmid, Cordelia},
  date = {2011-01},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {33},
  number = {1},
  pages = {117--128},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2010.57},
  url = {https://ieeexplore.ieee.org/document/5432202},
  urldate = {2023-11-13},
  abstract = {This paper introduces a product quantization-based approach for approximate nearest neighbor search. The idea is to decompose the space into a Cartesian product of low-dimensional subspaces and to quantize each subspace separately. A vector is represented by a short code composed of its subspace quantization indices. The euclidean distance between two vectors can be efficiently estimated from their codes. An asymmetric version increases precision, as it computes the approximate distance between a vector and a code. Experimental results show that our approach searches for nearest neighbors efficiently, in particular in combination with an inverted file system. Results for SIFT and GIST image descriptors show excellent search accuracy, outperforming three state-of-the-art approaches. The scalability of our approach is validated on a data set of two billion vectors.},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/jegou2011.pdf;/home/thomas/Zotero/storage/7T72YLS5/5432202.html}
}

@book{judson2022,
  title = {Abstract {{Algebra}}: {{Theory}} and {{Applications}}},
  shorttitle = {Abstract {{Algebra}}},
  author = {Judson, Thomas},
  date = {2022-07-28},
  edition = {2022nd edition},
  publisher = {Orthogonal Publishing L3c},
  abstract = {Abstract Algebra: Theory and Applications is an open-source textbook that is designed to teach the principles and theory of abstract algebra to college juniors and seniors in a rigorous manner. Its strengths include a wide range of exercises, both computational and theoretical, plus many non-trivial applications. The first half of the book presents group theory, through the Sylow theorems, with enough material for a semester-long course. The second half is suitable for a second semester and presents rings, integral domains, Boolean algebras, vector spaces, and fields, concluding with Galois Theory.},
  isbn = {978-1-944325-16-9},
  langid = {english},
  pagetotal = {438}
}

@article{kalman1960,
  title = {A {{New Approach}} to {{Linear Filtering}} and {{Prediction Problems}}},
  author = {Kalman, R. E.},
  date = {1960-03-01},
  journaltitle = {Journal of Basic Engineering},
  doi = {10.1115/1.3662552},
  url = {https://www.scinapse.io/papers/2105934661},
  urldate = {2023-12-31},
  abstract = {R. E. Kalman},
  langid = {english}
}

@article{kohonen1972,
  title = {Correlation {{Matrix Memories}}},
  author = {Kohonen, Teuvo},
  date = {1972-04},
  journaltitle = {IEEE Transactions on Computers},
  volume = {C-21},
  number = {4},
  pages = {353--359},
  issn = {1557-9956},
  doi = {10.1109/TC.1972.5008975},
  url = {https://ieeexplore.ieee.org/document/5008975},
  urldate = {2024-07-27},
  abstract = {A new model for associative memory, based on a correlation matrix, is suggested. In this model information is accumulated on memory elements as products of component data. Denoting a key vector by q(p), and the data associated with it by another vector x(p), the pairs (q(p), x(p)) are memorized in the form of a matrix see the Equation in PDF File where c is a constant. A randomly selected subset of the elements of Mxq can also be used for memorizing. The recalling of a particular datum x(r) is made by a transformation x(r)=Mxqq(r). This model is failure tolerant and facilitates associative search of information; these are properties that are usually assigned to holographic memories. Two classes of memories are discussed: a complete correlation matrix memory (CCMM), and randomly organized incomplete correlation matrix memories (ICMM). The data recalled from the latter are stochastic variables but the fidelity of recall is shown to have a deterministic limit if the number of memory elements grows without limits. A special case of correlation matrix memories is the auto-associative memory in which any part of the memorized information can be used as a key. The memories are selective with respect to accumulated data. The ICMM exhibits adaptive improvement under certain circumstances. It is also suggested that correlation matrix memories could be applied for the classification of data.},
  eventtitle = {{{IEEE Transactions}} on {{Computers}}},
  keywords = {Associative memory,associative net,associative recall,Correlation,correlation matrix memory,Data mining,Finite element methods,Manganese,Mathematical model,Noise,nonholographic associative memory,pattern recognition,Reactive power},
  file = {/home/thomas/Zotero/storage/VBH4KUVE/5008975.html}
}

@book{kreyszig1978,
  title = {Introductory Functional Analysis with Applications},
  author = {Kreyszig, Erwin},
  date = {1978},
  series = {Wiley Classics Library},
  edition = {15. print},
  publisher = {Wiley},
  location = {New York, N.Y.},
  abstract = {Provides avenues for applying functional analysis to the practical study of natural sciences as well as mathematics. Contains worked problems on Hilbert space theory and on Banach spaces and emphasizes concepts, principles, methods and major applications of functional analysis.},
  isbn = {978-0-471-50459-7},
  langid = {english},
  pagetotal = {688},
  keywords = {Funktionalanalysis},
  annotation = {OCLC: 474285534}
}

@book{kreyszig2007,
  title = {Introductory Functional Analysis with Applications},
  author = {Kreyszig},
  date = {2007},
  series = {Wiley Classics Library},
  publisher = {Wiley India Pvt. Limited},
  url = {https://books.google.de/books?id=osXw-pRsptoC},
  isbn = {978-81-265-1191-4}
}

@inproceedings{kurz2014,
  title = {Efficient Evaluation of the Probability Density Function of a Wrapped Normal Distribution},
  booktitle = {2014 {{Sensor Data Fusion}}: {{Trends}}, {{Solutions}}, {{Applications}} ({{SDF}})},
  author = {Kurz, Gerhard and Gilitschenski, Igor and Hanebeck, Uwe D.},
  date = {2014-10},
  pages = {1--5},
  doi = {10.1109/SDF.2014.6954713},
  url = {https://ieeexplore.ieee.org/document/6954713},
  urldate = {2024-06-22},
  abstract = {The wrapped normal distribution arises when the density of a one-dimensional normal distribution is wrapped around the circle infinitely many times. At first look, evaluation of its probability density function appears tedious as an infinite series is involved. In this paper, we investigate the evaluation of two truncated series representations. As one representation performs well for small uncertainties, whereas the other performs well for large uncertainties, we show that in all cases a small number of summands is sufficient to achieve high accuracy.},
  eventtitle = {2014 {{Sensor Data Fusion}}: {{Trends}}, {{Solutions}}, {{Applications}} ({{SDF}})},
  keywords = {Accuracy,Approximation methods,Artificial neural networks,Robots},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/kurz2014.pdf;/home/thomas/Zotero/storage/KCFI48QX/6954713.html}
}

@book{kwak2004,
  title = {Linear {{Algebra}}},
  author = {Kwak, Jin Ho and Hong, Sungpyo},
  date = {2004},
  publisher = {Birkhäuser},
  location = {Boston, MA},
  doi = {10.1007/978-0-8176-8194-4},
  url = {http://link.springer.com/10.1007/978-0-8176-8194-4},
  urldate = {2023-11-28},
  isbn = {978-0-8176-4294-5 978-0-8176-8194-4},
  langid = {english},
  keywords = {algebra,computer,computer science,Eigenvalue,Eigenvector,linear algebra,Matrix,matrix theory,Transformation},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/kwak2004.pdf}
}

@book{lang1999,
  title = {Complex {{Analysis}}},
  author = {Lang, Serge},
  date = {1999},
  series = {Graduate {{Texts}} in {{Mathematics}}},
  volume = {103},
  publisher = {Springer},
  location = {New York, NY},
  doi = {10.1007/978-1-4757-3083-8},
  url = {http://link.springer.com/10.1007/978-1-4757-3083-8},
  urldate = {2024-03-13},
  isbn = {978-1-4419-3135-1 978-1-4757-3083-8},
  keywords = {calculus,Cauchy's integral formula,Complex analysis,differential equation,gamma function,Jensen's formula,maximum,Meromorphic function},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/lang1999.pdf}
}

@book{lange2019,
  title = {Hebbian Learning Approaches Based on General Inner Products and Distance Measures in Non-{{Euclidean}} Spaces},
  author = {Lange, Mandy},
  date = {2019},
  publisher = {University of Groningen},
  location = {[Groningen]},
  abstract = {The topic of this thesis is to define a unified and generalized scheme for Hebbian approaches in non-Euclidean spaces for unsupervised and supervised learning. This can be realized in different ways. One possibility is the replacement of the inner product by a semi-inner product (SIP). A SIP relaxes the strict properties of an inner product but preserves the linear aspect in the first argument. Thus, these SIPs are natural equivalents of inner products generating Banach spaces instead of Hilbert spaces for inner products. In this work SIPs for Banach spaces are considered for unsupervised Hebbian like learning approaches. Further, the learning scheme of the supervised Learning Vector Quantization (LVQ) network, which is originally designed for applications in Euclidean data space, can be interpreted under specific circumstances as a Hebbian like learning, too. It is shown that, non-Euclidean metrics applied in LVQ can improve the performance of classification learning compared to Euclidean variants.The previously addressed Hebbian learning methods are vectorial approaches. However, if the data space is a vector space of matrices equipped with a respective matrix norm, then matrix approaches for Hebbian like learning methods become of interest. The extension of these methods in non-Euclidean spaces of matrices to process matrix data is the last main point of this thesis.},
  isbn = {978-94-034-1470-6},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/lange2019.pdf}
}

@online{lin2018,
  title = {Focal {{Loss}} for {{Dense Object Detection}}},
  author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr},
  date = {2018-02-07},
  eprint = {1708.02002},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1708.02002},
  url = {http://arxiv.org/abs/1708.02002},
  urldate = {2023-12-16},
  abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/lin2018.pdf;/home/thomas/Zotero/storage/4YJ9LBZT/1708.html}
}

@online{lingle2023,
  title = {Transformer-{{VQ}}: {{Linear-Time Transformers}} via {{Vector Quantization}}},
  shorttitle = {Transformer-{{VQ}}},
  author = {Lingle, Lucas D.},
  date = {2023-09-28},
  eprint = {2309.16354},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2309.16354},
  url = {http://arxiv.org/abs/2309.16354},
  urldate = {2023-11-13},
  abstract = {We introduce Transformer-VQ, a decoder-only transformer computing softmax-based dense self-attention in linear time. Transformer-VQ's efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In large-scale experiments, Transformer-VQ is shown highly competitive in quality, with strong results on Enwik8 (0.99 bpb), PG-19 (26.6 ppl), and ImageNet64 (3.16 bpb). Code: https://github.com/transformer-vq/transformer\_vq},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/lingle2023.pdf;/home/thomas/Zotero/storage/AMZDGPVR/2309.html}
}

@online{liu2024,
  title = {{{LongVQ}}: {{Long Sequence Modeling}} with {{Vector Quantization}} on {{Structured Memory}}},
  shorttitle = {{{LongVQ}}},
  author = {Liu, Zicheng and Wang, Li and Li, Siyuan and Wang, Zedong and Lin, Haitao and Li, Stan Z.},
  date = {2024-04-18},
  eprint = {2404.11163},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2404.11163},
  url = {http://arxiv.org/abs/2404.11163},
  urldate = {2024-04-21},
  abstract = {Transformer models have been successful in various sequence processing tasks, but the self-attention mechanism's computational cost limits its practicality for long sequences. Although there are existing attention variants that improve computational efficiency, they have a limited ability to abstract global information effectively based on their hand-crafted mixing strategies. On the other hand, state-space models (SSMs) are tailored for long sequences but cannot capture complicated local information. Therefore, the combination of them as a unified token mixer is a trend in recent long-sequence models. However, the linearized attention degrades performance significantly even when equipped with SSMs. To address the issue, we propose a new method called LongVQ. LongVQ uses the vector quantization (VQ) technique to compress the global abstraction as a length-fixed codebook, enabling the linear-time computation of the attention matrix. This technique effectively maintains dynamic global and local patterns, which helps to complement the lack of long-range dependency issues. Our experiments on the Long Range Arena benchmark, autoregressive language modeling, and image and speech classification demonstrate the effectiveness of LongVQ. Our model achieves significant improvements over other sequence models, including variants of Transformers, Convolutions, and recent State Space Models.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/liu2024.pdf;/home/thomas/Zotero/storage/GL4588R4/2404.html}
}

@article{lumer1961,
  title = {Semi-Inner-Product Spaces},
  author = {Lumer, G.},
  date = {1961},
  journaltitle = {Trans. Amer. Math. Soc.},
  volume = {100},
  number = {1},
  pages = {29--43},
  issn = {0002-9947, 1088-6850},
  doi = {10.1090/S0002-9947-1961-0133024-2},
  url = {https://www.ams.org/tran/1961-100-01/S0002-9947-1961-0133024-2/},
  urldate = {2023-11-15},
  abstract = {Advancing research. Creating connections.},
  langid = {english},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/lumer1961.pdf}
}

@book{mcandrew2016,
  title = {Introduction to {{Cryptography}} with {{Open-Source Software}}},
  author = {McAndrew, Alasdair},
  date = {2016-04-19},
  eprint = {9lTRBQAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {CRC Press},
  abstract = {Once the privilege of a secret few, cryptography is now taught at universities around the world. Introduction to Cryptography with Open-Source Software illustrates algorithms and cryptosystems using examples and the open-source computer algebra system of Sage. The author, a noted educator in the field, provides a highly practical learning experienc},
  isbn = {978-1-4398-2571-6},
  langid = {english},
  pagetotal = {456},
  keywords = {Computers / Computer Science,Computers / Programming / Games,Computers / Security / General,Law / Computer & Internet,Mathematics / Combinatorics,Mathematics / Discrete Mathematics,Mathematics / Logic}
}

@book{mckenzie2011,
  title = {Algebras, {{Lattices}}, {{Varieties}}},
  author = {McKenzie, Ralph N. and McNulty, George F. and Taylor, Walter F.},
  date = {2011}
}

@online{millar2011,
  title = {Maximum {{Likelihood Estimation}} and {{Inference}}: {{With Examples}} in {{R}}, {{SAS}} and {{ADMB}} | {{Wiley}}},
  shorttitle = {Maximum {{Likelihood Estimation}} and {{Inference}}},
  author = {{millar}},
  date = {2011},
  url = {https://www.wiley.com/en-us/Maximum+Likelihood+Estimation+and+Inference%3A+With+Examples+in+R%2C+SAS+and+ADMB-p-9780470094822},
  urldate = {2023-12-23},
  abstract = {This book takes a fresh look at the popular and well-established method of maximum likelihood for statistical estimation and inference. It begins with an intuitive introduction to the concepts and background of likelihood, and moves through to the latest developments in maximum likelihood methodology, including general latent variable models and new material for the practical implementation of integrated likelihood using the free ADMB software. Fundamental issues of statistical inference are also examined, with a presentation of some of the philosophical debates underlying the choice of statistical paradigm. Key features: Provides an accessible introduction to pragmatic maximum likelihood modelling. Covers more advanced topics, including general forms of latent variable models (including non-linear and non-normal mixed-effects and state-space models) and the use of maximum likelihood variants, such as estimating equations, conditional likelihood, restricted likelihood and integrated likelihood. Adopts a practical approach, with a focus on providing the relevant tools required by researchers and practitioners who collect and analyze real data. Presents numerous examples and case studies across a wide range of applications including medicine, biology and ecology. Features applications from a range of disciplines, with implementation in R, SAS and/or ADMB. Provides all program code and software extensions on a supporting website. Confines supporting theory to the final chapters to maintain a readable and pragmatic focus of the preceding chapters. This book is not just an accessible and practical text about maximum likelihood, it is a comprehensive guide to modern maximum likelihood estimation and inference. It will be of interest to readers of all levels, from novice to expert. It will be of great benefit to researchers, and to students of statistics from senior undergraduate to graduate level. For use as a course text, exercises are provided at the end of each chapter.},
  langid = {american},
  organization = {Wiley.com},
  file = {/home/thomas/Zotero/storage/ASYFM2KI/Maximum+Likelihood+Estimation+and+Inference+With+Examples+in+R,+SAS+and+ADMB-p-9780470094822.html}
}

@article{nagy,
  title = {Ordinary {{Diﬀerential Equations}}},
  author = {Nagy, Gabriel},
  langid = {english},
  file = {/home/thomas/Zotero/storage/DMSY57RB/Nagy - Ordinary Diﬀerential Equations.pdf}
}

@inproceedings{nam2014,
  title = {Large-{{Scale Multi-label Text Classification}} — {{Revisiting Neural Networks}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Nam, Jinseok and Kim, Jungi and Loza Mencía, Eneldo and Gurevych, Iryna and Fürnkranz, Johannes},
  editor = {Calders, Toon and Esposito, Floriana and Hüllermeier, Eyke and Meo, Rosa},
  date = {2014},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {437--452},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-662-44851-9_28},
  abstract = {Neural networks have recently been proposed for multi-label classification because they are able to capture and model label dependencies in the output layer. In this work, we investigate limitations of BP-MLL, a neural network (NN) architecture that aims at minimizing pairwise ranking error. Instead, we propose to use a comparably simple NN approach with recently proposed learning techniques for large-scale multi-label text classification tasks. In particular, we show that BP-MLL’s ranking loss minimization can be efficiently and effectively replaced with the commonly used cross entropy error function, and demonstrate that several advances in neural network training that have been developed in the realm of deep learning can be effectively employed in this setting. Our experimental results show that simple NN models equipped with advanced techniques such as rectified linear units, dropout, and AdaGrad perform as well as or even outperform state-of-the-art approaches on six large-scale textual datasets with diverse characteristics.},
  isbn = {978-3-662-44851-9},
  langid = {english},
  keywords = {Cross Entropy,Hide Layer,Hide Unit,Single Hide Layer,Stochastic Gradient Descent},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/nam2014.pdf}
}

@book{needham1999,
  title = {Visual {{Complex Analysis}}},
  author = {Needham, Tristan},
  date = {1999-02-18},
  edition = {Reprint edition},
  publisher = {Oxford University Press, USA},
  location = {Oxford},
  abstract = {This radical first course on complex analysis brings a beautiful and powerful subject to life by consistently using geometry (not calculation) as the means of explanation. Aimed at undergraduate students in mathematics, physics, and engineering, the book's intuitive explanations, lack of advanced prerequisites, and consciously user-friendly prose style will help students to master the subject more readily than was previously possible. The key to this is the book's use of new geometric arguments in place of the standard calculational ones. These geometric arguments are communicated with the aid of hundreds of diagrams of a standard seldom encountered in mathematical works. A new approach to a classical topic, this work will be of interest to students in mathematics, physics, and engineering, as well as to professionals in these fields.},
  isbn = {978-0-19-853446-4},
  langid = {english},
  pagetotal = {616}
}

@article{nelder1972,
  title = {Generalized {{Linear Models}}},
  author = {Nelder, J. A. and Wedderburn, R. W. M.},
  date = {1972},
  journaltitle = {Journal of the Royal Statistical Society. Series A (General)},
  volume = {135},
  number = {3},
  eprint = {2344614},
  eprinttype = {jstor},
  pages = {370--384},
  publisher = {[Royal Statistical Society, Wiley]},
  issn = {0035-9238},
  doi = {10.2307/2344614},
  url = {https://www.jstor.org/stable/2344614},
  urldate = {2023-12-23},
  abstract = {The technique of iterative weighted linear regression can be used to obtain maximum likelihood estimates of the parameters with observations distributed according to some exponential family and systematic effects that can be made linear by a suitable transformation. A generalization of the analysis of variance is given for these models using log-likelihoods. These generalized linear models are illustrated by examples relating to four distributions; the Normal, Binomial (probit analysis, etc.), Poisson (contingency tables) and gamma (variance components). The implications of the approach in designing statistics courses are discussed.}
}

@book{nita2023,
  title = {Advances to {{Homomorphic}} and {{Searchable Encryption}}},
  author = {Nita, Stefania Loredana and Mihailescu, Marius Iulian},
  date = {2023},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-43214-9},
  url = {https://link.springer.com/10.1007/978-3-031-43214-9},
  urldate = {2024-05-21},
  isbn = {978-3-031-43213-2 978-3-031-43214-9},
  langid = {english},
  keywords = {big data security,cloud computing security,data science,homomorphic encryption,information security,lattice-based cryptography,multivariate cryptography,quantum cryptography,searchable encryption}
}

@book{olson2017,
  title = {Applied {{Fourier Analysis}}},
  author = {Olson, Tim},
  date = {2017},
  publisher = {Springer},
  location = {New York, NY},
  doi = {10.1007/978-1-4939-7393-4},
  url = {http://link.springer.com/10.1007/978-1-4939-7393-4},
  urldate = {2024-02-18},
  isbn = {978-1-4939-7391-0 978-1-4939-7393-4},
  langid = {english},
  keywords = {analysis,communications,Fourier applications,medical imaging,partial differential equations,sampling},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/olson2017.pdf}
}

@online{peikert2015,
  title = {A {{Decade}} of {{Lattice Cryptography}}},
  author = {Peikert, Chris},
  date = {2015},
  number = {2015/939},
  url = {https://eprint.iacr.org/2015/939},
  urldate = {2024-06-11},
  abstract = {\textbackslash emph\{Lattice-based cryptography\} is the use of conjectured hard problems on point lattices in\textasciitilde\$\textbackslash R\textasciicircum\{n\}\$ as the foundation for secure cryptographic systems. Attractive features of lattice cryptography include apparent resistance to \textbackslash emph\{quantum\} attacks (in contrast with most number-theoretic cryptography), high asymptotic efficiency and parallelism, security under \textbackslash emph\{worst-case\} intractability assumptions, and solutions to long-standing open problems in cryptography. This work surveys most of the major developments in lattice cryptography over the past ten years. The main focus is on the foundational \textbackslash emph\{short integer solution\}\textasciitilde (SIS) and \textbackslash emph\{learning with errors\}\textasciitilde (LWE) problems (and their more efficient ring-based variants), their provable hardness assuming the worst-case intractability of standard lattice problems, and their many cryptographic applications.},
  pubstate = {prepublished},
  keywords = {lattices,learning with errors,short integer solution,survey},
  annotation = {Publication info: Preprint. MINOR revision.},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/peikert2015.pdf}
}

@article{prasad2012,
  title = {A {{Study}} on {{Associative Neural Memories}}},
  author = {Prasad, B. D. C. N. and Prasad, P. E. S. N. Krishna and Yeruva, Sagar and Murty, P. Sita Rama},
  date = {2012-02-01},
  journaltitle = {International Journal of Advanced Computer Science and Applications (IJACSA)},
  volume = {1},
  number = {6},
  publisher = {{The Science and Information (SAI) Organization Limited}},
  issn = {2156-5570},
  doi = {10.14569/IJACSA.2010.010619},
  url = {https://thesai.org/Publications/ViewPaper?Volume=1&Issue=6&Code=IJACSA&SerialNo=19},
  urldate = {2024-07-27},
  abstract = {Memory plays a major role in Artificial Neural Networks. Without memory, Neural Network can not be learned itself. One of the primary concepts of memory in neural networks is Associative neural memories. A survey has been made on associative neural memories such as Simple associative memories (SAM), Dynamic associative memories (DAM), Bidirectional Associative memories (BAM), Hopfield memories, Context Sensitive Auto-associative memories (CSAM) and so on. These memories can be applied in various fields to get the effective outcomes. We present a study on these associative memories in artificial neural networks.},
  issue = {6},
  langid = {english},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/prasad2012.pdf}
}

@report{rabin1979,
  type = {Technical Report},
  title = {{{DIGITALIZED SIGNATURES AND PUBLIC-KEY FUNCTIONS AS INTRACTABLE AS FACTORIZATION}}},
  author = {Rabin, M. O.},
  date = {1979},
  institution = {Massachusetts Institute of Technology},
  location = {USA},
  abstract = {We introduce a new class of public-key functions involving a number n = pq having two large prime factors. As usual, the key n is public, while p and q are the private key used by the issuer for production of signatures and function inversion. These functions can be used for all the applications involving public-key functions proposed by Diffie and Hellman, including digitalized signatures. We prove that for any given n, if we can invert the function y = E (x1) for even a small percentage of the values y then we can factor n. Thus, as long as factorization of large numbers remains practically intractable, for appropriate chosen keys not even a small percentage of signatures are forgeable. Breaking the RSA function is at most hard as factorization, but is not known to be equivalent to factorization even in the weak sense that ability to invert all function values entails ability to factor the key. Computation time for these functions, i.e. signature verification, is several hundred times faster than for the RSA scheme. Inversion time, using the private key, is comparable. The almost-everywhere intractability of signature-forgery for our functions (on the assumption that factoring is intractable) is of great practical significance and seems to be the first proved result of this kind.}
}

@inproceedings{rabin1979a,
  title = {{{DIGITALIZED SIGNATURES AND PUBLIC-KEY FUNCTIONS AS INTRACTABLE AS FACTORIZATION}}},
  author = {Rabin, M.},
  date = {1979},
  url = {https://www.semanticscholar.org/paper/DIGITALIZED-SIGNATURES-AND-PUBLIC-KEY-FUNCTIONS-AS-Rabin/468600c74bf5e30f206c997c0f9f09561ceae7d2},
  urldate = {2024-08-08},
  abstract = {We introduce a new class of public-key functions involving a number n = pq having two large prime factors. As usual, the key n is public, while p and q are the private key used by the issuer for production of signatures and function inversion. These functions can be used for all the applications involving public-key functions proposed by Diffie and Hellman, including digitalized signatures. We prove that for any given n, if we can invert the function y = E (x1) for even a small percentage of the values y then we can factor n. Thus, as long as factorization of large numbers remains practically intractable, for appropriate chosen keys not even a small percentage of signatures are forgeable. Breaking the RSA function is at most hard as factorization, but is not known to be equivalent to factorization even in the weak sense that ability to invert all function values entails ability to factor the key. Computation time for these functions, i.e. signature verification, is several hundred times faster than for the RSA scheme. Inversion time, using the private key, is comparable. The almost-everywhere intractability of signature-forgery for our functions (on the assumption that factoring is intractable) is of great practical significance and seems to be the first proved result of this kind.}
}

@article{ramos2018,
  title = {Deconstructing {{Cross-Entropy}} for {{Probabilistic Binary Classifiers}}},
  author = {Ramos, Daniel and Franco-Pedroso, Javier and Lozano-Diez, Alicia and Gonzalez-Rodriguez, Joaquin},
  date = {2018-03},
  journaltitle = {Entropy},
  volume = {20},
  number = {3},
  pages = {208},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1099-4300},
  doi = {10.3390/e20030208},
  url = {https://www.mdpi.com/1099-4300/20/3/208},
  urldate = {2023-12-15},
  abstract = {In this work, we analyze the cross-entropy function, widely used in classifiers both as a performance measure and as an optimization objective. We contextualize cross-entropy in the light of Bayesian decision theory, the formal probabilistic framework for making decisions, and we thoroughly analyze its motivation, meaning and interpretation from an information-theoretical point of view. In this sense, this article presents several contributions: First, we explicitly analyze the contribution to cross-entropy of (i) prior knowledge; and (ii) the value of the features in the form of a likelihood ratio. Second, we introduce a decomposition of cross-entropy into two components: discrimination and calibration. This decomposition enables the measurement of different performance aspects of a classifier in a more precise way; and justifies previously reported strategies to obtain reliable probabilities by means of the calibration of the output of a discriminating classifier. Third, we give different information-theoretical interpretations of cross-entropy, which can be useful in different application scenarios, and which are related to the concept of reference probabilities. Fourth, we present an analysis tool, the Empirical Cross-Entropy (ECE) plot, a compact representation of cross-entropy and its aforementioned decomposition. We show the power of ECE plots, as compared to other classical performance representations, in two diverse experimental examples: a speaker verification system, and a forensic case where some glass findings are present.},
  issue = {3},
  langid = {english},
  keywords = {Bayesian,calibration,classifier,cross-entropy,discrimination,ECE plot,probabilistic},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/ramos2018.pdf}
}

@online{reddi2018,
  title = {Stochastic {{Negative Mining}} for {{Learning}} with {{Large Output Spaces}}},
  author = {Reddi, Sashank J. and Kale, Satyen and Yu, Felix and Holtmann-Rice, Dan and Chen, Jiecao and Kumar, Sanjiv},
  date = {2018-10-16},
  eprint = {1810.07076},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1810.07076},
  url = {http://arxiv.org/abs/1810.07076},
  urldate = {2023-11-13},
  abstract = {We consider the problem of retrieving the most relevant labels for a given input when the size of the output space is very large. Retrieval methods are modeled as set-valued classifiers which output a small set of classes for each input, and a mistake is made if the label is not in the output set. Despite its practical importance, a statistically principled, yet practical solution to this problem is largely missing. To this end, we first define a family of surrogate losses and show that they are calibrated and convex under certain conditions on the loss parameters and data distribution, thereby establishing a statistical and analytical basis for using these losses. Furthermore, we identify a particularly intuitive class of loss functions in the aforementioned family and show that they are amenable to practical implementation in the large output space setting (i.e. computation is possible without evaluating scores of all labels) by developing a technique called Stochastic Negative Mining. We also provide generalization error bounds for the losses in the family. Finally, we conduct experiments which demonstrate that Stochastic Negative Mining yields benefits over commonly used negative sampling approaches.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/reddi2018.pdf;/home/thomas/Zotero/storage/W6JVNWF3/1810.html}
}

@online{regev2024,
  title = {On {{Lattices}}, {{Learning}} with {{Errors}}, {{Random Linear Codes}}, and {{Cryptography}}},
  author = {Regev, Oded},
  date = {2024-01-08},
  eprint = {2401.03703},
  eprinttype = {arXiv},
  eprintclass = {quant-ph},
  doi = {10.48550/arXiv.2401.03703},
  url = {http://arxiv.org/abs/2401.03703},
  urldate = {2024-06-17},
  abstract = {Our main result is a reduction from worst-case lattice problems such as GapSVP and SIVP to a certain learning problem. This learning problem is a natural extension of the `learning from parity with error' problem to higher moduli. It can also be viewed as the problem of decoding from a random linear code. This, we believe, gives a strong indication that these problems are hard. Our reduction, however, is quantum. Hence, an efficient solution to the learning problem implies a quantum algorithm for GapSVP and SIVP. A main open question is whether this reduction can be made classical (i.e., non-quantum). We also present a (classical) public-key cryptosystem whose security is based on the hardness of the learning problem. By the main result, its security is also based on the worst-case quantum hardness of GapSVP and SIVP. The new cryptosystem is much more efficient than previous lattice-based cryptosystems: the public key is of size \$\textbackslash tilde\{O\}(n\textasciicircum 2)\$ and encrypting a message increases its size by a factor of \$\textbackslash tilde\{O\}(n)\$ (in previous cryptosystems these values are \$\textbackslash tilde\{O\}(n\textasciicircum 4)\$ and \$\textbackslash tilde\{O\}(n\textasciicircum 2)\$, respectively). In fact, under the assumption that all parties share a random bit string of length \$\textbackslash tilde\{O\}(n\textasciicircum 2)\$, the size of the public key can be reduced to \$\textbackslash tilde\{O\}(n)\$.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Complexity,Computer Science - Cryptography and Security,Quantum Physics},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/regev2024.pdf;/home/thomas/Zotero/storage/J2S97PSF/2401.html}
}

@inproceedings{rivest1978,
  title = {{{ON DATA BANKS AND PRIVACY HOMOMORPHISMS}}},
  author = {Rivest, Ronald L. and Dertouzos, M.},
  date = {1978},
  url = {https://www.semanticscholar.org/paper/ON-DATA-BANKS-AND-PRIVACY-HOMOMORPHISMS-Rivest-Dertouzos/c365f01d330b2211e74069120e88cff37eacbcf5},
  urldate = {2024-05-21},
  abstract = {Encryption is a well—known technique for preserving the privacy of sensitive information. One of the basic, apparently inherent, limitations of this technique is that an information system working with encrypted data can at most store or retrieve the data for the user; any more complicated operations seem to require that the data be decrypted before being operated on. This limitation follows from the choice of encryption functions used, however, and although there are some truly inherent limitations on what can be accomplished, we shall see that it appears likely that there exist encryption functions which permit encrypted data to be operated on without preliminary decryption of the operands, for many sets of interesting operations. These special encryption functions we call “privacy homomorphisms”; they form an interesting subset of arbitrary encryption schemes (called “privacy transformations”).}
}

@article{rivest1978a,
  title = {A Method for Obtaining Digital Signatures and Public-Key Cryptosystems},
  author = {Rivest, R. L. and Shamir, A. and Adleman, L.},
  date = {1978-02-01},
  journaltitle = {Commun. ACM},
  volume = {21},
  number = {2},
  pages = {120--126},
  issn = {0001-0782},
  doi = {10.1145/359340.359342},
  url = {https://dl.acm.org/doi/10.1145/359340.359342},
  urldate = {2024-08-08},
  abstract = {An encryption method is presented with the novel property that publicly revealing an encryption key does not thereby reveal the corresponding decryption key. This has two important consequences: (1) Couriers or other secure means are not needed to transmit keys, since a message can be enciphered using an encryption key publicly revealed by the intented recipient. Only he can decipher the message, since only he knows the corresponding decryption key. (2) A message can be “signed” using a privately held decryption key. Anyone can verify this signature using the corresponding publicly revealed encryption key. Signatures cannot be forged, and a signer cannot later deny the validity of his signature. This has obvious applications in “electronic mail” and “electronic funds transfer” systems. A message is encrypted by representing it as a number M, raising M to a publicly specified power e, and then taking the remainder when the result is divided by the publicly specified product, n, of two large secret primer numbers p and q. Decryption is similar; only a different, secret, power d is used, where e * d ≡ 1(mod (p - 1) * (q - 1)). The security of the system rests in part on the difficulty of factoring the published divisor, n.},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/rivest1978a.pdf}
}

@book{rudin1976,
  title = {Principles of {{Mathematical Analysis}}},
  author = {Rudin, Walter},
  date = {1976},
  eprint = {kwqzPAAACAAJ},
  eprinttype = {googlebooks},
  publisher = {McGraw-Hill},
  abstract = {The third edition of this well known text continues to provide a solid foundation in mathematical analysis for undergraduate and first-year graduate students. The text begins with a discussion of the real number system as a complete ordered field. (Dedekind's construction is now treated in an appendix to Chapter I.) The topological background needed for the development of convergence, continuity, differentiation and integration is provided in Chapter 2. There is a new section on the gamma function, and many new and interesting exercises are included. This text is part of the Walter Rudin Student Series in Advanced Mathematics.},
  isbn = {978-0-07-085613-4},
  langid = {english},
  pagetotal = {342},
  keywords = {Mathematics / Calculus}
}

@article{sabani2024,
  title = {Learning with {{Errors}}: {{A Lattice-Based Keystone}} of {{Post-Quantum Cryptography}}},
  author = {Sabani, Maria E. and Savvas, Illias K. and Garani, Georgia},
  date = {2024-04-13},
  doi = {10.3390/signals5020012},
  url = {https://www.mdpi.com/2624-6120/5/2/12},
  abstract = {Abstract The swift advancement of quantum computing devices holds the potential to create robust machines that can tackle an extensive array of issues beyond the scope of conventional computers. Consequently, quantum computing machines create new risks at a velocity and scale never seen before, especially with regard to encryption. Lattice-based cryptography is regarded as post-quantum cryptography’s future and a competitor to a quantum computer attack. Thus, there are several advantages to lattice-based cryptographic protocols, including security, effectiveness, reduced energy usage and speed. In this work, we study the learning with errors (LWE) problem and the cryptosystems that are based on the LWE problem and, in addition, we present a new efficient variant of LWE cryptographic scheme.},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/sabani2024.pdf}
}

@inproceedings{sato1995,
  title = {Generalized {{Learning Vector Quantization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Sato, Atsushi and Yamada, Keiji},
  date = {1995},
  volume = {8},
  publisher = {MIT Press},
  url = {https://papers.nips.cc/paper_files/paper/1995/hash/9c3b1830513cc3b8fc4b76635d32e692-Abstract.html},
  urldate = {2024-05-12},
  abstract = {We  propose  a  new  learning  method,  "Generalized  Learning  Vec(cid:173) tor Quantization (GLVQ),"  in which reference vectors are updated  based on the steepest descent method in order to minimize the cost  function .  The  cost  function  is  determined  so  that  the  obtained  learning  rule  satisfies  the  convergence  condition.  We  prove  that  Kohonen's  rule  as  used  in  LVQ  does  not  satisfy  the  convergence  condition  and  thus  degrades  recognition  ability.  Experimental re(cid:173) sults  for  printed  Chinese  character recognition  reveal  that  GLVQ  is  superior to  LVQ  in  recognition  ability.},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/sato1995.pdf}
}

@book{schneier1996,
  title = {Applied {{Cryptography}}: {{Protocols}}, {{Algorithms}}, and {{Source Code}} in {{C}}},
  shorttitle = {Applied {{Cryptography}}},
  author = {Schneier, Bruce},
  date = {1996},
  eprint = {6NdQAAAAMAAJ},
  eprinttype = {googlebooks},
  publisher = {Wiley},
  abstract = {". . .the best introduction to cryptography I've ever seen. . . .The book the National Security Agency wanted never to be published. . . ." -Wired Magazine  ". . .monumental . . . fascinating . . . comprehensive . . . the definitive work on cryptography for computer programmers . . ." -Dr. Dobb's Journal  ". . .easily ranks as one of the most authoritative in its field." -PC Magazine  ". . .the bible of code hackers." -The Millennium Whole Earth Catalog  This new edition of the cryptography classic provides you with a comprehensive survey of modern cryptography. The book details how programmers and electronic communications professionals can use cryptography-the technique of enciphering and deciphering messages-to maintain the privacy of computer data. It describes dozens of cryptography algorithms, gives practical advice on how to implement them into cryptographic software, and shows how they can be used to solve security problems. Covering the latest developments in practical cryptographic techniques, this new edition shows programmers who design computer applications, networks, and storage systems how they can build security into their software and systems.  What's new in the Second Edition? * New information on the Clipper Chip, including ways to defeat the key escrow mechanism * New encryption algorithms, including algorithms from the former Soviet Union and South Africa, and the RC4 stream cipher * The latest protocols for digital signatures, authentication, secure elections, digital cash, and more * More detailed information on key management and cryptographic implementations},
  isbn = {978-0-471-11709-4},
  langid = {english},
  pagetotal = {796},
  keywords = {Computers / Computer Science,Computers / Information Technology,Computers / Security / Cryptography & Encryption,Computers / Security / General,Language Arts & Disciplines / Communication Studies,Mathematics / Discrete Mathematics,Technology & Engineering / Telecommunications}
}

@book{shao2003,
  title = {Mathematical {{Statistics}}},
  author = {Shao, Jun},
  editor = {Casella, G. and Fienberg, S. and Olkin, I.},
  editortype = {redactor},
  date = {2003},
  series = {Springer {{Texts}} in {{Statistics}}},
  publisher = {Springer},
  location = {New York, NY},
  doi = {10.1007/b97553},
  url = {http://link.springer.com/10.1007/b97553},
  urldate = {2023-12-26},
  isbn = {978-0-387-95382-3 978-0-387-21718-5},
  keywords = {likelihood,Markov chain,Mathematica,mathematical statistics,Mathematical Statistics,probability,probability theory,statistical theory,Statistical Theory,statistics},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/shao2003.pdf}
}

@book{shifrin2005,
  title = {Multivariable Mathematics: {{Linear}} Algebra, Multivariable Calculus, and Manifolds},
  author = {Shifrin, Theodore},
  date = {2005},
  publisher = {John Wiley \& Sons, Inc.},
  isbn = {978-0-471-52638-4 0-471-52638-X}
}

@book{stallings1999,
  title = {Cryptography and {{Network Security}}: {{Principles}} and {{Practice}}},
  shorttitle = {Cryptography and {{Network Security}}},
  author = {Stallings, William},
  date = {1999},
  eprint = {Dam9zrViJjEC},
  eprinttype = {googlebooks},
  publisher = {Prentice Hall},
  abstract = {As we enter the age of universal electronic connectivity in which viruses, hackers, electronic eavesdropping, and electronic fraud can threaten the prosperity and productivity of corporations and individuals, security is increasingly important. Fortunately, the disciplines of cryptography and network security have matured, leading to the development of practical, available applications to enforce network security.  Best-selling author and two-time winner of the TEXTY award for the best computer science and engineering text, William Stallings provides a practical survey of both the principles and practice of cryptography and network security.  Extensively reorganized to provide the optimal sequence for classroom instruction and self-study, the second edition includes these key features. Looks at system-level security issues, including the threat of and countermeasures for intruders and viruses, and the use of firewalls and trusted systems. NEW - Discussion of block cipher design principles, plus coverage of Blowfish, CAST-128, Triple DES, and other algorithms NEW - Chapters on IP security and Web security Expanded coverage of public-key encryption algorithms and design principles, including RSA and elliptic curve cryptography Covers important network security tools and applications, including Kerberos, X.509v3, PGP, S/MIME, IP security, SSL/TLS, and SET On-line transparency masters, an Internet mailing list, and links to relevant web sites are available to http: //www.shore.net/\textasciitilde ws/Security2e.html},
  isbn = {978-0-13-869017-5},
  langid = {english},
  pagetotal = {600},
  keywords = {Computers / Database Administration & Management,Computers / Information Theory,Computers / Networking / General,Computers / Security / Cryptography & Encryption,Computers / Security / General,Computers / Security / Network Security}
}

@book{stanley2012,
  title = {Enumerative {{Combinatorics}}: {{Volume}} 1},
  shorttitle = {Enumerative {{Combinatorics}}},
  author = {Stanley, Richard P.},
  date = {2012-02-23},
  edition = {2. edition},
  publisher = {Cambridge University Press},
  location = {Cambridge, NY},
  abstract = {Richard Stanley's two-volume basic introduction to enumerative combinatorics has become the standard guide to the topic for students and experts alike. This thoroughly revised second edition of Volume 1 includes ten new sections and more than 300 new exercises, most with solutions, reflecting numerous new developments since the publication of the first edition in 1986. The material in Volume 1 was chosen to cover those parts of enumerative combinatorics of greatest applicability and with the most important connections with other areas of mathematics. The four chapters are devoted to an introduction to enumeration (suitable for advanced undergraduates), sieve methods, partially ordered sets, and rational generating functions. Much of the material is related to generating functions, a fundamental tool in enumerative combinatorics. In this new edition, the author brings the coverage up to date and includes a wide variety of additional applications and examples, as well as updated and expanded chapter bibliographies. Many of the less difficult new exercises have no solutions so that they can more easily be assigned to students. The material on P-partitions has been rearranged and generalized; the treatment of permutation statistics has been greatly enlarged; and there are also new sections on q-analogues of permutations, hyperplane arrangements, the cd-index, promotion and evacuation, and differential posets.},
  isbn = {978-1-107-01542-5},
  langid = {english},
  pagetotal = {632}
}

@book{vasudeva2017,
  title = {Elements of {{Hilbert Spaces}} and {{Operator Theory}}},
  author = {Vasudeva, Harkrishan Lal},
  date = {2017},
  publisher = {Springer},
  location = {Singapore},
  doi = {10.1007/978-981-10-3020-8},
  url = {http://link.springer.com/10.1007/978-981-10-3020-8},
  urldate = {2023-11-14},
  isbn = {978-981-10-3019-2 978-981-10-3020-8},
  langid = {english},
  keywords = {Banach Spaces,Finite Dimensional Spaces,Functional analysis,Linear operators,Operator theory,Riesz Lemma,Special theory},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/vasudeva2017.pdf}
}

@online{vaswani2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2023-08-01},
  eprint = {1706.03762},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2023-11-17},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/vaswani2023.pdf;/home/thomas/Zotero/storage/SCJWFGAA/1706.html}
}

@online{xie2014,
  title = {Crypto-{{Nets}}: {{Neural Networks}} over {{Encrypted Data}}},
  shorttitle = {Crypto-{{Nets}}},
  author = {Xie, Pengtao and Bilenko, Misha and Finley, Tom and Gilad-Bachrach, Ran and Lauter, Kristin and Naehrig, Michael},
  date = {2014-12-24},
  eprint = {1412.6181},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1412.6181},
  url = {http://arxiv.org/abs/1412.6181},
  urldate = {2024-05-21},
  abstract = {The problem we address is the following: how can a user employ a predictive model that is held by a third party, without compromising private information. For example, a hospital may wish to use a cloud service to predict the readmission risk of a patient. However, due to regulations, the patient's medical files cannot be revealed. The goal is to make an inference using the model, without jeopardizing the accuracy of the prediction or the privacy of the data. To achieve high accuracy, we use neural networks, which have been shown to outperform other learning models for many tasks. To achieve the privacy requirements, we use homomorphic encryption in the following protocol: the data owner encrypts the data and sends the ciphertexts to the third party to obtain a prediction from a trained model. The model operates on these ciphertexts and sends back the encrypted prediction. In this protocol, not only the data remains private, even the values predicted are available only to the data owner. Using homomorphic encryption and modifications to the activation functions and training algorithms of neural networks, we show that it is protocol is possible and may be feasible. This method paves the way to build a secure cloud-based neural network prediction services without invading users' privacy.},
  pubstate = {prepublished},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/xie2014.pdf;/home/thomas/Zotero/storage/NE4WJDV2/1412.html}
}

@book{yi2014,
  title = {Homomorphic {{Encryption}} and {{Applications}}},
  author = {Yi, Xun and Paulet, Russell and Bertino, Elisa},
  date = {2014},
  series = {{{SpringerBriefs}} in {{Computer Science}}},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-12229-8},
  url = {https://link.springer.com/10.1007/978-3-319-12229-8},
  urldate = {2024-05-21},
  isbn = {978-3-319-12228-1 978-3-319-12229-8},
  langid = {english},
  keywords = {Critical Infrastructure Outsourcing,Cryptographic Assumption,Encrypted Email,Homomorphic Cryptography,Homomorphic Encryption,Homomorphic Encryption Application,Homomorphic Spam Filtering,Lattice Based Cryptography,Privacy Preserving Data Mining,Privacy Preserving Email},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/yi2014.pdf}
}

@book{young1988,
  title = {An {{Introduction}} to {{Hilbert Space}}},
  author = {Young, N.},
  date = {1988},
  publisher = {Cambridge University Press},
  location = {Cambridge},
  doi = {10.1017/CBO9781139172011},
  url = {https://www.cambridge.org/core/books/an-introduction-to-hilbert-space/41D80B163B8E186820B5A48F3665EC22},
  urldate = {2023-11-15},
  abstract = {This textbook is an introduction to the theory of Hilbert space and its applications. The notion of Hilbert space is central in functional analysis and is used in numerous branches of pure and applied mathematics. Dr Young has stressed applications of the theory, particularly to the solution of partial differential equations in mathematical physics and to the approximation of functions in complex analysis. Some basic familiarity with real analysis, linear algebra and metric spaces is assumed, but otherwise the book is self-contained. It is based on courses given at the University of Glasgow and contains numerous examples and exercises (many with solutions). Thus it will make an excellent first course in Hilbert space theory at either undergraduate or graduate level and will also be of interest to electrical engineers and physicists, particularly those involved in control theory and filter design.},
  isbn = {978-0-521-33717-5},
  file = {/home/thomas/Zotero/storage/Q72XTLTI/41D80B163B8E186820B5A48F3665EC22.html}
}

@book{zabczyk2008,
  title = {Mathematical {{Control Theory}}},
  author = {Zabczyk, Jerzy},
  date = {2008},
  publisher = {Birkhäuser},
  location = {Boston, MA},
  doi = {10.1007/978-0-8176-4733-9},
  url = {http://link.springer.com/10.1007/978-0-8176-4733-9},
  urldate = {2024-01-01},
  isbn = {978-0-8176-4732-2 978-0-8176-4733-9},
  langid = {english},
  keywords = {control,control system,control theory,dynamic programming,infinite dimensional linear systems,linear systems,mathematical control theory,nonlinear control,nonlinear system,observability,optimal control,programming,stability,stabilization,sys},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/zabczyk2008.pdf}
}

@online{zotero-182,
  title = {{{OpenStax}} | {{Calculus Volume}} 2},
  url = {https://openstax.org/},
  abstract = {OpenStax offers free college textbooks for all types of students, making education accessible \& affordable for everyone. Browse our list of available subjects!},
  langid = {american},
  file = {/home/thomas/Zotero/storage/8HXCFBAT/calculus-volume-2.html}
}

@online{zotero-211,
  title = {Amazon.de:{{Customer Reviews}}: {{Fourier Analysis}} and {{Its Applications}} ({{Pure}} and {{Applied Undergraduate Texts}}, 4, {{Band}} 4)},
  url = {https://www.amazon.de/-/en/Gerald-B-Folland/product-reviews/0821847902/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews},
  urldate = {2024-02-26},
  file = {/home/thomas/Zotero/storage/6UVGAD4G/ref=cm_cr_dp_d_show_all_btm.html}
}

@online{zotero-256,
  title = {Applied {{Cryptography}}: {{Protocols}}, {{Algorithms}} and {{Source Code}} in {{C}}, 20th {{Anniversary Edition}} | {{Wiley}}},
  shorttitle = {Applied {{Cryptography}}},
  url = {https://www.wiley.com/en-au/Applied+Cryptography%3A+Protocols%2C+Algorithms+and+Source+Code+in+C%2C+20th+Anniversary+Edition-p-9781119096726},
  urldate = {2024-06-08},
  abstract = {Generated by create next app},
  langid = {english},
  organization = {Wiley.com},
  file = {/home/thomas/Zotero/storage/ZVVS37YF/Applied+Cryptography+Protocols,+Algorithms+and+Source+Code+in+C,+20th+Anniversary+Edition-p-978.html}
}

@online{zotero-258,
  title = {Applied {{Cryptography}}: {{Protocols}}, {{Algorithms}}, and {{Source Code}} in {{C}} : {{Schneier}}, {{Bruce}}: {{Amazon}}.de: {{Books}}},
  url = {https://www.amazon.de/Applied-Cryptography-Protocols-Algorithms-Source/dp/0471117099},
  urldate = {2024-06-08},
  file = {/home/thomas/Zotero/storage/64CQI3CL/0471117099.html}
}

@online{zotero-272,
  title = {Signals | {{Free Full-Text}} | {{Learning}} with {{Errors}}: {{A Lattice-Based Keystone}} of {{Post-Quantum Cryptography}}},
  url = {https://www.mdpi.com/2624-6120/5/2/12},
  urldate = {2024-06-17},
  file = {/home/thomas/Zotero/storage/5HJPH4KI/12.html}
}

@online{zotero-282,
  title = {Topics in {{Circular Statistics}} | {{Series}} on {{Multivariate Analysis}}},
  url = {https://www.worldscientific.com/worldscibooks/10.1142/4031},
  urldate = {2024-06-22},
  abstract = {This research monograph on circular data analysis covers some recent advances in the field, besides providing a brief introduction to, and a review of, existing methods and models. The primary focu...},
  langid = {english},
  file = {/home/thomas/Zotero/storage/PZCI3EKY/4031.html}
}

@online{zotero-316,
  title = {Digitalized {{Signatures}} and {{Public-Key Functions}} as {{Intractable}} as {{Factorization}},},
  url = {https://apps.dtic.mil/sti/citations/ADA078415},
  urldate = {2024-08-08},
  file = {/home/thomas/Zotero/storage/2PWTU4R5/ADA078415.html}
}

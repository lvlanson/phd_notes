@online{acar2017,
  title = {A {{Survey}} on {{Homomorphic Encryption Schemes}}: {{Theory}} and {{Implementation}}},
  shorttitle = {A {{Survey}} on {{Homomorphic Encryption Schemes}}},
  author = {Acar, Abbas and Aksu, Hidayet and Uluagac, A. Selcuk and Conti, Mauro},
  date = {2017-10-05},
  eprint = {1704.03578},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1704.03578},
  url = {http://arxiv.org/abs/1704.03578},
  urldate = {2024-08-30},
  abstract = {Legacy encryption systems depend on sharing a key (public or private) among the peers involved in exchanging an encrypted message. However, this approach poses privacy concerns. Especially with popular cloud services, the control over the privacy of the sensitive data is lost. Even when the keys are not shared, the encrypted material is shared with a third party that does not necessarily need to access the content. Moreover, untrusted servers, providers, and cloud operators can keep identifying elements of users long after users end the relationship with the services. Indeed, Homomorphic Encryption (HE), a special kind of encryption scheme, can address these concerns as it allows any third party to operate on the encrypted data without decrypting it in advance. Although this extremely useful feature of the HE scheme has been known for over 30 years, the first plausible and achievable Fully Homomorphic Encryption (FHE) scheme, which allows any computable function to perform on the encrypted data, was introduced by Craig Gentry in 2009. Even though this was a major achievement, different implementations so far demonstrated that FHE still needs to be improved significantly to be practical on every platform. First, we present the basics of HE and the details of the well-known Partially Homomorphic Encryption (PHE) and Somewhat Homomorphic Encryption (SWHE), which are important pillars of achieving FHE. Then, the main FHE families, which have become the base for the other follow-up FHE schemes are presented. Furthermore, the implementations and recent improvements in Gentry-type FHE schemes are also surveyed. Finally, further research directions are discussed. This survey is intended to give a clear knowledge and foundation to researchers and practitioners interested in knowing, applying, as well as extending the state of the art HE, PHE, SWHE, and FHE systems.},
  pubstate = {prepublished},
  keywords = {Computer Science - Cryptography and Security,E.3,K.4.1,K.6.5},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/acar2017.pdf;/home/thomas/Zotero/storage/ZCXVUCMV/1704.html}
}

@article{albertsson2018,
  title = {Machine {{Learning}} in {{High Energy Physics Community White Paper}}},
  author = {Albertsson, Kim and Altoe, Piero and Anderson, Dustin and Andrews, Michael and Espinosa, Juan Pedro Araque and Aurisano, Adam and Basara, Laurent and Bevan, Adrian and Bhimji, Wahid and Bonacorsi, Daniele and Calafiura, Paolo and Campanelli, Mario and Capps, Louis and Carminati, Federico and Carrazza, Stefano and Childers, Taylor and Coniavitis, Elias and Cranmer, Kyle and David, Claire and Davis, Douglas and Duarte, Javier and Erdmann, Martin and Eschle, Jonas and Farbin, Amir and Feickert, Matthew and Castro, Nuno Filipe and Fitzpatrick, Conor and Floris, Michele and Forti, Alessandra and Garra-Tico, Jordi and Gemmler, Jochen and Girone, Maria and Glaysher, Paul and Gleyzer, Sergei and Gligorov, Vladimir and Golling, Tobias and Graw, Jonas and Gray, Lindsey and Greenwood, Dick and Hacker, Thomas and Harvey, John and Hegner, Benedikt and Heinrich, Lukas and Hooberman, Ben and Junggeburth, Johannes and Kagan, Michael and Kane, Meghan and Kanishchev, Konstantin and Karpiński, Przemysław and Kassabov, Zahari and Kaul, Gaurav and Kcira, Dorian and Keck, Thomas and Klimentov, Alexei and Kowalkowski, Jim and Kreczko, Luke and Kurepin, Alexander and Kutschke, Rob and Kuznetsov, Valentin and Köhler, Nicolas and Lakomov, Igor and Lannon, Kevin and Lassnig, Mario and Limosani, Antonio and Louppe, Gilles and Mangu, Aashrita and Mato, Pere and Meinhard, Helge and Menasce, Dario and Moneta, Lorenzo and Moortgat, Seth and Narain, Meenakshi and Neubauer, Mark and Newman, Harvey and Pabst, Hans and Paganini, Michela and Paulini, Manfred and Perdue, Gabriel and Perez, Uzziel and Picazio, Attilio and Pivarski, Jim and Prosper, Harrison and Psihas, Fernanda and Radovic, Alexander and Reece, Ryan and Rinkevicius, Aurelius and Rodrigues, Eduardo and Rorie, Jamal and Rousseau, David and Sauers, Aaron and Schramm, Steven and Schwartzman, Ariel and Severini, Horst and Seyfert, Paul and Siroky, Filip and Skazytkin, Konstantin and Sokoloff, Mike and Stewart, Graeme and Stienen, Bob and Stockdale, Ian and Strong, Giles and Thais, Savannah and Tomko, Karen and Upfal, Eli and Usai, Emanuele and Ustyuzhanin, Andrey and Vala, Martin and Vallecorsa, Sofia and Vasel, Justin and Verzetti, Mauro and Vilasís-Cardona, Xavier and Vlimant, Jean-Roch and Vukotic, Ilija and Wang, Sean-Jiun and Watts, Gordon and Williams, Michael and Wu, Wenjing and Wunsch, Stefan and Zapata, Omar},
  date = {2018-09},
  journaltitle = {J. Phys.: Conf. Ser.},
  volume = {1085},
  number = {2},
  pages = {022008},
  publisher = {IOP Publishing},
  issn = {1742-6596},
  doi = {10.1088/1742-6596/1085/2/022008},
  url = {https://dx.doi.org/10.1088/1742-6596/1085/2/022008},
  urldate = {2024-01-05},
  abstract = {Machine learning is an important applied research area in particle physics, beginning with applications to high-level physics analysis in the 1990s and 2000s, followed by an explosion of applications in particle and event identification and reconstruction in the 2010s. In this document we discuss promising future research and development areas in machine learning in particle physics with a roadmap for their implementation, software and hardware resource requirements, collaborative initiatives with the data science community, academia and industry, and training the particle physics community in data science. The main objective of the document is to connect and motivate these areas of research and development with the physics drivers of the High-Luminosity Large Hadron Collider and future neutrino experiments and identify the resource needs for their implementation. Additionally we identify areas where collaboration with external communities will be of great benefit.},
  langid = {english},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/albertsson2018.pdf}
}

@inproceedings{anderson1977,
  title = {Distinctive Features, Categorical Perception, and Probability Learning: {{Some}} Applications of a Neural Model.},
  shorttitle = {Distinctive Features, Categorical Perception, and Probability Learning},
  booktitle = {Psychological {{Review}}},
  author = {Anderson, James A. and Silverstein, Jack W. and Ritz, Stephen A. and Jones, Randall S.},
  date = {1977-09},
  volume = {84},
  number = {5},
  pages = {413--451},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/0033-295X.84.5.413},
  url = {https://doi.apa.org/doi/10.1037/0033-295X.84.5.413},
  urldate = {2024-07-27},
  abstract = {A previously proposed model for memory based on neurophysiolo gical considerations is reviewed. We assume that (a) nervous system activity is usefully represented as the set of simultaneous individual neuron activities in a group of neurons; (b) different memory traces make use of the same synapses; and (c) synapses associate two patterns of neural activity by incrementing synaptic connectivity proportionally to the product of pre- and postsynaptic activity, forming a matrix of synaptic connectivities. We extend this model by (a) introducing positive feedback of a set of neurons onto itself and (b) allowing the individual neurons to saturate. A hybrid model, partly analog and partly binary, arises. The system has certain characteristics reminiscent of analysis by distinctive features. Next, we apply the model to "categorical perception." Finally, we discuss probability learning. The model can predict overshooting, recency data, and probabilities occurring in systems with more than two events with reasonably good accuracy.},
  langid = {english}
}

@book{apostol1967,
  title = {Calculus, Vol. 2: {{Multi-variable}} Calculus and Linear Algebra with Applications to Differential Equations and Probability},
  author = {Apostol, Tom M.},
  date = {1967},
  publisher = {J. Wiley},
  location = {New York},
  isbn = {0-471-00005-1 978-0-471-00005-1 0-471-00007-8 978-0-471-00007-5},
  keywords = {calculus textbook}
}

@article{armknecht2015,
  title = {A {{Guide}} to {{Fully Homomorphic Encryption}}},
  author = {Armknecht, Frederik and Boyd, C. and Carr, Christopher and Gjøsteen, Kristian and Jäschke, Angela and Reuter, Christian A. and Strand, Martin},
  date = {2015},
  journaltitle = {IACR Cryptol. ePrint Arch.},
  url = {https://www.semanticscholar.org/paper/A-Guide-to-Fully-Homomorphic-Encryption-Armknecht-Boyd/7ee670d05930c034d2224a42b37db8862a566810},
  urldate = {2024-08-30},
  abstract = {Fully homomorphic encryption (FHE) has been dubbed the holy grail of cryptography, an elusive goal which could solve the IT world’s problems of security and trust. Research in the area exploded after 2009 when Craig Gentry showed that FHE can be realised in principle. Since that time considerable progress has been made in finding more practical and more efficient solutions. Whilst research quickly developed, terminology and concepts became diverse and confusing so that today it can be difficult to understand what the achievements of different works actually are. The purpose of this paper is to address three fundamental questions: What is FHE? What can FHE be used for? What is the state of FHE today? As well as surveying the field, we clarify different terminology in use and prove connections between different FHE notions.}
}

@online{armknecht2015a,
  title = {A {{Guide}} to {{Fully Homomorphic Encryption}}},
  author = {Armknecht, Frederik and Boyd, Colin and Carr, Christopher and Gjøsteen, Kristian and Jäschke, Angela and Reuter, Christian A. and Strand, Martin},
  date = {2015},
  number = {2015/1192},
  url = {https://eprint.iacr.org/2015/1192},
  urldate = {2024-08-30},
  abstract = {Fully homomorphic encryption (FHE) has been dubbed the holy grail of cryptography, an elusive goal which could solve the IT world's problems of security and trust. Research in the area exploded after 2009 when Craig Gentry showed that FHE can be realised in principle. Since that time considerable progress has been made in finding more practical and more efficient solutions. Whilst research quickly developed, terminology and concepts became diverse and confusing so that today it can be difficult to understand what the achievements of different works actually are. The purpose of this paper is to address three fundamental questions: What is FHE? What can FHE be used for? What is the state of FHE today? As well as surveying the field, we clarify different terminology in use and prove connections between different FHE notions.},
  pubstate = {prepublished},
  keywords = {Fully Homomorphic Encryption},
  annotation = {Publication info: Preprint. MINOR revision.},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/armknecht2015a.pdf}
}

@online{badawi2023,
  title = {Demystifying {{Bootstrapping}} in {{Fully Homomorphic Encryption}}},
  author = {Badawi, Ahmad Al and Polyakov, Yuriy},
  date = {2023},
  number = {2023/149},
  url = {https://eprint.iacr.org/2023/149},
  urldate = {2025-01-15},
  abstract = {Bootstrapping is a term used very often in the context of Fully Homomorphic Encryption (FHE). Anyone who is familiar with FHE knows that bootstrapping is the most sophisticated and compute-intensive component of an FHE scheme. However, very few non-FHE-experts understand what the bootstrapping operation really is and that there are various bootstrapping methods, each with its own tradeoffs. The goal of this paper is to provide a high-level introduction to common bootstrapping methods and evaluate their performance using the existing implementations in OpenFHE and HElib open-source libraries.  Our performance evaluation suggests that the bootstrapping in the Cheon-Kim-Kim-Song (CKKS) scheme provides highest throughput and efficiently achieves large precision for vectors of real numbers, which are often used in machine learning applications. The Ducas-Micciancio (DM) and Chillotti-Gama-Georgieva-Izabachene (CGGI) schemes achieve the smallest latency (typically for small integers or small-precision fixed-point numbers) and provide a general capability for evaluating arbitrary functions (programmable bootstrapping) via lookup tables. The Brakerski-Gentry-Vaikuntanathan (BGV) and Brakerski/Fan-Vercauteren (BFV) schemes provide higher bootstrapping throughput than DM/CGGI for vectors of small integers or finite-field elements but do not support programmable bootstrapping.  The target audience is anyone interested in FHE. We intend to keep this paper up-to-date to include new bootstrapping results as they become available.},
  pubstate = {prepublished},
  keywords = {BFV,BGV,Bootstrapping,CGGI,CKKS,DM,FHEW,Fully Homomorphic Encryption,Programmable Bootstrapping,TFHE},
  annotation = {Publication info: Preprint.},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/Badawi2023.pdf}
}

@online{beck2024,
  title = {{{xLSTM}}: {{Extended Long Short-Term Memory}}},
  shorttitle = {{{xLSTM}}},
  author = {Beck, Maximilian and Pöppel, Korbinian and Spanring, Markus and Auer, Andreas and Prudnikova, Oleksandra and Kopp, Michael and Klambauer, Günter and Brandstetter, Johannes and Hochreiter, Sepp},
  date = {2024-05-07},
  eprint = {2405.04517},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2405.04517},
  url = {http://arxiv.org/abs/2405.04517},
  urldate = {2024-07-15},
  abstract = {In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/beck2024.pdf;/home/thomas/Zotero/storage/PNS37KYI/2405.html}
}

@online{ben-baruch2021,
  title = {Asymmetric {{Loss For Multi-Label Classification}}},
  author = {Ben-Baruch, Emanuel and Ridnik, Tal and Zamir, Nadav and Noy, Asaf and Friedman, Itamar and Protter, Matan and Zelnik-Manor, Lihi},
  date = {2021-07-29},
  eprint = {2009.14119},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2009.14119},
  url = {http://arxiv.org/abs/2009.14119},
  urldate = {2023-12-27},
  abstract = {In a typical multi-label setting, a picture contains on average few positive labels, and many negative ones. This positive-negative imbalance dominates the optimization process, and can lead to under-emphasizing gradients from positive labels during training, resulting in poor accuracy. In this paper, we introduce a novel asymmetric loss ("ASL"), which operates differently on positive and negative samples. The loss enables to dynamically down-weights and hard-thresholds easy negative samples, while also discarding possibly mislabeled samples. We demonstrate how ASL can balance the probabilities of different samples, and how this balancing is translated to better mAP scores. With ASL, we reach state-of-the-art results on multiple popular multi-label datasets: MS-COCO, Pascal-VOC, NUS-WIDE and Open Images. We also demonstrate ASL applicability for other tasks, such as single-label classification and object detection. ASL is effective, easy to implement, and does not increase the training time or complexity. Implementation is available at: https://github.com/Alibaba-MIIL/ASL.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,I.0,I.2.10,I.2.6,I.4.0},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/ben-baruch2021.pdf;/home/thomas/Zotero/storage/QGNK38MC/2009.html}
}

@book{bishop2006,
  title = {Pattern {{Recognition}} and {{Machine Learning}}},
  author = {Bishop, Christopher M.},
  date = {2006-08-17},
  eprint = {kTNoQgAACAAJ},
  eprinttype = {googlebooks},
  publisher = {Springer},
  abstract = {This is the first textbook on pattern recognition to present the Bayesian viewpoint. The book presents approximate inference algorithms that permit fast approximate answers in situations where exact answers are not feasible. It uses graphical models to describe probability distributions when no other books apply graphical models to machine learning. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory.},
  isbn = {978-0-387-31073-2},
  langid = {english},
  pagetotal = {738},
  keywords = {Computers / Computer Graphics,Computers / Computer Vision & Pattern Recognition,Computers / Intelligence (AI) & Semantics,Mathematics / Probability & Statistics / General}
}

@article{boneh1999,
  title = {{{TWENTY YEARS OF ATTACKS ON THE RSA CRYPTOSYSTEM}}},
  author = {Boneh, D.},
  date = {1999},
  journaltitle = {Notices of the American Mathematical Society},
  url = {https://www.semanticscholar.org/paper/TWENTY-YEARS-OF-ATTACKS-ON-THE-RSA-CRYPTOSYSTEM-Boneh/e85c8789db8a9379c60a1621878222bcbe7a11a3},
  urldate = {2024-08-14},
  abstract = {Introduction The RSA cryptosystem, invented by Ron Rivest, Adi Shamir, and Len Adleman [18], was first publicized in the August 1977 issue of Scientific American. The cryptosystem is most commonly used for providing privacy and ensuring authenticity of digital data. These days RSA is deployed in many commercial systems. It is used by Web servers and browsers to secure Web traffic, it is used to ensure privacy and authenticity of e-mail, it is used to secure remote login sessions, and it is at the heart of electronic credit card payment systems. In short, RSA is frequently used in applications where security of digital data is a concern. Since its initial publication, the RSA system has been analyzed for vulnerability by many researchers. Although twenty years of research have led to a number of fascinating attacks, none of them is devastating. They mostly illustrate the dangers of improper use of RSA. Indeed, securely implementing RSA is a nontrivial task. Our goal is to survey some of these attacks and describe the underlying mathematical tools they use. Throughout the survey we follow standard naming conventions and use “Alice” and “Bob” to denote two generic parties wishing to communicate with each other. We use “Marvin” to denote a malicious attacker wishing to eavesdrop or tamper with the communication between Alice and Bob. We begin by describing a simplified version of RSA encryption. Let N = pq be the product of two large primes of the same size (n/2 bits each). A typical size for N is n = 1024 bits, i.e., 309 decimal digits. Each of the factors is 512 bits. Let e, d be two integers satisfying ed = 1 mod φ(N) where φ(N) = (p − 1)(q − 1) is the order of the multiplicative group ZN. We call N the RSA modulus, e the encryption exponent, and d the decryption exponent. The pair 〈N, e〉 is the public key. As its name suggests, it is public and is used to encrypt messages. The pair 〈N,d〉 is called the secret key or private key and is known only to the recipient of encrypted messages. The secret key enables decryption of ciphertexts. A message is an integer M ∈ ZN. To encrypt M, one computes C =Me mod N . To decrypt the ciphertext, the legitimate receiver computes Cd mod N. Indeed, Cd =Med =M mod N,}
}

@inproceedings{bos2013,
  title = {Improved {{Security}} for a {{Ring-Based Fully Homomorphic Encryption Scheme}}},
  booktitle = {Cryptography and {{Coding}}},
  author = {Bos, Joppe W. and Lauter, Kristin and Loftus, Jake and Naehrig, Michael},
  editor = {Stam, Martijn},
  date = {2013},
  pages = {45--64},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-45239-0_4},
  abstract = {In 1996, Hoffstein, Pipher and Silverman introduced an efficient lattice based encryption scheme dubbed NTRUEncrypt. Unfortunately, this scheme lacks a proof of security. However, in 2011, Stehlé and Steinfeld showed how to modify NTRUEncrypt to reduce security to standard problems in ideal lattices. In 2012, López-Alt, Tromer and Vaikuntanathan proposed a fully homomorphic scheme based on this modified system. However, to allow homomorphic operations and prove security, a non-standard assumption is required. In this paper, we show how to remove this non-standard assumption via techniques introduced by Brakerski and construct a new fully homomorphic encryption scheme from the Stehlé and Steinfeld version based on standard lattice assumptions and a circular security assumption. The scheme is scale-invariant and therefore avoids modulus switching and the size of ciphertexts is one ring element. Moreover, we present a practical variant of our scheme, which is secure under stronger assumptions, along with parameter recommendations and promising implementation results. Finally, we present an approach for encrypting larger input sizes by extending ciphertexts to several ring elements via the CRT on the message space.},
  isbn = {978-3-642-45239-0},
  langid = {english},
  keywords = {Arithmetic Circuit,Homomorphic Encryption,Homomorphic Encryption Scheme,Message Space,Ring Element},
  file = {/home/thomas/Zotero/storage/JPF4DK25/Bos et al. - 2013 - Improved Security for a Ring-Based Fully Homomorphic Encryption Scheme.pdf}
}

@book{boyer2011,
  title = {A {{History}} of {{Mathematics}}},
  author = {Boyer, Carl B. and Merzbach, Uta C.},
  date = {2011-01-25},
  eprint = {bR9HAAAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {John Wiley \& Sons},
  abstract = {The updated new edition of the classic and comprehensive guide to the history of mathematics For more than forty years, A History of Mathematics has been the reference of choice for those looking to learn about the fascinating history of humankind’s relationship with numbers, shapes, and patterns. This revised edition features up-to-date coverage of topics such as Fermat’s Last Theorem and the Poincaré Conjecture, in addition to recent advances in areas such as finite group theory and computer-aided proofs.  Distills thousands of years of mathematics into a single, approachable volume Covers mathematical discoveries, concepts, and thinkers, from Ancient Egypt to the present Includes up-to-date references and an extensive chronological table of mathematical and general historical developments.  Whether you're interested in the age of Plato and Aristotle or Poincaré and Hilbert, whether you want to know more about the Pythagorean theorem or the golden mean, A History of Mathematics is an essential reference that will help you explore the incredible history of mathematics and the men and women who created it.},
  isbn = {978-0-470-63056-3},
  langid = {english},
  pagetotal = {695},
  keywords = {Mathematics / General,Mathematics / History & Philosophy}
}

@online{brakerski2011,
  title = {Fully {{Homomorphic Encryption}} without {{Bootstrapping}}},
  author = {Brakerski, Zvika and Gentry, Craig and Vaikuntanathan, Vinod},
  date = {2011},
  number = {2011/277},
  url = {https://eprint.iacr.org/2011/277},
  urldate = {2025-01-17},
  abstract = {We present a radically new approach to fully homomorphic encryption (FHE) that dramatically improves performance and bases security on weaker assumptions. A central conceptual contribution in our work is a new way of constructing leveled fully homomorphic encryption schemes (capable of evaluating arbitrary polynomial-size circuits), \{\textbackslash em without Gentry's bootstrapping procedure\}. Specifically, we offer a choice of FHE schemes based on the learning with error (LWE) or ring-LWE (RLWE) problems that have 2\textbackslash secparam security against known attacks. For RLWE, we have: 1. A leveled FHE scheme that can evaluate L-level arithmetic circuits with O\textasciitilde (\textbackslash secparam⋅L3) per-gate computation -- i.e., computation \{\textbackslash em quasi-linear\} in the security parameter. Security is based on RLWE for an approximation factor exponential in L. This construction does not use the bootstrapping procedure. 2. A leveled FHE scheme that uses bootstrapping \{\textbackslash em as an optimization\}, where the per-gate computation (which includes the bootstrapping procedure) is O\textasciitilde (\textbackslash secparam2), \{\textbackslash em independent of L\}. Security is based on the hardness of RLWE for \{\textbackslash em quasi-polynomial\} factors (as opposed to the sub-exponential factors needed in previous schemes). We obtain similar results for LWE, but with worse performance. We introduce a number of further optimizations to our schemes. As an example, for circuits of large width -- e.g., where a constant fraction of levels have width at least \textbackslash secparam -- we can reduce the per-gate computation of the bootstrapped version to O\textasciitilde (\textbackslash secparam), independent of L, by \{\textbackslash em batching the bootstrapping operation\}. Previous FHE schemes all required Ω\textasciitilde (\textbackslash secparam3.5) computation per gate. At the core of our construction is a much more effective approach for managing the noise level of lattice-based ciphertexts as homomorphic operations are performed, using some new techniques recently introduced by Brakerski and Vaikuntanathan (FOCS 2011).},
  pubstate = {prepublished},
  keywords = {cryptography,fully homomorphic encryption},
  annotation = {Publication info: Published elsewhere. Unknown where it was published},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/Brakerski2011.pdf}
}

@inproceedings{brakerski2012,
  title = {Fully {{Homomorphic Encryption}} without {{Modulus Switching}} from {{Classical GapSVP}}},
  booktitle = {Advances in {{Cryptology}} – {{CRYPTO}} 2012},
  author = {Brakerski, Zvika},
  editor = {Safavi-Naini, Reihaneh and Canetti, Ran},
  date = {2012},
  volume = {7417},
  pages = {868--886},
  publisher = {Springer Berlin Heidelberg},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-32009-5_50},
  url = {http://link.springer.com/10.1007/978-3-642-32009-5_50},
  urldate = {2024-06-27},
  abstract = {We present a new tensoring technique for LWE-based fully homomorphic encryption. While in all previous works, the ciphertext noise grows quadratically \$\$B \textbackslash rightarrow B\textasciicircum 2\textbackslash cdot \textbackslash text \{poly\}n\$\$ with every multiplication before "refreshing", our noise only grows linearly \$\$B \textbackslash rightarrow B\textbackslash cdot \textbackslash text \{poly\}n\$\$.    We use this technique to construct a scale-invariant fully homomorphic encryption scheme, whose properties only depend on the ratio between the modulus q and the initial noise level B, and not on their absolute values.    Our scheme has a number of advantages over previous candidates: It uses the same modulus throughout the evaluation process no need for "modulus switching", and this modulus can take arbitrary form. In addition, security can be classically reduced from the worst-case hardness of the GapSVP problem with quasi-polynomial approximation factor, whereas previous constructions could only exhibit a quantum reduction from GapSVP.},
  isbn = {978-3-642-32008-8 978-3-642-32009-5},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/brakerski2012.pdf}
}

@book{briggs2019,
  title = {Calculus},
  author = {Briggs, William L. and Cochran, Lyle and Gillett, Bernard and Schulz, Eric P.},
  date = {2019},
  eprint = {DNUGtAEACAAJ},
  eprinttype = {googlebooks},
  publisher = {Pearson},
  abstract = {For 3- to 4-semester courses covering single-variable and multivariable calculus, taken by students of mathematics, engineering, natural sciences, or economics.  T he most successful new calculus text in the last two decades  The much-anticipated 3rd Edition of Briggs' Calculus Series  retains its hallmark features while introducing important advances and refinements. Briggs, Cochran, Gillett, and Schulz build from a foundation of meticulously crafted exercise sets, then draw students into the narrative through writing that reflects the voice of the instructor. Examples are stepped out and thoughtfully annotated, and figures are designed to teach rather than simply supplement the narrative. The groundbreaking eBook contains approximately 700 Interactive Figures that can be manipulated to shed light on key concepts.  For the 3rd Edition, the authors synthesized feedback on the text and MyLab(TM) Math content from over 140 instructors and an Engineering Review Panel. This thorough and extensive review process, paired with the authors' own teaching experiences, helped create a text that was designed for today's calculus instructors and students.  Also available with MyLab Math  MyLab Math is the teaching and learning platform that empowers instructors to reach every student. By combining trusted author content with digital tools and a flexible platform, MyLab Math personalizes the learning experience and improves results for each student.    Note: You are purchasing a standalone product; MyLab Math does not come packaged with this content. Students, if interested in purchasing this title with MyLab Math, ask your instructor to confirm the correct package ISBN and Course ID. Instructors, contact your Pearson representative for more information.    If you would like to purchase both the physical text and MyLab Math, search for:  0134996720 / 9780134996721 Calculus and MyLab Math with Pearson eText - Title-Specific Access Card Package, 3/e Package consists of:  013476563X / 9780134765631 Calculus 013485683X / 9780134856834 MyLab Math with Pearson eText - Standalone Access Card - for Calculus},
  isbn = {978-0-13-476563-1},
  langid = {english},
  pagetotal = {1344}
}

@book{brogan1991,
  title = {Modern Control Theory (3rd Ed.)},
  author = {Brogan, William L.},
  date = {1991-02},
  publisher = {Prentice-Hall, Inc.},
  location = {USA},
  isbn = {978-0-13-589763-8},
  pagetotal = {653}
}

@book{brualdi2004,
  title = {Introductory {{Combinatorics}}},
  author = {Brualdi, Richard A.},
  date = {2004-01-01},
  edition = {Subsequent edition},
  publisher = {Pearson College Div},
  location = {Upper Saddle River, N.J},
  abstract = {This book emphasizes combinatorial ideas including the pigeon-hole principle, counting techniques, permutations and combinations, Pólya counting, binomial coefficients, inclusion-exclusion principle, generating functions and recurrence relations, and combinatortial structures (matchings, designs, graphs). The volume provides a complete examination of combinatorial ideas and techniques. For individuals interested in combinatorial concepts.},
  isbn = {978-0-13-100119-0},
  langid = {english},
  pagetotal = {608}
}

@book{burton2010,
  title = {Elementary {{Number Theory}}},
  author = {Burton, David M.},
  date = {2010-02-04},
  edition = {7th edition},
  publisher = {McGraw Hill Higher Education},
  location = {Boston},
  abstract = {Elementary Number Theory, Seventh Edition, is written for the one-semester undergraduate number theory course taken by math majors, secondary education majors, and computer science students. This contemporary text provides a simple account of classical number theory, set against a historical background that shows the subject's evolution from antiquity to recent research. Written in David Burton’s engaging style, Elementary Number Theory reveals the attraction that has drawn leading mathematicians and amateurs alike to number theory over the course of history.},
  isbn = {978-0-07-338314-9},
  langid = {english},
  pagetotal = {436}
}

@book{calinger2016,
  title = {Leonhard {{Euler}}: {{Mathematical Genius}} in the {{Enlightenment}}},
  shorttitle = {Leonhard {{Euler}}},
  author = {Calinger, Ronald S.},
  date = {2016},
  eprint = {j.ctv7h0smb},
  eprinttype = {jstor},
  publisher = {Princeton University Press},
  doi = {10.2307/j.ctv7h0smb},
  url = {https://www.jstor.org/stable/j.ctv7h0smb},
  urldate = {2024-08-15},
  abstract = {This is the first full-scale biography of Leonhard Euler (1707-83), one of the greatest mathematicians and theoretical physicists of all time. In this comprehensive and authoritative account, Ronald Calinger connects the story of Euler's eventful life to the astonishing achievements that place him in the company of Archimedes, Newton, and Gauss. Drawing chiefly on Euler's massive published works and correspondence, which fill more than eighty volumes so far, this biography sets Euler's work in its multilayered context-personal, intellectual, institutional, political, cultural, religious, and social. It is a story of nearly incessant accomplishment, from Euler's fundamental contributions to almost every area of pure and applied mathematics-especially calculus, number theory, notation, optics, and celestial, rational, and fluid mechanics-to his advancements in shipbuilding, telescopes, ballistics, cartography, chronology, and music theory.  The narrative takes the reader from Euler's childhood and education in Basel through his first period in St. Petersburg, 1727-41, where he gained a European reputation by solving the Basel problem and systematically developing analytical mechanics. Invited to Berlin by Frederick II, Euler published his famous \mkbibemph{Introductio in analysin infinitorum} , devised continuum mechanics, and proposed a pulse theory of light. Returning to St. Petersburg in 1766, he created the analytical calculus of variations, developed the most precise lunar theory of the time that supported Newton's dynamics, and published the best-selling \mkbibemph{Letters to a German Princess} -all despite eye problems that ended in near-total blindness. In telling the remarkable story of Euler and how his achievements brought pan-European distinction to the Petersburg and Berlin academies of sciences, the book also demonstrates with new depth and detail the central role of mathematics in the Enlightenment.  Some images inside the book are unavailable due to digital copyright restrictions.},
  isbn = {978-0-691-11927-4}
}

@online{chen2018,
  title = {Homomorphic {{Lower Digits Removal}} and {{Improved FHE Bootstrapping}}},
  author = {Chen, Hao and Han, Kyoohyung},
  date = {2018},
  number = {2018/067},
  url = {https://eprint.iacr.org/2018/067},
  urldate = {2025-01-14},
  abstract = {Bootstrapping is a crucial operation in Gentry's breakthrough work on fully homomorphic encryption (FHE), where a homomorphic encryption scheme evaluates its own decryption algorithm. There has been a couple of implementations of bootstrapping, among which HElib arguably marks the state-of-the-art in terms of throughput, ciphertext/message size ratio and support for large plaintext moduli. In this work, we apply a family of "lowest digit removal" polynomials to improve homomorphic digit extraction algorithm which is crucial part in bootstrapping for both FV and BGV schemes. If the secret key has 1-norm \$h=l\_1(s)\$ and the plaintext modulus is \$t = p\textasciicircum r\$, we achieved bootstrapping depth \$\textbackslash log h + \textbackslash log( \textbackslash log\_p(ht))\$ in FV scheme. In case of the BGV scheme, we bring down the depth from \$\textbackslash log h + 2 \textbackslash log t\$ to \$\textbackslash log h + \textbackslash log t\$. We implemented bootstrapping for FV in the SEAL library. Besides the regular mode, we introduce another "slim mode'", which restrict the plaintexts to batched vectors in \$\textbackslash mathbb\{Z\}\_\{p\textasciicircum r\}\$. The slim mode has similar throughput as the regular mode, while each individual run is much faster and uses much smaller memory. For example, bootstrapping takes \$6.75\$ seconds for 7 bit plaintext space with 64 slots and \$1381\$ seconds for \$GF(257\textasciicircum\{128\})\$ plaintext space with 128 slots. We also implemented our improved digit extraction procedure for the BGV scheme in HElib.},
  pubstate = {prepublished},
  keywords = {Bootstrapping,Fully Homomorphic Encryption},
  annotation = {Publication info: A minor revision of an IACR publication in EUROCRYPT 2018}
}

@inproceedings{chen2018a,
  title = {Homomorphic {{Lower Digits Removal}} and {{Improved FHE Bootstrapping}}},
  booktitle = {Advances in {{Cryptology}} – {{EUROCRYPT}} 2018},
  author = {Chen, Hao and Han, Kyoohyung},
  editor = {Nielsen, Jesper Buus and Rijmen, Vincent},
  date = {2018},
  pages = {315--337},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-78381-9_12},
  abstract = {Bootstrapping is a crucial operation in Gentry’s breakthrough work on fully homomorphic encryption (FHE), where a homomorphic encryption scheme evaluates its own decryption algorithm. There has been a couple of implementations of bootstrapping, among which HElib arguably marks the state-of-the-art in terms of throughput, ciphertext/message size ratio and support for large plaintext moduli.},
  isbn = {978-3-319-78381-9},
  langid = {english},
  keywords = {Bootstrapping,Homomorphic encryption,Implementation},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/Chen2018.pdf}
}

@online{cheon2016,
  title = {Homomorphic {{Encryption}} for {{Arithmetic}} of {{Approximate Numbers}}},
  author = {Cheon, Jung Hee and Kim, Andrey and Kim, Miran and Song, Yongsoo},
  date = {2016},
  number = {2016/421},
  url = {https://eprint.iacr.org/2016/421},
  urldate = {2025-01-17},
  abstract = {We suggest a method to construct a homomorphic encryption scheme for approximate arithmetic. It supports an approximate addition and multiplication of encrypted messages, together with a new rescaling procedure for managing the magnitude of plaintext. This procedure truncates a ciphertext into a smaller modulus, which leads to rounding of plaintext. The main idea is to add a noise following significant figures which contain a main message. This noise is originally added to the plaintext for security, but considered to be a part of error occurring during approximate computations that is reduced along with plaintext by rescaling. As a result, our decryption structure outputs an approximate value of plaintext with a predetermined precision. We also propose a new batching technique for a RLWE-based construction. A plaintext polynomial is an element of a cyclotomic ring of characteristic zero and it is mapped to a message vector of complex numbers via complex canonical embedding map, which is an isometric ring homomorphism. This transformation does not blow up the size of errors, therefore enables us to preserve the precision of plaintext after encoding. In our construction, the bit size of ciphertext modulus grows linearly with the depth of the circuit being evaluated due to rescaling procedure, while all the previous works either require an exponentially large size of modulus or expensive computations such as bootstrapping or bit extraction. One important feature of our method is that the precision loss during evaluation is bounded by the depth of a circuit and it exceeds at most one more bit compared to unencrypted approximate arithmetic such as floating-point operations. In addition to the basic approximate circuits, we show that our scheme can be applied to the efficient evaluation of transcendental functions such as multiplicative inverse, exponential function, logistic function and discrete Fourier transform.},
  pubstate = {prepublished},
  keywords = {approximate arithmetic,Homomorphic encryption},
  annotation = {Publication info: Published by the IACR in ASIACRYPT 2017},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/Cheon2016.pdf}
}

@article{chillotti,
  title = {{{CONCRETE}}: {{Concrete Operates oN Ciphertexts Rapidly}} by {{Extending TfhE}}},
  author = {Chillotti, Ilaria and Joye, Marc and Ligier, Damien and Orfila, Jean-Baptiste and Tap, Samuel},
  abstract = {Fully homomorphic encryption (FHE) extends traditional encryption schemes. It allows one to directly compute on encrypted data without requiring access to the decryption key. This paper introduces CONCRETE, an open-source library developed in Rust that builds on the state-of-art TFHE cryptosystem. It provides a userfriendly interface making FHE easy to integrate. The library deals with inputs of arbitrary format and comes with an extensive set of operations to play with ciphertexts, including a programmable bootstrapping. CONCRETE is available on GitHub at URL https:// github.com/zama-ai/concrete and on https://crates.io.},
  langid = {english},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/Chillotti.pdf}
}

@online{conci2018,
  title = {Distance {{Between Sets}} - {{A}} Survey},
  author = {Conci, A. and Kubrusly, C. S.},
  date = {2018-08-07},
  eprint = {1808.02574},
  eprinttype = {arXiv},
  eprintclass = {math},
  doi = {10.48550/arXiv.1808.02574},
  url = {http://arxiv.org/abs/1808.02574},
  urldate = {2024-02-21},
  abstract = {The purpose of this paper is to give a survey on the notions of distance between subsets either of a metric space or of a measure space, including definitions, a classification, and a discussion of the best-known distance functions, which is followed by a review on applications used in many areas of knowledge, ranging from theoretical to practical applications.},
  pubstate = {prepublished},
  keywords = {28A78 54E35,Mathematics - Functional Analysis},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/conci2018.pdf;/home/thomas/Zotero/storage/TTV8T363/1808.html}
}

@inproceedings{coppersmith1996,
  title = {Low-{{Exponent RSA}} with {{Related Messages}}},
  booktitle = {Advances in {{Cryptology}} — {{EUROCRYPT}} ’96},
  author = {Coppersmith, Don and Franklin, Matthew and Patarin, Jacques and Reiter, Michael},
  editor = {Maurer, Ueli},
  date = {1996},
  pages = {1--9},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/3-540-68339-9_1},
  abstract = {In this paper we present a new class of attacks against RSA with low encrypting exponent. The attacks enable the recovery of plain- text messages from their ciphertexts and a known polynomial relationship among the messages, provided that the ciphertexts were created using the same RSA public key with low encrypting exponent.},
  isbn = {978-3-540-68339-1},
  langid = {english},
  keywords = {Linear Polynomial,Polynomial Relation,Secret Sharing Scheme,Univariate Polynomial,Verifiable Signature},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/coppersmith1996.pdf}
}

@book{cormen2022,
  title = {Introduction to {{Algorithms}}, Fourth Edition},
  author = {Cormen, Thomas H. and Leiserson, Charles E. and Rivest, Ronald L. and Stein, Clifford},
  date = {2022-04-05},
  eprint = {HOJyzgEACAAJ},
  eprinttype = {googlebooks},
  publisher = {MIT Press},
  abstract = {A comprehensive update of the leading algorithms text, with new material on matchings in bipartite graphs, online algorithms, machine learning, and other topics.Some books on algorithms are rigorous but incomplete; others cover masses of material but lack rigor. Introduction to Algorithms uniquely combines rigor and comprehensiveness. It covers a broad range of algorithms in depth, yet makes their design and analysis accessible to all levels of readers, with self-contained chapters and algorithms in pseudocode. Since the publication of the first edition, Introduction to Algorithms has become the leading algorithms text in universities worldwide as well as the standard reference for professionals. This fourth edition has been updated throughout.New for the fourth edition New chapters on matchings in bipartite graphs, online algorithms, and machine learningNew material on topics including solving recurrence equations, hash tables, potential functions, and suffix arrays140 new exercises and 22 new problemsReader feedback–informed improvements to old problemsClearer, more personal, and gender-neutral writing styleColor added to improve visual presentationNotes, bibliography, and index updated to reflect developments in the fieldWebsite with new supplementary materialWarning: Avoid counterfeit copies of Introduction to Algorithms by buying only from reputable retailers. Counterfeit and pirated copies are incomplete and contain errors.},
  isbn = {978-0-262-04630-5},
  langid = {english},
  pagetotal = {1313},
  keywords = {Computers / Computer Science,Computers / Programming / Algorithms,Computers / Reference}
}

@online{devlin2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  date = {2019-05-24},
  eprint = {1810.04805},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1810.04805},
  url = {http://arxiv.org/abs/1810.04805},
  urldate = {2023-12-05},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  pubstate = {prepublished},
  version = {2},
  keywords = {Computer Science - Computation and Language},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/devlin2019.pdf;/home/thomas/Zotero/storage/KEMTIW53/1810.html}
}

@article{diffie1976,
  title = {New Directions in Cryptography},
  author = {Diffie, W. and Hellman, M.},
  date = {1976-11},
  journaltitle = {IEEE Transactions on Information Theory},
  volume = {22},
  number = {6},
  pages = {644--654},
  issn = {1557-9654},
  doi = {10.1109/TIT.1976.1055638},
  url = {https://ieeexplore.ieee.org/document/1055638},
  urldate = {2024-08-14},
  abstract = {Two kinds of contemporary developments in cryptography are examined. Widening applications of teleprocessing have given rise to a need for new types of cryptographic systems, which minimize the need for secure key distribution channels and supply the equivalent of a written signature. This paper suggests ways to solve these currently open problems. It also discusses how the theories of communication and computation are beginning to provide the tools to solve cryptographic problems of long standing.},
  eventtitle = {{{IEEE Transactions}} on {{Information Theory}}},
  keywords = {Authentication,Business,Costs,Cryptography,Eavesdropping,Public key cryptography,Receivers},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/diffie1976.pdf;/home/thomas/Zotero/storage/H2HDYABX/1055638.html}
}

@book{dummit2003,
  title = {Abstract {{Algebra}}},
  author = {Dummit, David S. and Foote, Richard M.},
  date = {2003-07-14},
  eprint = {KJDBQgAACAAJ},
  eprinttype = {googlebooks},
  publisher = {John Wiley \& Sons},
  abstract = {This revision of Dummit and Foote's widely acclaimed introduction to abstract algebra helps students experience the power and beauty that develops from the rich interplay between different areas of mathematics.   The book carefully develops the theory of different algebraic structures, beginning from basic definitions to some in-depth results, using numerous examples and exercises to aid the student's understanding. With this approach, students gain an appreciation for how mathematical structures and their interplay lead to powerful results and insights in a number of different settings.  The text is designed for a full-year introduction to abstract algebra at the advanced undergraduate or graduate level, but contains substantially more material than would normally be covered in one year. Portions of the book may also be used for various one-semester topics courses in advanced algebra, each of which would provide a solid background for a follow-up course delving more deeply into one of many possible areas: algebraic number theory, algebraic topology, algebraic geometry, representation theory, Lie groups, etc.},
  isbn = {978-0-471-43334-7},
  langid = {english},
  pagetotal = {944},
  keywords = {Mathematics / Algebra / General,Mathematics / General}
}

@article{elgamal1985,
  title = {A Public Key Cryptosystem and a Signature Scheme Based on Discrete Logarithms},
  author = {Elgamal, T.},
  date = {1985-07},
  journaltitle = {IEEE Transactions on Information Theory},
  volume = {31},
  number = {4},
  pages = {469--472},
  issn = {1557-9654},
  doi = {10.1109/TIT.1985.1057074},
  url = {https://ieeexplore.ieee.org/document/1057074},
  urldate = {2024-08-27},
  abstract = {A new signature scheme is proposed, together with an implementation of the Diffie-Hellman key distribution scheme that achieves a public key cryptosystem. The security of both systems relies on the difficulty of computing discrete logarithms over finite fields.},
  eventtitle = {{{IEEE Transactions}} on {{Information Theory}}},
  file = {/home/thomas/Zotero/storage/VKP2FNCN/1057074.html}
}

@unpublished{elhousni2018,
  title = {Introduction to the {{Mathematical Foundations}} of {{Elliptic Curve Cryptography}}},
  author = {El Housni, Youssef},
  date = {2018-12},
  url = {https://hal.science/hal-01914807},
  urldate = {2024-09-07},
  abstract = {The history of cryptography can be split into two eras: the classical era and the modern era. The turning point between the two occurred when asymmetric cryptography was introduced. Elliptic curves are one of the most interesting tools used in this asymmetric approach. Here, we introduce the mathematical foundation of elliptic curve cryptography.},
  keywords = {Algebra geometry,Cryptography research,Elliptic Curve Cryptography},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/elhousni2018.pdf}
}

@online{fan2012,
  title = {Somewhat {{Practical Fully Homomorphic Encryption}}},
  author = {Fan, Junfeng and Vercauteren, Frederik},
  date = {2012},
  number = {2012/144},
  url = {https://eprint.iacr.org/2012/144},
  urldate = {2024-11-08},
  abstract = {In this paper we port Brakerski's fully homomorphic scheme based on the Learning With Errors (LWE) problem to the ring-LWE setting. We introduce two optimised versions of relinearisation that not only result in a smaller relinearisation key, but also faster computations. We provide a detailed, but simple analysis of the various homomorphic operations, such as multiplication, relinearisation and bootstrapping, and derive tight worst case bounds on the noise caused by these operations. The analysis of the bootstrapping step is greatly simplified by using a modulus switching trick. Finally, we derive concrete parameters for which the scheme provides a given level of security and becomes fully homomorphic.},
  pubstate = {prepublished},
  keywords = {Fully homomorphic encryption},
  annotation = {Publication info: Published elsewhere. Unknown where it was published},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/Fan2012.pdf}
}

@book{folland2009,
  title = {Fourier {{Analysis}} and {{Its Applications}}},
  author = {Folland, Gerald B.},
  date = {2009-01-13},
  publisher = {American Mathematical Society},
  location = {Providence, RI},
  abstract = {This book presents the theory and applications of Fourier series and integrals, eigenfunction expansions, and related topics, on a level suitable for advanced undergraduates. It includes material on Bessel functions, orthogonal polynomials, and Laplace transforms, and it concludes with chapters on generalized functions and Green's functions for ordinary and partial differential equations. The book deals almost exclusively with aspects of these subjects that are useful in physics and engineering, and includes a wide variety of applications. On the theoretical side, it uses ideas from modern analysis to develop the concepts and reasoning behind the techniques without getting bogged down in the technicalities of rigorous proofs.},
  isbn = {978-0-8218-4790-9},
  langid = {english},
  pagetotal = {433}
}

@book{forman2015,
  title = {The {{Whole Truth About Whole Numbers}}: {{An Elementary Introduction}} to {{Number Theory}}},
  shorttitle = {The {{Whole Truth About Whole Numbers}}},
  author = {Forman, Sylvia and Rash, Agnes M.},
  date = {2015},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-11035-6},
  url = {https://link.springer.com/10.1007/978-3-319-11035-6},
  urldate = {2024-08-12},
  isbn = {978-3-319-11034-9 978-3-319-11035-6},
  langid = {english},
  keywords = {Division algorithm,Euclidean algorithm,Integers,Number theory,Pythagorean triplets,Whole numbers},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/forman2015.pdf}
}

@article{ge2014,
  title = {Optimized {{Product Quantization}}},
  author = {Ge, Tiezheng and He, Kaiming and Ke, Qifa and Sun, Jian},
  date = {2014-04},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {36},
  number = {4},
  pages = {744--755},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2013.240},
  url = {https://ieeexplore.ieee.org/document/6678503},
  urldate = {2023-11-13},
  abstract = {Product quantization (PQ) is an effective vector quantization method. A product quantizer can generate an exponentially large codebook at very low memory/time cost. The essence of PQ is to decompose the high-dimensional vector space into the Cartesian product of subspaces and then quantize these subspaces separately. The optimal space decomposition is important for the PQ performance, but still remains an unaddressed issue. In this paper, we optimize PQ by minimizing quantization distortions w.r.t the space decomposition and the quantization codebooks. We present two novel solutions to this challenging optimization problem. The first solution iteratively solves two simpler sub-problems. The second solution is based on a Gaussian assumption and provides theoretical analysis of the optimality. We evaluate our optimized product quantizers in three applications: (i) compact encoding for exhaustive ranking [1], (ii) building inverted multi-indexing for non-exhaustive search [2], and (iii) compacting image representations for image retrieval [3]. In all applications our optimized product quantizers outperform existing solutions.},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/ge2014.pdf;/home/thomas/Zotero/storage/3HGULA2E/6678503.html}
}

@article{geelen,
  title = {Bootstrapping {{Algorithms}} for {{BGV}} and {{FV}}},
  author = {Geelen, Robin},
  langid = {english},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/Geelen.pdf}
}

@online{geelen2022,
  title = {Bootstrapping for {{BGV}} and {{BFV Revisited}}},
  author = {Geelen, Robin and Vercauteren, Frederik},
  date = {2022},
  number = {2022/1363},
  url = {https://eprint.iacr.org/2022/1363},
  urldate = {2024-11-12},
  abstract = {We unify the state-of-the-art bootstrapping algorithms for BGV and BFV in a single framework, and show that both schemes can be bootstrapped with identical complexity. This result corrects a claim by Chen and Han (Eurocrypt 2018) that BFV is more efficient to bootstrap than BGV. We also fix an error in their optimized procedure for power-of-two cyclotomics, which occurs for some parameter sets. Our analysis is simpler, yet more general than earlier work, in that it simultaneously covers both BGV and BFV. Furthermore, we also design and implement a high-level open source software library for bootstrapping in the Magma Computer Algebra System. It is the first library to support both BGV and BFV bootstrapping in full generality, with all recent techniques (including the above fixes) and trade-offs.},
  pubstate = {prepublished},
  keywords = {Bootstrapping,Brakerski-Fan-Vercauteren,Brakerski-Gentry-Vaikuntanathan,Fully homomorphic encryption,Recryption},
  annotation = {Publication info: A minor revision of an IACR publication in JOC 2023},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/Geelen2022.pdf}
}

@misc{gentry2009,
  title = {Fully Homomorphic Encryption Using Ideal Lattices.},
  author = {Gentry, Craig},
  date = {2009},
  url = {crypto.stanford.edu/craig},
  organization = {Stanford University}
}

@inproceedings{gentry2013,
  title = {Homomorphic {{Encryption}} from {{Learning}} with {{Errors}}: {{Conceptually-Simpler}}, {{Asymptotically-Faster}}, {{Attribute-Based}}},
  shorttitle = {Homomorphic {{Encryption}} from {{Learning}} with {{Errors}}},
  booktitle = {Advances in {{Cryptology}} – {{CRYPTO}} 2013},
  author = {Gentry, Craig and Sahai, Amit and Waters, Brent},
  editor = {Canetti, Ran and Garay, Juan A.},
  date = {2013},
  pages = {75--92},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-40041-4_5},
  abstract = {We describe a comparatively simple fully homomorphic encryption (FHE) scheme based on the learning with errors (LWE) problem. In previous LWE-based FHE schemes, multiplication is a complicated and expensive step involving “relinearization”. In this work, we propose a new technique for building FHE schemes that we call the approximate eigenvector method. In our scheme, for the most part, homomorphic addition and multiplication are just matrix addition and multiplication. This makes our scheme both asymptotically faster and (we believe) easier to understand.},
  isbn = {978-3-642-40041-4},
  langid = {english},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/gentry2013.pdf}
}

@online{greene2019,
  title = {Econometric {{Analysis}}},
  author = {Greene, William},
  date = {2019},
  publisher = {Pearson Deutschland},
  url = {https://elibrary.pearson.de/book/99.150005/9781292231150},
  urldate = {2023-12-23},
  abstract = {{$<$}br{$><$}h4{$>$}For first-year graduate courses in Econometrics for Social Scientists.{$<$}/h4{$>$} {$<$}br{$><$}p{$>$}Bridging the gap between social science studies and econometric analysis Designed to bridge the gap between social science studies and field-econometrics, Econometric Analysis, 8th Edition, Global Edition, presents this ever-growing area at an accessible graduate level. The book first introduces students to basic techniques, a rich variety of models, and underlying theory that is easy to put into practice. It then presents students with a sufficient theoretical background to understand advanced techniques and to recognise new variants of established models. This focus, along with hundreds of worked numerical examples, ensures that students can apply the theory to real-world application and are prepared to be successful economists in the field.{$<$}/p{$>$} {$<$}br{$><$}br{$>$}},
  isbn = {9781292231150},
  langid = {english},
  file = {/home/thomas/Zotero/storage/NE6XIYDM/9781292231150.html}
}

@book{grimmett1986,
  title = {Probability: {{An Introduction}}},
  shorttitle = {Probability},
  author = {Grimmett, Geoffrey and Welsh, D. J. A.},
  date = {1986},
  eprint = {DyifaCLXxkIC},
  eprinttype = {googlebooks},
  publisher = {Clarendon Press},
  abstract = {This new undergraduate text offers a concise introduction to probability and random processes. Exercises and problems range from simple to difficult, and the overall treatment, though elementary, includes rigorous mathematical arguments. Chapters contain core material for a beginning course in probability, a treatment of joint distributions leading to accounts of moment-generating functions, the law of large numbers and the central limit theorem, and basic random processes.},
  isbn = {978-0-19-853264-4},
  langid = {english},
  pagetotal = {230}
}

@article{gritsenko2017,
  title = {Neural {{Distributed Autoassociative Memories}}: {{A Survey}}},
  shorttitle = {Neural {{Distributed Autoassociative Memories}}},
  author = {Gritsenko, V. I. and Rachkovskij, D. A. and Frolov, A. A. and Gayler, R. and Kleyko, D. and Osipov, E.},
  date = {2017-06-12},
  journaltitle = {Kibern. vyčisl. teh.},
  volume = {2017},
  eprint = {1709.00848},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {5--35},
  issn = {04549910, 25192205},
  doi = {10.15407/kvt188.02.005},
  url = {http://arxiv.org/abs/1709.00848},
  urldate = {2024-07-27},
  abstract = {Introduction. Neural network models of autoassociative, distributed memory allow storage and retrieval of many items (vectors) where the number of stored items can exceed the vector dimension (the number of neurons in the network). This opens the possibility of a sublinear time search (in the number of stored items) for approximate nearest neighbors among vectors of high dimension. The purpose of this paper is to review models of autoassociative, distributed memory that can be naturally implemented by neural networks (mainly with local learning rules and iterative dynamics based on information locally available to neurons). Scope. The survey is focused mainly on the networks of Hopfield, Willshaw and Potts, that have connections between pairs of neurons and operate on sparse binary vectors. We discuss not only autoassociative memory, but also the generalization properties of these networks. We also consider neural networks with higher-order connections and networks with a bipartite graph structure for non-binary data with linear constraints. Conclusions. In conclusion we discuss the relations to similarity search, advantages and drawbacks of these techniques, and topics for further research. An interesting and still not completely resolved question is whether neural autoassociative memories can search for approximate nearest neighbors faster than other index structures for similarity search, in particular for the case of very high dimensional vectors.},
  issue = {2(188)},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/gritsenko2017.pdf;/home/thomas/Zotero/storage/T5IK487F/1709.html}
}

@online{gu2022,
  title = {Efficiently {{Modeling Long Sequences}} with {{Structured State Spaces}}},
  author = {Gu, Albert and Goel, Karan and Ré, Christopher},
  date = {2022-08-05},
  eprint = {2111.00396},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2111.00396},
  url = {http://arxiv.org/abs/2111.00396},
  urldate = {2024-01-01},
  abstract = {A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of \$10000\$ or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) \textbackslash ( x'(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t) \textbackslash ), and showed that for appropriate choices of the state matrix \textbackslash ( A \textbackslash ), this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning \textbackslash ( A \textbackslash ) with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91\textbackslash\% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation \$60\textbackslash times\$ faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/gu2022.pdf;/home/thomas/Zotero/storage/LGUIB4RK/2111.html}
}

@online{gu2023,
  title = {Mamba: {{Linear-Time Sequence Modeling}} with {{Selective State Spaces}}},
  shorttitle = {Mamba},
  author = {Gu, Albert and Dao, Tri},
  date = {2023-12-01},
  eprint = {2312.00752},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.00752},
  url = {http://arxiv.org/abs/2312.00752},
  urldate = {2023-12-31},
  abstract = {Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5\$\textbackslash times\$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/gu2023.pdf;/home/thomas/Zotero/storage/3SUGQ2QC/2312.html}
}

@online{guo2015,
  title = {Quantization Based {{Fast Inner Product Search}}},
  author = {Guo, Ruiqi and Kumar, Sanjiv and Choromanski, Krzysztof and Simcha, David},
  date = {2015-09-04},
  eprint = {1509.01469},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1509.01469},
  url = {http://arxiv.org/abs/1509.01469},
  urldate = {2023-11-13},
  abstract = {We propose a quantization based approach for fast approximate Maximum Inner Product Search (MIPS). Each database vector is quantized in multiple subspaces via a set of codebooks, learned directly by minimizing the inner product quantization error. Then, the inner product of a query to a database vector is approximated as the sum of inner products with the subspace quantizers. Different from recently proposed LSH approaches to MIPS, the database vectors and queries do not need to be augmented in a higher dimensional feature space. We also provide a theoretical analysis of the proposed approach, consisting of the concentration results under mild assumptions. Furthermore, if a small sample of example queries is given at the training time, we propose a modified codebook learning procedure which further improves the accuracy. Experimental results on a variety of datasets including those arising from deep neural networks show that the proposed approach significantly outperforms the existing state-of-the-art.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/guo2015.pdf;/home/thomas/Zotero/storage/PUC7YB59/1509.html}
}

@online{guo2020,
  title = {Accelerating {{Large-Scale Inference}} with {{Anisotropic Vector Quantization}}},
  author = {Guo, Ruiqi and Sun, Philip and Lindgren, Erik and Geng, Quan and Simcha, David and Chern, Felix and Kumar, Sanjiv},
  date = {2020-12-04},
  eprint = {1908.10396},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1908.10396},
  url = {http://arxiv.org/abs/1908.10396},
  urldate = {2023-11-13},
  abstract = {Quantization based techniques are the current state-of-the-art for scaling maximum inner product search to massive databases. Traditional approaches to quantization aim to minimize the reconstruction error of the database points. Based on the observation that for a given query, the database points that have the largest inner products are more relevant, we develop a family of anisotropic quantization loss functions. Under natural statistical assumptions, we show that quantization with these loss functions leads to a new variant of vector quantization that more greatly penalizes the parallel component of a datapoint's residual relative to its orthogonal component. The proposed approach achieves state-of-the-art results on the public benchmarks available at \textbackslash url\{ann-benchmarks.com\}.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/guo2020.pdf;/home/thomas/Zotero/storage/E6AVEC3F/1908.html}
}

@online{halevi2020,
  title = {Design and Implementation of {{HElib}}: A Homomorphic Encryption Library},
  shorttitle = {Design and Implementation of {{HElib}}},
  author = {Halevi, Shai and Shoup, Victor},
  date = {2020},
  number = {2020/1481},
  url = {https://eprint.iacr.org/2020/1481},
  urldate = {2024-11-12},
  abstract = {HElib is a C++ open source library (see https://github.com/homenc/HElib) that implements both the BGV and CKKS fully homomorphic encryption (FHE) schemes. This document summarizes some of the basic design principles of HElib, and describes some of its fundamental algorithms and data structures in signi\&\#64257;cant detail. It is a work in progress, and currently focuses exclusively on the BGV scheme.},
  pubstate = {prepublished},
  keywords = {fully homomorphic encryption},
  annotation = {Publication info: Preprint. MINOR revision.},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/Halevi2020.pdf}
}

@book{hall2015,
  title = {Lie {{Groups}}, {{Lie Algebras}}, and {{Representations}}: {{An Elementary Introduction}}},
  shorttitle = {Lie {{Groups}}, {{Lie Algebras}}, and {{Representations}}},
  author = {Hall, Brian C.},
  date = {2015},
  series = {Graduate {{Texts}} in {{Mathematics}}},
  volume = {222},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-13467-3},
  url = {https://link.springer.com/10.1007/978-3-319-13467-3},
  urldate = {2024-03-15},
  isbn = {978-3-319-13466-6 978-3-319-13467-3},
  langid = {english},
  keywords = {Baker-Campbell-Hausdorff formula,Cartan-Weyl theory,Lie algebras,Lie groups,representation theory},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/hall2015.pdf}
}

@online{hornrogara,
  title = {Matrix Analysis 2nd Edition | {{Algebra}} | {{Cambridge University Press}}},
  author = {Horn, Rogar A},
  url = {https://www.cambridge.org/de/universitypress/subjects/mathematics/algebra/matrix-analysis-2nd-edition?format=PB&isbn=9780521548236},
  urldate = {2023-11-29},
  abstract = {Linear algebra and matrix theory are fundamental tools in mathematical and physical science, as well as fertile fields for research. This second edition of this acclaimed text presents results of both classic and recent matrix analysis using canonical forms as a unifying theme and demonstrates their importance in a variety of applications. This thoroughly revised and updated second edition is a text for a second course on linear algebra and has more than 1,100 problems and exercises, new sections on the singular value and CS decompositions and the Weyr canonical form, expanded treatments of inverse problems and of block matrices, and much more.},
  file = {/home/thomas/Downloads/Roger A. Horn, Charles R. Johnson - Matrix Analysis-Cambridge University Press (2013).pdf;/home/thomas/Zotero/storage/S4UW9BI7/matrix-analysis-2nd-edition.html}
}

@book{howell2016,
  title = {Principles of {{Fourier Analysis}}},
  author = {Howell, Kenneth B.},
  date = {2016-12-12},
  edition = {2nd edition},
  publisher = {CRC Press},
  abstract = {Fourier analysis is one of the most useful and widely employed sets of tools for the engineer, the scientist, and the applied mathematician. As such, students and practitioners in these disciplines need a practical and mathematically solid introduction to its principles. They need straightforward verifications of its results and formulas, and they need clear indications of the limitations of those results and formulas.Principles of Fourier Analysis furnishes all this and more. It provides a comprehensive overview of the mathematical theory of Fourier analysis, including the development of Fourier series, "classical" Fourier transforms, generalized Fourier transforms and analysis, and the discrete theory. Much of the author's development is strikingly different from typical presentations. His approach to defining the classical Fourier transform results in a much cleaner, more coherent theory that leads naturally to a starting point for the generalized theory. He also introduces a new generalized theory based on the use of Gaussian test functions that yields an even more general -yet simpler -theory than usually presented.Principles of Fourier Analysis stimulates the appreciation and understanding of the fundamental concepts and serves both beginning students who have seen little or no Fourier analysis as well as the more advanced students who need a deeper understanding. Insightful, non-rigorous derivations motivate much of the material, and thought-provoking examples illustrate what can go wrong when formulas are misused. With clear, engaging exposition, readers develop the ability to intelligently handle the more sophisticated mathematics that Fourier analysis ultimately requires.},
  langid = {english},
  pagetotal = {791}
}

@book{ibe2014,
  title = {Fundamentals of {{Applied Probability}} and {{Random Processes}}},
  author = {Ibe, Oliver},
  date = {2014-07-28},
  edition = {2nd edition},
  publisher = {Academic Press},
  location = {Amsterdam Boston},
  isbn = {978-0-12-800852-2},
  langid = {english},
  pagetotal = {456}
}

@article{iliashenko2021,
  title = {Faster Homomorphic Comparison Operations for {{BGV}} and {{BFV}}},
  author = {Iliashenko, Ilia and Zucca, Vincent},
  date = {2021-07-01},
  journaltitle = {Proceedings on Privacy Enhancing Technologies},
  volume = {2021},
  number = {3},
  pages = {246--264},
  issn = {2299-0984},
  doi = {10.2478/popets-2021-0046},
  url = {https://petsymposium.org/popets/2021/popets-2021-0046.php},
  urldate = {2024-10-01},
  abstract = {Abstract             Fully homomorphic encryption (FHE) allows to compute any function on encrypted values. However, in practice, there is no universal FHE scheme that is effi-cient in all possible use cases. In this work, we show that FHE schemes suitable for arithmetic circuits (e.g. BGV or BFV) have a similar performance as FHE schemes for non-arithmetic circuits (TFHE) in basic comparison tasks such as less-than, maximum and minimum operations. Our implementation of the less-than function in the HElib library is up to 3 times faster than the prior work based on BGV/BFV. It allows to compare a pair of 64-bit integers in 11 milliseconds, sort 64 32-bit integers in 19 seconds and find the minimum of 64 32-bit integers in 9.5 seconds on an average laptop without multi-threading.},
  langid = {english},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/Iliashenko and Zucca - 2021 - Faster homomorphic comparison operations for BGV and BFV.pdf}
}

@article{iliashenko2021b,
  title = {Faster Homomorphic Comparison Operations for {{BGV}} and {{BFV}}},
  author = {Iliashenko, Ilia and Zucca, Vincent},
  date = {2021-07-01},
  journaltitle = {Proceedings on Privacy Enhancing Technologies},
  volume = {2021},
  number = {3},
  pages = {246--264},
  issn = {2299-0984},
  doi = {10.2478/popets-2021-0046},
  url = {https://petsymposium.org/popets/2021/popets-2021-0046.php},
  urldate = {2024-10-02},
  abstract = {Abstract             Fully homomorphic encryption (FHE) allows to compute any function on encrypted values. However, in practice, there is no universal FHE scheme that is effi-cient in all possible use cases. In this work, we show that FHE schemes suitable for arithmetic circuits (e.g. BGV or BFV) have a similar performance as FHE schemes for non-arithmetic circuits (TFHE) in basic comparison tasks such as less-than, maximum and minimum operations. Our implementation of the less-than function in the HElib library is up to 3 times faster than the prior work based on BGV/BFV. It allows to compare a pair of 64-bit integers in 11 milliseconds, sort 64 32-bit integers in 19 seconds and find the minimum of 64 32-bit integers in 9.5 seconds on an average laptop without multi-threading.},
  langid = {english},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/Iliashenko2021.pdf}
}

@book{imhausen2007,
  title = {The {{Mathematics}} of {{Egypt}}, {{Mesopotamia}}, {{China}}, {{India}}, and {{Islam}}: {{A Sourcebook}}},
  shorttitle = {The {{Mathematics}} of {{Egypt}}, {{Mesopotamia}}, {{China}}, {{India}}, and {{Islam}}},
  author = {Imhausen, Annette and Robson, Eleanor and Dauben, Joseph W. and Plofker, Kim and Berggren, J. Lennart},
  editor = {Katz, Victor J.},
  editortype = {redactor},
  date = {2007},
  eprint = {j.ctv1qgnq84},
  eprinttype = {jstor},
  publisher = {Princeton University Press},
  doi = {10.2307/j.ctv1qgnq84},
  url = {https://www.jstor.org/stable/j.ctv1qgnq84},
  urldate = {2024-08-14},
  abstract = {In recent decades it has become obvious that mathematics has always been a worldwide activity. But this is the first book to provide a substantial collection of English translations of key mathematical texts from the five most important ancient and medieval non-Western mathematical cultures, and to put them into full historical and mathematical context. {$<$}em{$>$}The Mathematics of Egypt, Mesopotamia, China, India, and Islam{$<$}/em{$>$} gives English readers a firsthand understanding and appreciation of these cultures' important contributions to world mathematics. The five section authors--Annette Imhausen (Egypt), Eleanor Robson (Mesopotamia), Joseph Dauben (China), Kim Plofker (India), and J. Lennart Berggren (Islam)--are experts in their fields. Each author has selected key texts and in many cases provided new translations. The authors have also written substantial section introductions that give an overview of each mathematical culture and explanatory notes that put each selection into context. This authoritative commentary allows readers to understand the sometimes unfamiliar mathematics of these civilizations and the purpose and significance of each text. Addressing a critical gap in the mathematics literature in English, this book is an essential resource for anyone with at least an undergraduate degree in mathematics who wants to learn about non-Western mathematical developments and how they helped shape and enrich world mathematics. The book is also an indispensable guide for mathematics teachers who want to use non-Western mathematical ideas in the classroom.},
  isbn = {978-0-691-11485-9}
}

@inproceedings{jadon2020,
  title = {A Survey of Loss Functions for Semantic Segmentation},
  booktitle = {2020 {{IEEE Conference}} on {{Computational Intelligence}} in {{Bioinformatics}} and {{Computational Biology}} ({{CIBCB}})},
  author = {Jadon, Shruti},
  date = {2020-10},
  pages = {1--7},
  doi = {10.1109/CIBCB48159.2020.9277638},
  url = {https://ieeexplore.ieee.org/abstract/document/9277638},
  urldate = {2023-12-16},
  abstract = {Image Segmentation has been an active field of research as it has a wide range of applications, ranging from automated disease detection to self driving cars. In the past five years, various papers came up with different objective loss functions used in different cases such as biased data, sparse segmentation, etc. In this paper, we have summarized some of the well-known loss functions widely used for Image Segmentation and listed out the cases where their usage can help in fast and better convergence of a model. Furthermore, we have also introduced a new log-cosh dice loss function and compared its performance on NBFS skull-segmentation open source data-set with widely used loss functions. We also showcased that certain loss functions perform well across all data-sets and can be taken as a good baseline choice in unknown data distribution scenarios.},
  eventtitle = {2020 {{IEEE Conference}} on {{Computational Intelligence}} in {{Bioinformatics}} and {{Computational Biology}} ({{CIBCB}})},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/jadon2020.pdf;/home/thomas/Zotero/storage/S689HGHR/9277638.html}
}

@online{jain2023,
  title = {Revisiting {{Fully Homomorphic Encryption Schemes}}},
  author = {Jain, Nimish and Cherukuri, Aswani Kumar},
  date = {2023-05-10},
  eprint = {2305.05904},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.05904},
  url = {http://arxiv.org/abs/2305.05904},
  urldate = {2024-08-30},
  abstract = {Homomorphic encryption is a sophisticated encryption technique that allows computations on encrypted data to be done without the requirement for decryption. This trait makes homomorphic encryption appropriate for safe computation in sensitive data scenarios, such as cloud computing, medical data exchange, and financial transactions. The data is encrypted using a public key in homomorphic encryption, and the calculation is conducted on the encrypted data using an algorithm that retains the encryption. The computed result is then decrypted with a private key to acquire the final output. This abstract notion protects data while allowing complicated computations to be done on the encrypted data, resulting in a secure and efficient approach to analysing sensitive information. This article is intended to give a clear idea about the various fully Homomorphic Encryption Schemes present in the literature and analyse and compare the results of each of these schemes. Further, we also provide applications and open-source tools of homomorphic encryption schemes.},
  pubstate = {prepublished},
  keywords = {Computer Science - Cryptography and Security},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/jain2023.pdf;/home/thomas/Zotero/storage/TXPZ2SG4/2305.html}
}

@book{jammalamadaka2001,
  title = {Topics in {{Circular Statistics}}},
  author = {Jammalamadaka, S. Rao and Sengupta, Ambar and Sengupta, Ashis},
  date = {2001},
  eprint = {sKqWMGqQXQkC},
  eprinttype = {googlebooks},
  publisher = {World Scientific},
  abstract = {This research monograph on circular data analysis covers some recent advances in the field, besides providing a brief introduction to, and a review of, existing methods and models. The primary focus is on recent research into topics such as change-point problems, predictive distributions, circular correlation and regression, etc. An important feature of this work is the S-plus subroutines provided for analyzing actual data sets. Coupled with the discussion of new theoretical research, the book should benefit both the researcher and the practitioner. Contents: Circular Probability Distributions; Some Sampling Distributions; Estimation of Parameters; Tests for Mean Direction and Concentration; Tests for Uniformity; Nonparametric Testing Procedures; Circular Correlation and Regression; Predictive Inference for Directional Data; Outliers and Related Problems; Change-Point Problems; Miscellaneous Topics; Some Facts on Bessel Functions; How to Use the CircStats Package. Readership: Researchers and practitioners dealing with circular data.},
  isbn = {978-981-277-926-7},
  langid = {english},
  pagetotal = {348},
  keywords = {Mathematics / Differential Equations / General,Mathematics / Probability & Statistics / General}
}

@online{jaschke2018,
  title = {Unsupervised {{Machine Learning}} on {{Encrypted Data}}},
  author = {Jäschke, Angela and Armknecht, Frederik},
  date = {2018},
  number = {2018/411},
  url = {https://eprint.iacr.org/2018/411},
  urldate = {2024-09-05},
  abstract = {In the context of Fully Homomorphic Encryption, which allows computations on encrypted data, Machine Learning has been one of the most popular applications in the recent past. All of these works, however, have focused on supervised learning, where there is a labeled training set that is used to configure the model. In this work, we take the first step into the realm of unsupervised learning, which is an important area in Machine Learning and has many real-world applications, by addressing the clustering problem. To this end, we show how to implement the K-Means-Algorithm. This algorithm poses several challenges in the FHE context, including a division, which we tackle by using a natural encoding that allows division and may be of independent interest. While this theoretically solves the problem, performance in practice is not optimal, so we then propose some changes to the clustering algorithm to make it executable under more conventional encodings. We show that our new algorithm achieves a clustering accuracy comparable to the original K-Means-Algorithm, but has less than 5\% of its runtime.},
  pubstate = {prepublished},
  keywords = {Clustering,Fully Homomorphic Encryption,Machine Learning},
  annotation = {Publication info: Preprint. MAJOR revision.},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/jaschke2018.pdf}
}

@inproceedings{jaschke2018a,
  title = {({{Finite}}) {{Field Work}}: {{Choosing}} the {{Best Encoding}} of {{Numbers}} for {{FHE Computation}}},
  shorttitle = {({{Finite}}) {{Field Work}}},
  booktitle = {Cryptology and {{Network Security}}},
  author = {Jäschke, Angela and Armknecht, Frederik},
  editor = {Capkun, Srdjan and Chow, Sherman S. M.},
  date = {2018},
  pages = {482--492},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-02641-7_23},
  abstract = {Fully Homomorphic Encryption (FHE) schemes operate over finite fields while many use cases call for real numbers, requiring appropriate encoding of the data into the scheme’s plaintext space. However, the choice of encoding can tremendously impact the computational effort on the encrypted data. In this work, we investigate this question for applications that operate over integers and rational numbers using p-adic encoding and the extensions p’s Complement and Sign-Magnitude, based on three natural metrics: the number of finite field additions, multiplications, and multiplicative depth. Our results are partly constructive and partly negative: For the first two metrics, an optimal choice exists and we state it explicitly. However, for multiplicative depth the optimum does not exist globally, but we do show how to choose this best encoding depending on the use-case.},
  isbn = {978-3-030-02641-7},
  langid = {english},
  keywords = {Efficiency,Encoding,Fully Homomorphic Encryption},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/Jäschke2018.pdf}
}

@article{jegou2011,
  title = {Product {{Quantization}} for {{Nearest Neighbor Search}}},
  author = {Jégou, Herve and Douze, Matthijs and Schmid, Cordelia},
  date = {2011-01},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {33},
  number = {1},
  pages = {117--128},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2010.57},
  url = {https://ieeexplore.ieee.org/document/5432202},
  urldate = {2023-11-13},
  abstract = {This paper introduces a product quantization-based approach for approximate nearest neighbor search. The idea is to decompose the space into a Cartesian product of low-dimensional subspaces and to quantize each subspace separately. A vector is represented by a short code composed of its subspace quantization indices. The euclidean distance between two vectors can be efficiently estimated from their codes. An asymmetric version increases precision, as it computes the approximate distance between a vector and a code. Experimental results show that our approach searches for nearest neighbors efficiently, in particular in combination with an inverted file system. Results for SIFT and GIST image descriptors show excellent search accuracy, outperforming three state-of-the-art approaches. The scalability of our approach is validated on a data set of two billion vectors.},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/jegou2011.pdf;/home/thomas/Zotero/storage/7T72YLS5/5432202.html}
}

@online{jin2024,
  title = {{{FedML-HE}}: {{An Efficient Homomorphic-Encryption-Based Privacy-Preserving Federated Learning System}}},
  shorttitle = {{{FedML-HE}}},
  author = {Jin, Weizhao and Yao, Yuhang and Han, Shanshan and Gu, Jiajun and Joe-Wong, Carlee and Ravi, Srivatsan and Avestimehr, Salman and He, Chaoyang},
  date = {2024-06-17},
  eprint = {2303.10837},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.10837},
  url = {http://arxiv.org/abs/2303.10837},
  urldate = {2024-08-30},
  abstract = {Federated Learning trains machine learning models on distributed devices by aggregating local model updates instead of local data. However, privacy concerns arise as the aggregated local models on the server may reveal sensitive personal information by inversion attacks. Privacy-preserving methods, such as homomorphic encryption (HE), then become necessary for FL training. Despite HE's privacy advantages, its applications suffer from impractical overheads, especially for foundation models. In this paper, we present FedML-HE, the first practical federated learning system with efficient HE-based secure model aggregation. FedML-HE proposes to selectively encrypt sensitive parameters, significantly reducing both computation and communication overheads during training while providing customizable privacy preservation. Our optimized system demonstrates considerable overhead reduction, particularly for large foundation models (e.g., \textasciitilde 10x reduction for ResNet-50, and up to \textasciitilde 40x reduction for BERT), demonstrating the potential for scalable HE-based FL deployment.},
  pubstate = {prepublished},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/jin2024.pdf;/home/thomas/Zotero/storage/PHDVEYAC/2303.html}
}

@book{judson2022,
  title = {Abstract {{Algebra}}: {{Theory}} and {{Applications}}},
  shorttitle = {Abstract {{Algebra}}},
  author = {Judson, Thomas},
  date = {2022-07-28},
  edition = {2022nd edition},
  publisher = {Orthogonal Publishing L3c},
  abstract = {Abstract Algebra: Theory and Applications is an open-source textbook that is designed to teach the principles and theory of abstract algebra to college juniors and seniors in a rigorous manner. Its strengths include a wide range of exercises, both computational and theoretical, plus many non-trivial applications. The first half of the book presents group theory, through the Sylow theorems, with enough material for a semester-long course. The second half is suitable for a second semester and presents rings, integral domains, Boolean algebras, vector spaces, and fields, concluding with Galois Theory.},
  isbn = {978-1-944325-16-9},
  langid = {english},
  pagetotal = {438}
}

@article{kalman1960,
  title = {A {{New Approach}} to {{Linear Filtering}} and {{Prediction Problems}}},
  author = {Kalman, R. E.},
  date = {1960-03-01},
  journaltitle = {Journal of Basic Engineering},
  doi = {10.1115/1.3662552},
  url = {https://www.scinapse.io/papers/2105934661},
  urldate = {2023-12-31},
  abstract = {R. E. Kalman},
  langid = {english}
}

@book{katz2020,
  title = {Introduction to {{Modern Cryptography}}},
  author = {Katz, Jonathan and Lindell, Yehuda},
  date = {2020-12-20},
  eprint = {zwIPEAAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {CRC Press},
  abstract = {Now the most used texbook for introductory cryptography courses in both mathematics and computer science, the Third Edition builds upon previous editions by offering several new sections, topics, and exercises. The authors present the core principles of modern cryptography, with emphasis on formal definitions, rigorous proofs of security.},
  isbn = {978-1-351-13302-9},
  langid = {english},
  pagetotal = {649},
  keywords = {Computers / Operating Systems / General,Computers / Security / Cryptography & Encryption,Computers / Security / General,Language Arts & Disciplines / Library & Information Science / General,Law / Computer & Internet,Mathematics / Applied,Mathematics / Combinatorics,Mathematics / Discrete Mathematics,Mathematics / General,Mathematics / Logic,Mathematics / Number Theory,Technology & Engineering / Automation,Technology & Engineering / Environmental / General}
}

@online{kim2021,
  title = {Revisiting {{Homomorphic Encryption Schemes}} for {{Finite Fields}}},
  author = {Kim, Andrey and Polyakov, Yuriy and Zucca, Vincent},
  date = {2021},
  number = {2021/204},
  url = {https://eprint.iacr.org/2021/204},
  urldate = {2024-11-12},
  abstract = {The Brakerski-Gentry-Vaikuntanathan (BGV) and Brakerski/ Fan-Vercauteren (BFV) schemes are the two main homomorphic encryption (HE) schemes to perform exact computations over finite fields and integers. Although the schemes work with the same plaintext space, there are significant differences in their noise management, algorithms for the core homomorphic multiplication operation, message encoding, and practical usability. The main goal of our work is to revisit both schemes, focusing on closing the gap between the schemes by improving their noise growth, computational complexity of the core algorithms, and usability. The other goal of our work is to provide both theoretical and experimental performance comparison of BGV and BFV. More precisely, we propose an improved variant of BFV where the encryption operation is modified to significantly reduce the noise growth, which makes the BFV noise growth somewhat better than for BGV (in contrast to prior results showing that BGV has smaller noise growth for larger plaintext moduli). We also modify the homomorphic multiplication procedure, which is the main bottleneck in BFV, to reduce its algorithmic complexity. Our work introduces several other novel optimizations, including lazy scaling in BFV homomorphic multiplication and an improved BFV decryption procedure in the Residue Number System (RNS) representation. We also develop a usable variant of BGV as a more efficient alternative to BFV for common practical scenarios. We implement our improved variants of BFV and BGV in PALISADE and evaluate their experimental performance for several benchmark computations. The experimental results suggest that our BGV implementation is faster for intermediate and large plaintext moduli, which are often used in practical scenarios with ciphertext packing, while our BFV implementation is faster for small plaintext moduli.},
  pubstate = {prepublished},
  keywords = {BFV,BGV,homomorphic encryption,software implementation},
  annotation = {Publication info: A major revision of an IACR publication in ASIACRYPT 2021},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/Kim2021.pdf}
}

@online{kim2024a,
  title = {Simpler and {{Faster BFV Bootstrapping}} for {{Arbitrary Plaintext Modulus}} from {{CKKS}}},
  author = {Kim, Jaehyung and Seo, Jinyeong and Song, Yongsoo},
  date = {2024},
  number = {2024/109},
  url = {https://eprint.iacr.org/2024/109},
  urldate = {2025-01-14},
  abstract = {Bootstrapping is currently the only known method for constructing fully homomorphic encryptions. In the BFV scheme specifically, bootstrapping aims to reduce the error of a ciphertext while preserving the encrypted plaintext. The existing BFV bootstrapping methods follow the same pipeline, relying on the evaluation of a digit extraction polynomial to annihilate the error located in the least significant digits. However, due to its strong dependence on performance, bootstrapping could only utilize a limited form of plaintext modulus, such as a power of a small prime number. In this paper, we present a novel approach to instantiate BFV bootstrapping, distinct from the previous digit extraction-based method.  The core idea of our bootstrapping is to utilize CKKS bootstrapping as a subroutine, so the performance of our method mainly depends on the underlying CKKS bootstrapping rather than the plaintext modulus. We implement our method at a proof-of-concept level to provide concrete benchmark results.  When performing the bootstrapping operation for a 51-bits plaintext modulus, our method improves the previous digit extraction-based method by a factor of 37.9 in latency and 29.4 in throughput.  Additionally, we achieve viable bootstrapping performance for large plaintext moduli, such as 144-bits and 234-bits, which has never been measured before.},
  pubstate = {prepublished},
  keywords = {BFV,Bootstrapping,Homomorphic Encryption},
  annotation = {Publication info: Published elsewhere. Minor revision. ACM CCS 2024},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/Kim2024.pdf}
}

@book{koblitz1994,
  title = {A {{Course}} in {{Number Theory}} and {{Cryptography}}},
  author = {Koblitz, Neal},
  date = {1994},
  series = {Graduate {{Texts}} in {{Mathematics}}},
  volume = {114},
  publisher = {Springer},
  location = {New York, NY},
  doi = {10.1007/978-1-4419-8592-7},
  url = {http://link.springer.com/10.1007/978-1-4419-8592-7},
  urldate = {2024-08-12},
  isbn = {978-1-4612-6442-2 978-1-4419-8592-7},
  keywords = {adopted textbook,continued fraction,cryptography,finite field,number theory,Prime},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/koblitz1994.pdf}
}

@article{kohonen1972,
  title = {Correlation {{Matrix Memories}}},
  author = {Kohonen, Teuvo},
  date = {1972-04},
  journaltitle = {IEEE Transactions on Computers},
  volume = {C-21},
  number = {4},
  pages = {353--359},
  issn = {1557-9956},
  doi = {10.1109/TC.1972.5008975},
  url = {https://ieeexplore.ieee.org/document/5008975},
  urldate = {2024-07-27},
  abstract = {A new model for associative memory, based on a correlation matrix, is suggested. In this model information is accumulated on memory elements as products of component data. Denoting a key vector by q(p), and the data associated with it by another vector x(p), the pairs (q(p), x(p)) are memorized in the form of a matrix see the Equation in PDF File where c is a constant. A randomly selected subset of the elements of Mxq can also be used for memorizing. The recalling of a particular datum x(r) is made by a transformation x(r)=Mxqq(r). This model is failure tolerant and facilitates associative search of information; these are properties that are usually assigned to holographic memories. Two classes of memories are discussed: a complete correlation matrix memory (CCMM), and randomly organized incomplete correlation matrix memories (ICMM). The data recalled from the latter are stochastic variables but the fidelity of recall is shown to have a deterministic limit if the number of memory elements grows without limits. A special case of correlation matrix memories is the auto-associative memory in which any part of the memorized information can be used as a key. The memories are selective with respect to accumulated data. The ICMM exhibits adaptive improvement under certain circumstances. It is also suggested that correlation matrix memories could be applied for the classification of data.},
  eventtitle = {{{IEEE Transactions}} on {{Computers}}},
  keywords = {Associative memory,associative net,associative recall,Correlation,correlation matrix memory,Data mining,Finite element methods,Manganese,Mathematical model,Noise,nonholographic associative memory,pattern recognition,Reactive power},
  file = {/home/thomas/Zotero/storage/VBH4KUVE/5008975.html}
}

@book{kreyszig1978,
  title = {Introductory Functional Analysis with Applications},
  author = {Kreyszig, Erwin},
  date = {1978},
  series = {Wiley Classics Library},
  edition = {15. print},
  publisher = {Wiley},
  location = {New York, N.Y.},
  abstract = {Provides avenues for applying functional analysis to the practical study of natural sciences as well as mathematics. Contains worked problems on Hilbert space theory and on Banach spaces and emphasizes concepts, principles, methods and major applications of functional analysis.},
  isbn = {978-0-471-50459-7},
  langid = {english},
  pagetotal = {688},
  keywords = {Funktionalanalysis},
  annotation = {OCLC: 474285534}
}

@book{kreyszig2007,
  title = {Introductory Functional Analysis with Applications},
  author = {Kreyszig},
  date = {2007},
  series = {Wiley Classics Library},
  publisher = {Wiley India Pvt. Limited},
  url = {https://books.google.de/books?id=osXw-pRsptoC},
  isbn = {978-81-265-1191-4}
}

@inproceedings{kurz2014,
  title = {Efficient Evaluation of the Probability Density Function of a Wrapped Normal Distribution},
  booktitle = {2014 {{Sensor Data Fusion}}: {{Trends}}, {{Solutions}}, {{Applications}} ({{SDF}})},
  author = {Kurz, Gerhard and Gilitschenski, Igor and Hanebeck, Uwe D.},
  date = {2014-10},
  pages = {1--5},
  doi = {10.1109/SDF.2014.6954713},
  url = {https://ieeexplore.ieee.org/document/6954713},
  urldate = {2024-06-22},
  abstract = {The wrapped normal distribution arises when the density of a one-dimensional normal distribution is wrapped around the circle infinitely many times. At first look, evaluation of its probability density function appears tedious as an infinite series is involved. In this paper, we investigate the evaluation of two truncated series representations. As one representation performs well for small uncertainties, whereas the other performs well for large uncertainties, we show that in all cases a small number of summands is sufficient to achieve high accuracy.},
  eventtitle = {2014 {{Sensor Data Fusion}}: {{Trends}}, {{Solutions}}, {{Applications}} ({{SDF}})},
  keywords = {Accuracy,Approximation methods,Artificial neural networks,Robots},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/kurz2014.pdf;/home/thomas/Zotero/storage/KCFI48QX/6954713.html}
}

@book{kwak2004,
  title = {Linear {{Algebra}}},
  author = {Kwak, Jin Ho and Hong, Sungpyo},
  date = {2004},
  publisher = {Birkhäuser},
  location = {Boston, MA},
  doi = {10.1007/978-0-8176-8194-4},
  url = {http://link.springer.com/10.1007/978-0-8176-8194-4},
  urldate = {2023-11-28},
  isbn = {978-0-8176-4294-5 978-0-8176-8194-4},
  langid = {english},
  keywords = {algebra,computer,computer science,Eigenvalue,Eigenvector,linear algebra,Matrix,matrix theory,Transformation},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/kwak2004.pdf}
}

@book{lang1999,
  title = {Complex {{Analysis}}},
  author = {Lang, Serge},
  date = {1999},
  series = {Graduate {{Texts}} in {{Mathematics}}},
  volume = {103},
  publisher = {Springer},
  location = {New York, NY},
  doi = {10.1007/978-1-4757-3083-8},
  url = {http://link.springer.com/10.1007/978-1-4757-3083-8},
  urldate = {2024-03-13},
  isbn = {978-1-4419-3135-1 978-1-4757-3083-8},
  keywords = {calculus,Cauchy's integral formula,Complex analysis,differential equation,gamma function,Jensen's formula,maximum,Meromorphic function},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/lang1999.pdf}
}

@book{lange2019,
  title = {Hebbian Learning Approaches Based on General Inner Products and Distance Measures in Non-{{Euclidean}} Spaces},
  author = {Lange, Mandy},
  date = {2019},
  publisher = {University of Groningen},
  location = {[Groningen]},
  abstract = {The topic of this thesis is to define a unified and generalized scheme for Hebbian approaches in non-Euclidean spaces for unsupervised and supervised learning. This can be realized in different ways. One possibility is the replacement of the inner product by a semi-inner product (SIP). A SIP relaxes the strict properties of an inner product but preserves the linear aspect in the first argument. Thus, these SIPs are natural equivalents of inner products generating Banach spaces instead of Hilbert spaces for inner products. In this work SIPs for Banach spaces are considered for unsupervised Hebbian like learning approaches. Further, the learning scheme of the supervised Learning Vector Quantization (LVQ) network, which is originally designed for applications in Euclidean data space, can be interpreted under specific circumstances as a Hebbian like learning, too. It is shown that, non-Euclidean metrics applied in LVQ can improve the performance of classification learning compared to Euclidean variants.The previously addressed Hebbian learning methods are vectorial approaches. However, if the data space is a vector space of matrices equipped with a respective matrix norm, then matrix approaches for Hebbian like learning methods become of interest. The extension of these methods in non-Euclidean spaces of matrices to process matrix data is the last main point of this thesis.},
  isbn = {978-94-034-1470-6},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/lange2019.pdf}
}

@online{lepoint2014a,
  title = {A {{Comparison}} of the {{Homomorphic Encryption Schemes FV}} and {{YASHE}}},
  author = {Lepoint, Tancrède and Naehrig, Michael},
  date = {2014},
  number = {2014/062},
  url = {https://eprint.iacr.org/2014/062},
  urldate = {2024-12-17},
  abstract = {We conduct a theoretical and practical comparison of two Ring-LWE-based, scale-invariant, leveled homomorphic encryption schemes – Fan and Vercauteren’s adaptation of BGV and the YASHE scheme proposed by Bos, Lauter, Loftus and Naehrig. In particular, we explain how to choose parameters to ensure correctness and security against lattice attacks. Our parameter selection improves the approach of van de Pol and Smart to choose parameters for schemes based on the Ring-LWE problem by using the BKZ-2.0 simulation algorithm. We implemented both encryption schemes in C++, using the arithmetic library FLINT, and compared them in practice to assess their respective strengths and weaknesses. In particular, we performed a homomorphic evaluation of the lightweight block cipher SIMON. Combining block ciphers with homomorphic encryption allows to solve the gargantuan ciphertext expansion in cloud applications.},
  pubstate = {prepublished},
  keywords = {BKZ,Implementations,Leveled Homomorphic Encryption,Parameter Selection,SIMON},
  annotation = {Publication info: Published elsewhere. Major revision. AFRICACRYPT 2014},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/Lepoint2014.pdf}
}

@inproceedings{li2010,
  title = {Research on {{Diffie-Hellman}} Key Exchange Protocol},
  booktitle = {2010 2nd {{International Conference}} on {{Computer Engineering}} and {{Technology}}},
  author = {Li, Nan},
  date = {2010-04},
  volume = {4},
  pages = {V4-634-V4-637},
  doi = {10.1109/ICCET.2010.5485276},
  url = {https://ieeexplore.ieee.org/document/5485276},
  urldate = {2024-08-24},
  abstract = {The purpose of the Diffie-Hellman protocol is to enable two users to exchange a secret key securely that can then be used for subsequent encryption of messages. The protocol itself is limited to exchange of the keys. But because of having no entity authentication mechanism, Diffie-Hellman protocol is easily attacked by the man-in-the-middle attack and impersonation attack in practice. In this paper, we compare the computational efficiency of various authentication methods. Finally an improved key exchange schema based on hash function is given, which improves the security and practicality of Diffie-Hellman protocol.},
  eventtitle = {2010 2nd {{International Conference}} on {{Computer Engineering}} and {{Technology}}},
  keywords = {Authentication,authentication mechanism,Communication channels,Computational efficiency,Cryptographic protocols,Cryptography,Diffie-Hellman protocol,Education,Equations,key exchange,Public key,Security,Transport protocols},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/li2010.pdf;/home/thomas/Zotero/storage/I8EKLK9A/5485276.html}
}

@online{lin2018,
  title = {Focal {{Loss}} for {{Dense Object Detection}}},
  author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr},
  date = {2018-02-07},
  eprint = {1708.02002},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1708.02002},
  url = {http://arxiv.org/abs/1708.02002},
  urldate = {2023-12-16},
  abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/lin2018.pdf;/home/thomas/Zotero/storage/4YJ9LBZT/1708.html}
}

@online{lingle2023,
  title = {Transformer-{{VQ}}: {{Linear-Time Transformers}} via {{Vector Quantization}}},
  shorttitle = {Transformer-{{VQ}}},
  author = {Lingle, Lucas D.},
  date = {2023-09-28},
  eprint = {2309.16354},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2309.16354},
  url = {http://arxiv.org/abs/2309.16354},
  urldate = {2023-11-13},
  abstract = {We introduce Transformer-VQ, a decoder-only transformer computing softmax-based dense self-attention in linear time. Transformer-VQ's efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In large-scale experiments, Transformer-VQ is shown highly competitive in quality, with strong results on Enwik8 (0.99 bpb), PG-19 (26.6 ppl), and ImageNet64 (3.16 bpb). Code: https://github.com/transformer-vq/transformer\_vq},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/lingle2023.pdf;/home/thomas/Zotero/storage/AMZDGPVR/2309.html}
}

@online{liu2024,
  title = {{{LongVQ}}: {{Long Sequence Modeling}} with {{Vector Quantization}} on {{Structured Memory}}},
  shorttitle = {{{LongVQ}}},
  author = {Liu, Zicheng and Wang, Li and Li, Siyuan and Wang, Zedong and Lin, Haitao and Li, Stan Z.},
  date = {2024-04-18},
  eprint = {2404.11163},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2404.11163},
  url = {http://arxiv.org/abs/2404.11163},
  urldate = {2024-04-21},
  abstract = {Transformer models have been successful in various sequence processing tasks, but the self-attention mechanism's computational cost limits its practicality for long sequences. Although there are existing attention variants that improve computational efficiency, they have a limited ability to abstract global information effectively based on their hand-crafted mixing strategies. On the other hand, state-space models (SSMs) are tailored for long sequences but cannot capture complicated local information. Therefore, the combination of them as a unified token mixer is a trend in recent long-sequence models. However, the linearized attention degrades performance significantly even when equipped with SSMs. To address the issue, we propose a new method called LongVQ. LongVQ uses the vector quantization (VQ) technique to compress the global abstraction as a length-fixed codebook, enabling the linear-time computation of the attention matrix. This technique effectively maintains dynamic global and local patterns, which helps to complement the lack of long-range dependency issues. Our experiments on the Long Range Arena benchmark, autoregressive language modeling, and image and speech classification demonstrate the effectiveness of LongVQ. Our model achieves significant improvements over other sequence models, including variants of Transformers, Convolutions, and recent State Space Models.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/liu2024.pdf;/home/thomas/Zotero/storage/GL4588R4/2404.html}
}

@online{logsdon2023,
  title = {Fully {{Homomorphic Encryption}}: {{A Mathematical Introduction}}},
  shorttitle = {Fully {{Homomorphic Encryption}}},
  author = {Logsdon, Sara},
  date = {2023},
  number = {2023/1402},
  url = {https://eprint.iacr.org/2023/1402},
  urldate = {2024-10-04},
  abstract = {This paper offers a mathematical introduction to fully homomorphic encryption, a concept that enables computation on encrypted data. We trace the historical development of FHE, describe Fully Homomorphic Encryption over the Torus (TFHE) and how it performs certain mathematical operations, and explore bootstrapping and the possibility for adjusting computational depth. This paper equips readers with a brief understanding of FHE's evolution and the essential mechanisms facilitating practical implementation.},
  pubstate = {prepublished},
  keywords = {Bootstrapping,Computational Depth,Fully Homomorphic Encryption,TFHE},
  annotation = {Publication info: Preprint.},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/Logsdon2023.pdf}
}

@article{lumer1961,
  title = {Semi-Inner-Product Spaces},
  author = {Lumer, G.},
  date = {1961},
  journaltitle = {Trans. Amer. Math. Soc.},
  volume = {100},
  number = {1},
  pages = {29--43},
  issn = {0002-9947, 1088-6850},
  doi = {10.1090/S0002-9947-1961-0133024-2},
  url = {https://www.ams.org/tran/1961-100-01/S0002-9947-1961-0133024-2/},
  urldate = {2023-11-15},
  abstract = {Advancing research. Creating connections.},
  langid = {english},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/lumer1961.pdf}
}

@online{marcolla2022,
  title = {Survey on {{Fully Homomorphic Encryption}}, {{Theory}}, and {{Applications}}},
  author = {Marcolla, Chiara and Sucasas, Victor and Manzano, Marc and Bassoli, Riccardo and Fitzek, Frank H. P. and Aaraj, Najwa},
  date = {2022},
  number = {2022/1602},
  url = {https://eprint.iacr.org/2022/1602},
  urldate = {2024-08-30},
  abstract = {Data privacy concerns are increasing significantly in the context of Internet of Things, cloud services, edge computing, artificial intelligence applications, and other applications enabled by next generation networks. Homomorphic Encryption addresses privacy challenges by enabling multiple operations to be performed on encrypted messages without decryption. This paper comprehensively addresses homomorphic encryption from both theoretical and practical perspectives. The paper delves into the mathematical foundations required to understand fully homomorphic encryption (FHE). It consequently covers design fundamentals and security properties of FHE and describes the main FHE schemes based on various mathematical problems. On a more practical level, the paper presents a view on privacy-preserving Machine Learning using homomorphic encryption, then surveys FHE at length from an engineering angle, covering the potential application of FHE in fog computing, and cloud computing services. It also provides a comprehensive analysis of existing state-of-the-art FHE libraries and tools, implemented in software and hardware, and the performance thereof.},
  pubstate = {prepublished},
  keywords = {Cloud Computing,Fog Computing,Fully Homomorphic Encryption,Homomorphic Encryption,IoT,Lattices,Neural Networks},
  annotation = {Publication info: Published elsewhere. Proceedings of the IEEE},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/marcolla2022.pdf}
}

@article{marcolla2022a,
  title = {Survey on {{Fully Homomorphic Encryption}}, {{Theory}}, and {{Applications}}},
  author = {Marcolla, Chiara and Sucasas, Victor and Manzano, Marc and Bassoli, Riccardo and Fitzek, Frank H. P. and Aaraj, Najwa},
  date = {2022-10},
  journaltitle = {Proceedings of the IEEE},
  volume = {110},
  number = {10},
  pages = {1572--1609},
  issn = {1558-2256},
  doi = {10.1109/JPROC.2022.3205665},
  url = {https://ieeexplore.ieee.org/document/9910347/?arnumber=9910347},
  urldate = {2025-01-15},
  abstract = {Data privacy concerns are increasing significantly in the context of the Internet of Things, cloud services, edge computing, artificial intelligence applications, and other applications enabled by next-generation networks. Homomorphic encryption addresses privacy challenges by enabling multiple operations to be performed on encrypted messages without decryption. This article comprehensively addresses homomorphic encryption from both theoretical and practical perspectives. This article delves into the mathematical foundations required to understand fully homomorphic encryption ( \textbackslash textsf FHE ). It consequently covers design fundamentals and security properties of \textbackslash textsf FHE and describes the main \textbackslash textsf FHE schemes based on various mathematical problems. On a more practical level, this article presents a view on privacy-preserving machine learning using homomorphic encryption and then surveys \textbackslash textsf FHE at length from an engineering angle, covering the potential application of \textbackslash textsf FHE in fog computing and cloud computing services. It also provides a comprehensive analysis of existing state-of-the-art \textbackslash textsf FHE libraries and tools, implemented in software and hardware, and the performance thereof.},
  eventtitle = {Proceedings of the {{IEEE}}},
  keywords = {Cloud computing,fog computing,fully homomorphic encryption (FHE),Gaussian distribution,homomorphic encryption,Homomorphic encryption,Internet of Things,Internet of Things (IoT),lattices,neural networks,Neural networks,Privacy,Public key},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/Marcolla2022.pdf;/home/thomas/Zotero/storage/UVQBDMJJ/9910347.html}
}

@book{mcandrew2016,
  title = {Introduction to {{Cryptography}} with {{Open-Source Software}}},
  author = {McAndrew, Alasdair},
  date = {2016-04-19},
  eprint = {9lTRBQAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {CRC Press},
  abstract = {Once the privilege of a secret few, cryptography is now taught at universities around the world. Introduction to Cryptography with Open-Source Software illustrates algorithms and cryptosystems using examples and the open-source computer algebra system of Sage. The author, a noted educator in the field, provides a highly practical learning experienc},
  isbn = {978-1-4398-2571-6},
  langid = {english},
  pagetotal = {456},
  keywords = {Computers / Computer Science,Computers / Programming / Games,Computers / Security / General,Law / Computer & Internet,Mathematics / Combinatorics,Mathematics / Discrete Mathematics,Mathematics / Logic}
}

@book{mckenzie2011,
  title = {Algebras, {{Lattices}}, {{Varieties}}},
  author = {McKenzie, Ralph N. and McNulty, George F. and Taylor, Walter F.},
  date = {2011}
}

@online{mcmahan2023,
  title = {Communication-{{Efficient Learning}} of {{Deep Networks}} from {{Decentralized Data}}},
  author = {McMahan, H. Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Agüera},
  date = {2023-01-26},
  eprint = {1602.05629},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1602.05629},
  url = {http://arxiv.org/abs/1602.05629},
  urldate = {2024-09-04},
  abstract = {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/mcmahan2023.pdf;/home/thomas/Zotero/storage/MQNHWPVV/1602.html}
}

@article{micciancio,
  title = {Lattice-Based {{Cryptography}}},
  author = {Micciancio, Daniele and Regev, Oded},
  langid = {english},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/Micciancio.pdf}
}

@online{millar2011,
  title = {Maximum {{Likelihood Estimation}} and {{Inference}}: {{With Examples}} in {{R}}, {{SAS}} and {{ADMB}} | {{Wiley}}},
  shorttitle = {Maximum {{Likelihood Estimation}} and {{Inference}}},
  author = {{millar}},
  date = {2011},
  url = {https://www.wiley.com/en-us/Maximum+Likelihood+Estimation+and+Inference%3A+With+Examples+in+R%2C+SAS+and+ADMB-p-9780470094822},
  urldate = {2023-12-23},
  abstract = {This book takes a fresh look at the popular and well-established method of maximum likelihood for statistical estimation and inference. It begins with an intuitive introduction to the concepts and background of likelihood, and moves through to the latest developments in maximum likelihood methodology, including general latent variable models and new material for the practical implementation of integrated likelihood using the free ADMB software. Fundamental issues of statistical inference are also examined, with a presentation of some of the philosophical debates underlying the choice of statistical paradigm. Key features: Provides an accessible introduction to pragmatic maximum likelihood modelling. Covers more advanced topics, including general forms of latent variable models (including non-linear and non-normal mixed-effects and state-space models) and the use of maximum likelihood variants, such as estimating equations, conditional likelihood, restricted likelihood and integrated likelihood. Adopts a practical approach, with a focus on providing the relevant tools required by researchers and practitioners who collect and analyze real data. Presents numerous examples and case studies across a wide range of applications including medicine, biology and ecology. Features applications from a range of disciplines, with implementation in R, SAS and/or ADMB. Provides all program code and software extensions on a supporting website. Confines supporting theory to the final chapters to maintain a readable and pragmatic focus of the preceding chapters. This book is not just an accessible and practical text about maximum likelihood, it is a comprehensive guide to modern maximum likelihood estimation and inference. It will be of interest to readers of all levels, from novice to expert. It will be of great benefit to researchers, and to students of statistics from senior undergraduate to graduate level. For use as a course text, exercises are provided at the end of each chapter.},
  langid = {american},
  organization = {Wiley.com},
  file = {/home/thomas/Zotero/storage/ASYFM2KI/Maximum+Likelihood+Estimation+and+Inference+With+Examples+in+R,+SAS+and+ADMB-p-9780470094822.html}
}

@article{nagy,
  title = {Ordinary {{Diﬀerential Equations}}},
  author = {Nagy, Gabriel},
  langid = {english},
  file = {/home/thomas/Zotero/storage/DMSY57RB/Nagy - Ordinary Diﬀerential Equations.pdf}
}

@inproceedings{nam2014,
  title = {Large-{{Scale Multi-label Text Classification}} — {{Revisiting Neural Networks}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Nam, Jinseok and Kim, Jungi and Loza Mencía, Eneldo and Gurevych, Iryna and Fürnkranz, Johannes},
  editor = {Calders, Toon and Esposito, Floriana and Hüllermeier, Eyke and Meo, Rosa},
  date = {2014},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {437--452},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-662-44851-9_28},
  abstract = {Neural networks have recently been proposed for multi-label classification because they are able to capture and model label dependencies in the output layer. In this work, we investigate limitations of BP-MLL, a neural network (NN) architecture that aims at minimizing pairwise ranking error. Instead, we propose to use a comparably simple NN approach with recently proposed learning techniques for large-scale multi-label text classification tasks. In particular, we show that BP-MLL’s ranking loss minimization can be efficiently and effectively replaced with the commonly used cross entropy error function, and demonstrate that several advances in neural network training that have been developed in the realm of deep learning can be effectively employed in this setting. Our experimental results show that simple NN models equipped with advanced techniques such as rectified linear units, dropout, and AdaGrad perform as well as or even outperform state-of-the-art approaches on six large-scale textual datasets with diverse characteristics.},
  isbn = {978-3-662-44851-9},
  langid = {english},
  keywords = {Cross Entropy,Hide Layer,Hide Unit,Single Hide Layer,Stochastic Gradient Descent},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/nam2014.pdf}
}

@book{needham1999,
  title = {Visual {{Complex Analysis}}},
  author = {Needham, Tristan},
  date = {1999-02-18},
  edition = {Reprint edition},
  publisher = {Oxford University Press, USA},
  location = {Oxford},
  abstract = {This radical first course on complex analysis brings a beautiful and powerful subject to life by consistently using geometry (not calculation) as the means of explanation. Aimed at undergraduate students in mathematics, physics, and engineering, the book's intuitive explanations, lack of advanced prerequisites, and consciously user-friendly prose style will help students to master the subject more readily than was previously possible. The key to this is the book's use of new geometric arguments in place of the standard calculational ones. These geometric arguments are communicated with the aid of hundreds of diagrams of a standard seldom encountered in mathematical works. A new approach to a classical topic, this work will be of interest to students in mathematics, physics, and engineering, as well as to professionals in these fields.},
  isbn = {978-0-19-853446-4},
  langid = {english},
  pagetotal = {616}
}

@article{nelder1972,
  title = {Generalized {{Linear Models}}},
  author = {Nelder, J. A. and Wedderburn, R. W. M.},
  date = {1972},
  journaltitle = {Journal of the Royal Statistical Society. Series A (General)},
  volume = {135},
  number = {3},
  eprint = {2344614},
  eprinttype = {jstor},
  pages = {370--384},
  publisher = {[Royal Statistical Society, Wiley]},
  issn = {0035-9238},
  doi = {10.2307/2344614},
  url = {https://www.jstor.org/stable/2344614},
  urldate = {2023-12-23},
  abstract = {The technique of iterative weighted linear regression can be used to obtain maximum likelihood estimates of the parameters with observations distributed according to some exponential family and systematic effects that can be made linear by a suitable transformation. A generalization of the analysis of variance is given for these models using log-likelihoods. These generalized linear models are illustrated by examples relating to four distributions; the Normal, Binomial (probit analysis, etc.), Poisson (contingency tables) and gamma (variance components). The implications of the approach in designing statistics courses are discussed.}
}

@book{neukirch1999,
  title = {Algebraic {{Number Theory}}},
  author = {Neukirch, Jürgen},
  editor = {Chern, S. S. and Eckmann, B. and De La Harpe, P. and Hironaka, H. and Hirzebruch, F. and Hitchin, N. and Hörmander, L. and Knus, M.-A. and Kupiainen, A. and Lannes, J. and Lebeau, G. and Ratner, M. and Serre, D. and Sinai, Ya. G. and Sloane, N. J. A. and Tits, J. and Waldschmidt, M. and Watanabe, S. and Berger, M. and Coates, J. and Varadhan, S. R. S.},
  editortype = {redactor},
  date = {1999},
  series = {Grundlehren Der Mathematischen {{Wissenschaften}}},
  volume = {322},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-662-03983-0},
  url = {http://link.springer.com/10.1007/978-3-662-03983-0},
  urldate = {2025-01-13},
  isbn = {978-3-642-08473-7 978-3-662-03983-0},
  keywords = {algebra,Algebraic Number Theory,Arithmetic Algebraic Geometry,File available at Lightning Source,number theory},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/Neukirch1999.pdf}
}

@book{nita2023,
  title = {Advances to {{Homomorphic}} and {{Searchable Encryption}}},
  author = {Nita, Stefania Loredana and Mihailescu, Marius Iulian},
  date = {2023},
  publisher = {Springer Nature Switzerland},
  location = {Cham},
  doi = {10.1007/978-3-031-43214-9},
  url = {https://link.springer.com/10.1007/978-3-031-43214-9},
  urldate = {2024-05-21},
  isbn = {978-3-031-43213-2 978-3-031-43214-9},
  langid = {english},
  keywords = {big data security,cloud computing security,data science,homomorphic encryption,information security,lattice-based cryptography,multivariate cryptography,quantum cryptography,searchable encryption}
}

@book{olson2017,
  title = {Applied {{Fourier Analysis}}},
  author = {Olson, Tim},
  date = {2017},
  publisher = {Springer},
  location = {New York, NY},
  doi = {10.1007/978-1-4939-7393-4},
  url = {http://link.springer.com/10.1007/978-1-4939-7393-4},
  urldate = {2024-02-18},
  isbn = {978-1-4939-7391-0 978-1-4939-7393-4},
  langid = {english},
  keywords = {analysis,communications,Fourier applications,medical imaging,partial differential equations,sampling},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/olson2017.pdf}
}

@article{peikert,
  title = {A {{Decade}} of {{Lattice Cryptography}}},
  author = {Peikert, Chris},
  langid = {english},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/Peikert.pdf}
}

@online{peikert2015,
  title = {A {{Decade}} of {{Lattice Cryptography}}},
  author = {Peikert, Chris},
  date = {2015},
  number = {2015/939},
  url = {https://eprint.iacr.org/2015/939},
  urldate = {2024-06-11},
  abstract = {\textbackslash emph\{Lattice-based cryptography\} is the use of conjectured hard problems on point lattices in\textasciitilde\$\textbackslash R\textasciicircum\{n\}\$ as the foundation for secure cryptographic systems. Attractive features of lattice cryptography include apparent resistance to \textbackslash emph\{quantum\} attacks (in contrast with most number-theoretic cryptography), high asymptotic efficiency and parallelism, security under \textbackslash emph\{worst-case\} intractability assumptions, and solutions to long-standing open problems in cryptography. This work surveys most of the major developments in lattice cryptography over the past ten years. The main focus is on the foundational \textbackslash emph\{short integer solution\}\textasciitilde (SIS) and \textbackslash emph\{learning with errors\}\textasciitilde (LWE) problems (and their more efficient ring-based variants), their provable hardness assuming the worst-case intractability of standard lattice problems, and their many cryptographic applications.},
  pubstate = {prepublished},
  keywords = {lattices,learning with errors,short integer solution,survey},
  annotation = {Publication info: Preprint. MINOR revision.},
  file = {/home/thomas/Zotero/storage/DNHAENMJ/peikert2015.pdf}
}

@article{perrin,
  title = {A {{Summary}} of the {{FV Homomorphic Encryption Scheme}} and the {{Average-Case Noise Growth}}},
  author = {Perrin, Derek},
  langid = {english},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/Perrin.pdf}
}

@article{prasad2012,
  title = {A {{Study}} on {{Associative Neural Memories}}},
  author = {Prasad, B. D. C. N. and Prasad, P. E. S. N. Krishna and Yeruva, Sagar and Murty, P. Sita Rama},
  date = {2012-02-01},
  journaltitle = {International Journal of Advanced Computer Science and Applications (IJACSA)},
  volume = {1},
  number = {6},
  publisher = {{The Science and Information (SAI) Organization Limited}},
  issn = {2156-5570},
  doi = {10.14569/IJACSA.2010.010619},
  url = {https://thesai.org/Publications/ViewPaper?Volume=1&Issue=6&Code=IJACSA&SerialNo=19},
  urldate = {2024-07-27},
  abstract = {Memory plays a major role in Artificial Neural Networks. Without memory, Neural Network can not be learned itself. One of the primary concepts of memory in neural networks is Associative neural memories. A survey has been made on associative neural memories such as Simple associative memories (SAM), Dynamic associative memories (DAM), Bidirectional Associative memories (BAM), Hopfield memories, Context Sensitive Auto-associative memories (CSAM) and so on. These memories can be applied in various fields to get the effective outcomes. We present a study on these associative memories in artificial neural networks.},
  issue = {6},
  langid = {english},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/prasad2012.pdf}
}

@report{rabin1979,
  type = {Technical Report},
  title = {{{DIGITALIZED SIGNATURES AND PUBLIC-KEY FUNCTIONS AS INTRACTABLE AS FACTORIZATION}}},
  author = {Rabin, M. O.},
  date = {1979},
  institution = {Massachusetts Institute of Technology},
  location = {USA},
  abstract = {We introduce a new class of public-key functions involving a number n = pq having two large prime factors. As usual, the key n is public, while p and q are the private key used by the issuer for production of signatures and function inversion. These functions can be used for all the applications involving public-key functions proposed by Diffie and Hellman, including digitalized signatures. We prove that for any given n, if we can invert the function y = E (x1) for even a small percentage of the values y then we can factor n. Thus, as long as factorization of large numbers remains practically intractable, for appropriate chosen keys not even a small percentage of signatures are forgeable. Breaking the RSA function is at most hard as factorization, but is not known to be equivalent to factorization even in the weak sense that ability to invert all function values entails ability to factor the key. Computation time for these functions, i.e. signature verification, is several hundred times faster than for the RSA scheme. Inversion time, using the private key, is comparable. The almost-everywhere intractability of signature-forgery for our functions (on the assumption that factoring is intractable) is of great practical significance and seems to be the first proved result of this kind.}
}

@inproceedings{rabin1979a,
  title = {{{DIGITALIZED SIGNATURES AND PUBLIC-KEY FUNCTIONS AS INTRACTABLE AS FACTORIZATION}}},
  author = {Rabin, M.},
  date = {1979},
  url = {https://www.semanticscholar.org/paper/DIGITALIZED-SIGNATURES-AND-PUBLIC-KEY-FUNCTIONS-AS-Rabin/468600c74bf5e30f206c997c0f9f09561ceae7d2},
  urldate = {2024-08-08},
  abstract = {We introduce a new class of public-key functions involving a number n = pq having two large prime factors. As usual, the key n is public, while p and q are the private key used by the issuer for production of signatures and function inversion. These functions can be used for all the applications involving public-key functions proposed by Diffie and Hellman, including digitalized signatures. We prove that for any given n, if we can invert the function y = E (x1) for even a small percentage of the values y then we can factor n. Thus, as long as factorization of large numbers remains practically intractable, for appropriate chosen keys not even a small percentage of signatures are forgeable. Breaking the RSA function is at most hard as factorization, but is not known to be equivalent to factorization even in the weak sense that ability to invert all function values entails ability to factor the key. Computation time for these functions, i.e. signature verification, is several hundred times faster than for the RSA scheme. Inversion time, using the private key, is comparable. The almost-everywhere intractability of signature-forgery for our functions (on the assumption that factoring is intractable) is of great practical significance and seems to be the first proved result of this kind.}
}

@article{ramos2018,
  title = {Deconstructing {{Cross-Entropy}} for {{Probabilistic Binary Classifiers}}},
  author = {Ramos, Daniel and Franco-Pedroso, Javier and Lozano-Diez, Alicia and Gonzalez-Rodriguez, Joaquin},
  date = {2018-03},
  journaltitle = {Entropy},
  volume = {20},
  number = {3},
  pages = {208},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1099-4300},
  doi = {10.3390/e20030208},
  url = {https://www.mdpi.com/1099-4300/20/3/208},
  urldate = {2023-12-15},
  abstract = {In this work, we analyze the cross-entropy function, widely used in classifiers both as a performance measure and as an optimization objective. We contextualize cross-entropy in the light of Bayesian decision theory, the formal probabilistic framework for making decisions, and we thoroughly analyze its motivation, meaning and interpretation from an information-theoretical point of view. In this sense, this article presents several contributions: First, we explicitly analyze the contribution to cross-entropy of (i) prior knowledge; and (ii) the value of the features in the form of a likelihood ratio. Second, we introduce a decomposition of cross-entropy into two components: discrimination and calibration. This decomposition enables the measurement of different performance aspects of a classifier in a more precise way; and justifies previously reported strategies to obtain reliable probabilities by means of the calibration of the output of a discriminating classifier. Third, we give different information-theoretical interpretations of cross-entropy, which can be useful in different application scenarios, and which are related to the concept of reference probabilities. Fourth, we present an analysis tool, the Empirical Cross-Entropy (ECE) plot, a compact representation of cross-entropy and its aforementioned decomposition. We show the power of ECE plots, as compared to other classical performance representations, in two diverse experimental examples: a speaker verification system, and a forensic case where some glass findings are present.},
  issue = {3},
  langid = {english},
  keywords = {Bayesian,calibration,classifier,cross-entropy,discrimination,ECE plot,probabilistic},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/ramos2018.pdf}
}

@online{reddi2018,
  title = {Stochastic {{Negative Mining}} for {{Learning}} with {{Large Output Spaces}}},
  author = {Reddi, Sashank J. and Kale, Satyen and Yu, Felix and Holtmann-Rice, Dan and Chen, Jiecao and Kumar, Sanjiv},
  date = {2018-10-16},
  eprint = {1810.07076},
  eprinttype = {arXiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1810.07076},
  url = {http://arxiv.org/abs/1810.07076},
  urldate = {2023-11-13},
  abstract = {We consider the problem of retrieving the most relevant labels for a given input when the size of the output space is very large. Retrieval methods are modeled as set-valued classifiers which output a small set of classes for each input, and a mistake is made if the label is not in the output set. Despite its practical importance, a statistically principled, yet practical solution to this problem is largely missing. To this end, we first define a family of surrogate losses and show that they are calibrated and convex under certain conditions on the loss parameters and data distribution, thereby establishing a statistical and analytical basis for using these losses. Furthermore, we identify a particularly intuitive class of loss functions in the aforementioned family and show that they are amenable to practical implementation in the large output space setting (i.e. computation is possible without evaluating scores of all labels) by developing a technique called Stochastic Negative Mining. We also provide generalization error bounds for the losses in the family. Finally, we conduct experiments which demonstrate that Stochastic Negative Mining yields benefits over commonly used negative sampling approaches.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/reddi2018.pdf;/home/thomas/Zotero/storage/W6JVNWF3/1810.html}
}

@online{regev2003,
  title = {Quantum {{Computation}} and {{Lattice Problems}}},
  author = {Regev, Oded},
  date = {2003-04-01},
  eprint = {cs/0304005},
  eprinttype = {arXiv},
  doi = {10.48550/arXiv.cs/0304005},
  url = {http://arxiv.org/abs/cs/0304005},
  urldate = {2025-01-03},
  abstract = {We present the first explicit connection between quantum computation and lattice problems. Namely, we show a solution to the Unique Shortest Vector Problem (SVP) under the assumption that there exists an algorithm that solves the hidden subgroup problem on the dihedral group by coset sampling. Moreover, we solve the hidden subgroup problem on the dihedral group by using an average case subset sum routine. By combining the two results, we get a quantum reduction from \$\textbackslash Theta(n\textasciicircum\{2.5\})\$-unique-SVP to the average case subset sum problem.},
  pubstate = {prepublished},
  keywords = {Computer Science - Data Structures and Algorithms},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/Regev2003.pdf;/home/thomas/Zotero/storage/EULZFPBE/0304005.html}
}

@article{regev2009,
  title = {On Lattices, Learning with Errors, Random Linear Codes, and Cryptography},
  author = {Regev, Oded},
  date = {2009-09-08},
  journaltitle = {J. ACM},
  volume = {56},
  number = {6},
  pages = {34:1--34:40},
  issn = {0004-5411},
  doi = {10.1145/1568318.1568324},
  url = {https://dl.acm.org/doi/10.1145/1568318.1568324},
  urldate = {2025-01-03},
  abstract = {Our main result is a reduction from worst-case lattice problems such as GapSVP and SIVP to a certain learning problem. This learning problem is a natural extension of the “learning from parity with error” problem to higher moduli. It can also be viewed as the problem of decoding from a random linear code. This, we believe, gives a strong indication that these problems are hard. Our reduction, however, is quantum. Hence, an efficient solution to the learning problem implies a quantum algorithm for GapSVP and SIVP. A main open question is whether this reduction can be made classical (i.e., nonquantum).We also present a (classical) public-key cryptosystem whose security is based on the hardness of the learning problem. By the main result, its security is also based on the worst-case quantum hardness of GapSVP and SIVP. The new cryptosystem is much more efficient than previous lattice-based cryptosystems: the public key is of size Õ(n2) and encrypting a message increases its size by a factor of Õ(n) (in previous cryptosystems these values are Õ(n4) and Õ(n2), respectively). In fact, under the assumption that all parties share a random bit string of length Õ(n2), the size of the public key can be reduced to Õ(n).}
}

@online{regev2024,
  title = {On {{Lattices}}, {{Learning}} with {{Errors}}, {{Random Linear Codes}}, and {{Cryptography}}},
  author = {Regev, Oded},
  date = {2024-01-08},
  eprint = {2401.03703},
  eprinttype = {arXiv},
  eprintclass = {quant-ph},
  doi = {10.48550/arXiv.2401.03703},
  url = {http://arxiv.org/abs/2401.03703},
  urldate = {2024-06-17},
  abstract = {Our main result is a reduction from worst-case lattice problems such as GapSVP and SIVP to a certain learning problem. This learning problem is a natural extension of the `learning from parity with error' problem to higher moduli. It can also be viewed as the problem of decoding from a random linear code. This, we believe, gives a strong indication that these problems are hard. Our reduction, however, is quantum. Hence, an efficient solution to the learning problem implies a quantum algorithm for GapSVP and SIVP. A main open question is whether this reduction can be made classical (i.e., non-quantum). We also present a (classical) public-key cryptosystem whose security is based on the hardness of the learning problem. By the main result, its security is also based on the worst-case quantum hardness of GapSVP and SIVP. The new cryptosystem is much more efficient than previous lattice-based cryptosystems: the public key is of size \$\textbackslash tilde\{O\}(n\textasciicircum 2)\$ and encrypting a message increases its size by a factor of \$\textbackslash tilde\{O\}(n)\$ (in previous cryptosystems these values are \$\textbackslash tilde\{O\}(n\textasciicircum 4)\$ and \$\textbackslash tilde\{O\}(n\textasciicircum 2)\$, respectively). In fact, under the assumption that all parties share a random bit string of length \$\textbackslash tilde\{O\}(n\textasciicircum 2)\$, the size of the public key can be reduced to \$\textbackslash tilde\{O\}(n)\$.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computational Complexity,Computer Science - Cryptography and Security,Quantum Physics},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/regev2024.pdf;/home/thomas/Zotero/storage/J2S97PSF/2401.html}
}

@inproceedings{rivest1978,
  title = {{{ON DATA BANKS AND PRIVACY HOMOMORPHISMS}}},
  author = {Rivest, Ronald L. and Dertouzos, M.},
  date = {1978},
  url = {https://www.semanticscholar.org/paper/ON-DATA-BANKS-AND-PRIVACY-HOMOMORPHISMS-Rivest-Dertouzos/c365f01d330b2211e74069120e88cff37eacbcf5},
  urldate = {2024-05-21},
  abstract = {Encryption is a well—known technique for preserving the privacy of sensitive information. One of the basic, apparently inherent, limitations of this technique is that an information system working with encrypted data can at most store or retrieve the data for the user; any more complicated operations seem to require that the data be decrypted before being operated on. This limitation follows from the choice of encryption functions used, however, and although there are some truly inherent limitations on what can be accomplished, we shall see that it appears likely that there exist encryption functions which permit encrypted data to be operated on without preliminary decryption of the operands, for many sets of interesting operations. These special encryption functions we call “privacy homomorphisms”; they form an interesting subset of arbitrary encryption schemes (called “privacy transformations”).}
}

@article{rivest1978a,
  title = {A Method for Obtaining Digital Signatures and Public-Key Cryptosystems},
  author = {Rivest, R. L. and Shamir, A. and Adleman, L.},
  date = {1978-02-01},
  journaltitle = {Commun. ACM},
  volume = {21},
  number = {2},
  pages = {120--126},
  issn = {0001-0782},
  doi = {10.1145/359340.359342},
  url = {https://dl.acm.org/doi/10.1145/359340.359342},
  urldate = {2024-08-08},
  abstract = {An encryption method is presented with the novel property that publicly revealing an encryption key does not thereby reveal the corresponding decryption key. This has two important consequences: (1) Couriers or other secure means are not needed to transmit keys, since a message can be enciphered using an encryption key publicly revealed by the intented recipient. Only he can decipher the message, since only he knows the corresponding decryption key. (2) A message can be “signed” using a privately held decryption key. Anyone can verify this signature using the corresponding publicly revealed encryption key. Signatures cannot be forged, and a signer cannot later deny the validity of his signature. This has obvious applications in “electronic mail” and “electronic funds transfer” systems. A message is encrypted by representing it as a number M, raising M to a publicly specified power e, and then taking the remainder when the result is divided by the publicly specified product, n, of two large secret primer numbers p and q. Decryption is similar; only a different, secret, power d is used, where e * d ≡ 1(mod (p - 1) * (q - 1)). The security of the system rests in part on the difficulty of factoring the published divisor, n.},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/rivest1978a.pdf}
}

@article{rosca,
  title = {On Algebraic Variants of {{Learning With Errors}}},
  author = {Rosca, Georgiana-Miruna},
  langid = {english},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/Rosca.pdf}
}

@book{rudin1976,
  title = {Principles of {{Mathematical Analysis}}},
  author = {Rudin, Walter},
  date = {1976},
  eprint = {kwqzPAAACAAJ},
  eprinttype = {googlebooks},
  publisher = {McGraw-Hill},
  abstract = {The third edition of this well known text continues to provide a solid foundation in mathematical analysis for undergraduate and first-year graduate students. The text begins with a discussion of the real number system as a complete ordered field. (Dedekind's construction is now treated in an appendix to Chapter I.) The topological background needed for the development of convergence, continuity, differentiation and integration is provided in Chapter 2. There is a new section on the gamma function, and many new and interesting exercises are included. This text is part of the Walter Rudin Student Series in Advanced Mathematics.},
  isbn = {978-0-07-085613-4},
  langid = {english},
  pagetotal = {342},
  keywords = {Mathematics / Calculus}
}

@article{sabani2024,
  title = {Learning with {{Errors}}: {{A Lattice-Based Keystone}} of {{Post-Quantum Cryptography}}},
  author = {Sabani, Maria E. and Savvas, Illias K. and Garani, Georgia},
  date = {2024-04-13},
  doi = {10.3390/signals5020012},
  url = {https://www.mdpi.com/2624-6120/5/2/12},
  abstract = {Abstract The swift advancement of quantum computing devices holds the potential to create robust machines that can tackle an extensive array of issues beyond the scope of conventional computers. Consequently, quantum computing machines create new risks at a velocity and scale never seen before, especially with regard to encryption. Lattice-based cryptography is regarded as post-quantum cryptography’s future and a competitor to a quantum computer attack. Thus, there are several advantages to lattice-based cryptographic protocols, including security, effectiveness, reduced energy usage and speed. In this work, we study the learning with errors (LWE) problem and the cryptosystems that are based on the LWE problem and, in addition, we present a new efficient variant of LWE cryptographic scheme.},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/sabani2024.pdf}
}

@incollection{salgado2022,
  title = {Trigonometric {{Interpolation}} and the {{Fast Fourier Transform}}},
  booktitle = {Classical {{Numerical Analysis}}: {{A Comprehensive Course}}},
  editor = {Salgado, Abner J. and Wise, Steven M.},
  date = {2022},
  pages = {345--371},
  publisher = {Cambridge University Press},
  location = {Cambridge},
  doi = {10.1017/9781108942607.017},
  url = {https://www.cambridge.org/core/books/classical-numerical-analysis/trigonometric-interpolation-and-the-fast-fourier-transform/670835DFB4586EEB2B4360F8D7C8B2B5},
  urldate = {2024-09-17},
  abstract = {This chapter begins with the study of trigonometric interpolation and the discrete Fourier transform. As a first application, the numerical integration of periodic functions is discussed. More detailed topics, like existence and uniqueness of trigonometric interpolants, as well as alias as convergence of trigonometric interpolation are discussed. An important practical tool, the fast Fourier transform (FFT) is then introduced. With this at hand the most rudimentary ideas of signal processing are presented.},
  isbn = {978-1-108-83770-5},
  keywords = {Fourier matrices,intergration of periodic functions,the finite Fourier transform,the FTT,Trigonometric interpolation,trigonometric least squares approximation},
  file = {/home/thomas/Zotero/storage/EZ79H2WB/670835DFB4586EEB2B4360F8D7C8B2B5.html}
}

@book{samuel1970,
  title = {Algebraic {{Theory}} of {{Numbers}}},
  author = {Samuel, Pierre},
  date = {1970},
  eprint = {uQzvAAAAMAAJ},
  eprinttype = {googlebooks},
  publisher = {Hermann},
  isbn = {978-0-901665-06-5},
  langid = {english},
  pagetotal = {118}
}

@inproceedings{sato1995,
  title = {Generalized {{Learning Vector Quantization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Sato, Atsushi and Yamada, Keiji},
  date = {1995},
  volume = {8},
  publisher = {MIT Press},
  url = {https://papers.nips.cc/paper_files/paper/1995/hash/9c3b1830513cc3b8fc4b76635d32e692-Abstract.html},
  urldate = {2024-05-12},
  abstract = {We  propose  a  new  learning  method,  "Generalized  Learning  Vec(cid:173) tor Quantization (GLVQ),"  in which reference vectors are updated  based on the steepest descent method in order to minimize the cost  function .  The  cost  function  is  determined  so  that  the  obtained  learning  rule  satisfies  the  convergence  condition.  We  prove  that  Kohonen's  rule  as  used  in  LVQ  does  not  satisfy  the  convergence  condition  and  thus  degrades  recognition  ability.  Experimental re(cid:173) sults  for  printed  Chinese  character recognition  reveal  that  GLVQ  is  superior to  LVQ  in  recognition  ability.},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/sato1995.pdf}
}

@book{schneier1996,
  title = {Applied {{Cryptography}}: {{Protocols}}, {{Algorithms}}, and {{Source Code}} in {{C}}},
  shorttitle = {Applied {{Cryptography}}},
  author = {Schneier, Bruce},
  date = {1996},
  eprint = {6NdQAAAAMAAJ},
  eprinttype = {googlebooks},
  publisher = {Wiley},
  abstract = {". . .the best introduction to cryptography I've ever seen. . . .The book the National Security Agency wanted never to be published. . . ." -Wired Magazine  ". . .monumental . . . fascinating . . . comprehensive . . . the definitive work on cryptography for computer programmers . . ." -Dr. Dobb's Journal  ". . .easily ranks as one of the most authoritative in its field." -PC Magazine  ". . .the bible of code hackers." -The Millennium Whole Earth Catalog  This new edition of the cryptography classic provides you with a comprehensive survey of modern cryptography. The book details how programmers and electronic communications professionals can use cryptography-the technique of enciphering and deciphering messages-to maintain the privacy of computer data. It describes dozens of cryptography algorithms, gives practical advice on how to implement them into cryptographic software, and shows how they can be used to solve security problems. Covering the latest developments in practical cryptographic techniques, this new edition shows programmers who design computer applications, networks, and storage systems how they can build security into their software and systems.  What's new in the Second Edition? * New information on the Clipper Chip, including ways to defeat the key escrow mechanism * New encryption algorithms, including algorithms from the former Soviet Union and South Africa, and the RC4 stream cipher * The latest protocols for digital signatures, authentication, secure elections, digital cash, and more * More detailed information on key management and cryptographic implementations},
  isbn = {978-0-471-11709-4},
  langid = {english},
  pagetotal = {796},
  keywords = {Computers / Computer Science,Computers / Information Technology,Computers / Security / Cryptography & Encryption,Computers / Security / General,Language Arts & Disciplines / Communication Studies,Mathematics / Discrete Mathematics,Technology & Engineering / Telecommunications}
}

@book{shao2003,
  title = {Mathematical {{Statistics}}},
  author = {Shao, Jun},
  editor = {Casella, G. and Fienberg, S. and Olkin, I.},
  editortype = {redactor},
  date = {2003},
  series = {Springer {{Texts}} in {{Statistics}}},
  publisher = {Springer},
  location = {New York, NY},
  doi = {10.1007/b97553},
  url = {http://link.springer.com/10.1007/b97553},
  urldate = {2023-12-26},
  isbn = {978-0-387-95382-3 978-0-387-21718-5},
  keywords = {likelihood,Markov chain,Mathematica,mathematical statistics,Mathematical Statistics,probability,probability theory,statistical theory,Statistical Theory,statistics},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/shao2003.pdf}
}

@book{shifrin2005,
  title = {Multivariable Mathematics: {{Linear}} Algebra, Multivariable Calculus, and Manifolds},
  author = {Shifrin, Theodore},
  date = {2005},
  publisher = {John Wiley \& Sons, Inc.},
  isbn = {978-0-471-52638-4 0-471-52638-X}
}

@book{silverman2008,
  title = {An {{Introduction}} to {{Mathematical Cryptography}}},
  author = {Silverman, J.H. and Pipher, Jill and Hoffstein, Jeffrey},
  date = {2008},
  series = {Undergraduate {{Texts}} in {{Mathematics}}},
  publisher = {Springer},
  location = {New York, NY},
  doi = {10.1007/978-0-387-77993-5},
  url = {https://link.springer.com/10.1007/978-0-387-77993-5},
  urldate = {2024-09-04},
  isbn = {978-0-387-77993-5 978-0-387-77994-2},
  langid = {english},
  keywords = {AES,algorithms,Crib,cryptography,cryptology,DES,information,information and communication circuits,information theory,Number theory},
  file = {/home/thomas/Zotero/storage/A6K5I76B/2008 - An Introduction to Mathematical Cryptography.pdf}
}

@book{silverman2009,
  title = {The {{Arithmetic}} of {{Elliptic Curves}}},
  author = {Silverman, Joseph H.},
  date = {2009},
  series = {Graduate {{Texts}} in {{Mathematics}}},
  volume = {106},
  publisher = {Springer},
  location = {New York, NY},
  doi = {10.1007/978-0-387-09494-6},
  url = {http://link.springer.com/10.1007/978-0-387-09494-6},
  urldate = {2024-09-07},
  isbn = {978-0-387-09493-9 978-0-387-09494-6},
  keywords = {algebra,Algebraic,Cohomology,cryptography,finite field,Geometry,Mordell-Weil,Number theory},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/silverman2009.pdf}
}

@book{silverman2015,
  title = {Rational {{Points}} on {{Elliptic Curves}}},
  author = {Silverman, Joseph H. and Tate, John T.},
  date = {2015},
  series = {Undergraduate {{Texts}} in {{Mathematics}}},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-18588-0},
  url = {https://link.springer.com/10.1007/978-3-319-18588-0},
  urldate = {2024-09-06},
  isbn = {978-3-319-18587-3 978-3-319-18588-0},
  langid = {english},
  keywords = {ABC conjecture,complex multiplication,elliptic curve cryptography,elliptic curves,Fermat's last theorem,Frey curves,rational points},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/silverman2015.pdf}
}

@book{stallings1999,
  title = {Cryptography and {{Network Security}}: {{Principles}} and {{Practice}}},
  shorttitle = {Cryptography and {{Network Security}}},
  author = {Stallings, William},
  date = {1999},
  eprint = {Dam9zrViJjEC},
  eprinttype = {googlebooks},
  publisher = {Prentice Hall},
  abstract = {As we enter the age of universal electronic connectivity in which viruses, hackers, electronic eavesdropping, and electronic fraud can threaten the prosperity and productivity of corporations and individuals, security is increasingly important. Fortunately, the disciplines of cryptography and network security have matured, leading to the development of practical, available applications to enforce network security.  Best-selling author and two-time winner of the TEXTY award for the best computer science and engineering text, William Stallings provides a practical survey of both the principles and practice of cryptography and network security.  Extensively reorganized to provide the optimal sequence for classroom instruction and self-study, the second edition includes these key features. Looks at system-level security issues, including the threat of and countermeasures for intruders and viruses, and the use of firewalls and trusted systems. NEW - Discussion of block cipher design principles, plus coverage of Blowfish, CAST-128, Triple DES, and other algorithms NEW - Chapters on IP security and Web security Expanded coverage of public-key encryption algorithms and design principles, including RSA and elliptic curve cryptography Covers important network security tools and applications, including Kerberos, X.509v3, PGP, S/MIME, IP security, SSL/TLS, and SET On-line transparency masters, an Internet mailing list, and links to relevant web sites are available to http: //www.shore.net/\textasciitilde ws/Security2e.html},
  isbn = {978-0-13-869017-5},
  langid = {english},
  pagetotal = {600},
  keywords = {Computers / Database Administration & Management,Computers / Information Theory,Computers / Networking / General,Computers / Security / Cryptography & Encryption,Computers / Security / General,Computers / Security / Network Security}
}

@book{stanley2012,
  title = {Enumerative {{Combinatorics}}: {{Volume}} 1},
  shorttitle = {Enumerative {{Combinatorics}}},
  author = {Stanley, Richard P.},
  date = {2012-02-23},
  edition = {2. edition},
  publisher = {Cambridge University Press},
  location = {Cambridge, NY},
  abstract = {Richard Stanley's two-volume basic introduction to enumerative combinatorics has become the standard guide to the topic for students and experts alike. This thoroughly revised second edition of Volume 1 includes ten new sections and more than 300 new exercises, most with solutions, reflecting numerous new developments since the publication of the first edition in 1986. The material in Volume 1 was chosen to cover those parts of enumerative combinatorics of greatest applicability and with the most important connections with other areas of mathematics. The four chapters are devoted to an introduction to enumeration (suitable for advanced undergraduates), sieve methods, partially ordered sets, and rational generating functions. Much of the material is related to generating functions, a fundamental tool in enumerative combinatorics. In this new edition, the author brings the coverage up to date and includes a wide variety of additional applications and examples, as well as updated and expanded chapter bibliographies. Many of the less difficult new exercises have no solutions so that they can more easily be assigned to students. The material on P-partitions has been rearranged and generalized; the treatment of permutation statistics has been greatly enlarged; and there are also new sections on q-analogues of permutations, hyperplane arrangements, the cd-index, promotion and evacuation, and differential posets.},
  isbn = {978-1-107-01542-5},
  langid = {english},
  pagetotal = {632}
}

@book{vasudeva2017,
  title = {Elements of {{Hilbert Spaces}} and {{Operator Theory}}},
  author = {Vasudeva, Harkrishan Lal},
  date = {2017},
  publisher = {Springer},
  location = {Singapore},
  doi = {10.1007/978-981-10-3020-8},
  url = {http://link.springer.com/10.1007/978-981-10-3020-8},
  urldate = {2023-11-14},
  isbn = {978-981-10-3019-2 978-981-10-3020-8},
  langid = {english},
  keywords = {Banach Spaces,Finite Dimensional Spaces,Functional analysis,Linear operators,Operator theory,Riesz Lemma,Special theory},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/vasudeva2017.pdf}
}

@online{vaswani2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2023-08-01},
  eprint = {1706.03762},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2023-11-17},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/vaswani2023.pdf;/home/thomas/Zotero/storage/SCJWFGAA/1706.html}
}

@online{wang2022,
  title = {On the {{Unreasonable Effectiveness}} of {{Federated Averaging}} with {{Heterogeneous Data}}},
  author = {Wang, Jianyu and Das, Rudrajit and Joshi, Gauri and Kale, Satyen and Xu, Zheng and Zhang, Tong},
  date = {2022-06-09},
  eprint = {2206.04723},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2206.04723},
  url = {http://arxiv.org/abs/2206.04723},
  urldate = {2024-09-04},
  abstract = {Existing theory predicts that data heterogeneity will degrade the performance of the Federated Averaging (FedAvg) algorithm in federated learning. However, in practice, the simple FedAvg algorithm converges very well. This paper explains the seemingly unreasonable effectiveness of FedAvg that contradicts the previous theoretical predictions. We find that the key assumption of bounded gradient dissimilarity in previous theoretical analyses is too pessimistic to characterize data heterogeneity in practical applications. For a simple quadratic problem, we demonstrate there exist regimes where large gradient dissimilarity does not have any negative impact on the convergence of FedAvg. Motivated by this observation, we propose a new quantity, average drift at optimum, to measure the effects of data heterogeneity, and explicitly use it to present a new theoretical analysis of FedAvg. We show that the average drift at optimum is nearly zero across many real-world federated training tasks, whereas the gradient dissimilarity can be large. And our new analysis suggests FedAvg can have identical convergence rates in homogeneous and heterogeneous data settings, and hence, leads to better understanding of its empirical success.},
  pubstate = {prepublished},
  keywords = {Computer Science - Machine Learning},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/wang2022.pdf;/home/thomas/Zotero/storage/33E57UFA/2206.html}
}

@book{washington2008,
  title = {Elliptic {{Curves}}: {{Number Theory}} and {{Cryptography}}, {{Second Edition}}},
  shorttitle = {Elliptic {{Curves}}},
  author = {Washington, Lawrence C.},
  date = {2008-04-03},
  eprint = {nBfCEqpYKW0C},
  eprinttype = {googlebooks},
  publisher = {CRC Press},
  abstract = {Like its bestselling predecessor, Elliptic Curves: Number Theory and Cryptography, Second Edition develops the theory of elliptic curves to provide a basis for both number theoretic and cryptographic applications. With additional exercises, this edition offers more comprehensive coverage of the fundamental theory, techniques, and application},
  isbn = {978-1-4200-7147-4},
  langid = {english},
  pagetotal = {533},
  keywords = {Computers / Security / General,Mathematics / Combinatorics,Mathematics / Discrete Mathematics,Mathematics / General,Mathematics / Logic,Mathematics / Number Theory}
}

@online{xie2014,
  title = {Crypto-{{Nets}}: {{Neural Networks}} over {{Encrypted Data}}},
  shorttitle = {Crypto-{{Nets}}},
  author = {Xie, Pengtao and Bilenko, Misha and Finley, Tom and Gilad-Bachrach, Ran and Lauter, Kristin and Naehrig, Michael},
  date = {2014-12-24},
  eprint = {1412.6181},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1412.6181},
  url = {http://arxiv.org/abs/1412.6181},
  urldate = {2024-05-21},
  abstract = {The problem we address is the following: how can a user employ a predictive model that is held by a third party, without compromising private information. For example, a hospital may wish to use a cloud service to predict the readmission risk of a patient. However, due to regulations, the patient's medical files cannot be revealed. The goal is to make an inference using the model, without jeopardizing the accuracy of the prediction or the privacy of the data. To achieve high accuracy, we use neural networks, which have been shown to outperform other learning models for many tasks. To achieve the privacy requirements, we use homomorphic encryption in the following protocol: the data owner encrypts the data and sends the ciphertexts to the third party to obtain a prediction from a trained model. The model operates on these ciphertexts and sends back the encrypted prediction. In this protocol, not only the data remains private, even the values predicted are available only to the data owner. Using homomorphic encryption and modifications to the activation functions and training algorithms of neural networks, we show that it is protocol is possible and may be feasible. This method paves the way to build a secure cloud-based neural network prediction services without invading users' privacy.},
  pubstate = {prepublished},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/xie2014.pdf;/home/thomas/Zotero/storage/NE4WJDV2/1412.html}
}

@book{yi2014,
  title = {Homomorphic {{Encryption}} and {{Applications}}},
  author = {Yi, Xun and Paulet, Russell and Bertino, Elisa},
  date = {2014},
  series = {{{SpringerBriefs}} in {{Computer Science}}},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-12229-8},
  url = {https://link.springer.com/10.1007/978-3-319-12229-8},
  urldate = {2024-05-21},
  isbn = {978-3-319-12228-1 978-3-319-12229-8},
  langid = {english},
  keywords = {Critical Infrastructure Outsourcing,Cryptographic Assumption,Encrypted Email,Homomorphic Cryptography,Homomorphic Encryption,Homomorphic Encryption Application,Homomorphic Spam Filtering,Lattice Based Cryptography,Privacy Preserving Data Mining,Privacy Preserving Email},
  file = {/home/thomas/Documents/Obsidian Vault/PDFs/yi2014.pdf}
}

@book{young1988,
  title = {An {{Introduction}} to {{Hilbert Space}}},
  author = {Young, N.},
  date = {1988},
  publisher = {Cambridge University Press},
  location = {Cambridge},
  doi = {10.1017/CBO9781139172011},
  url = {https://www.cambridge.org/core/books/an-introduction-to-hilbert-space/41D80B163B8E186820B5A48F3665EC22},
  urldate = {2023-11-15},
  abstract = {This textbook is an introduction to the theory of Hilbert space and its applications. The notion of Hilbert space is central in functional analysis and is used in numerous branches of pure and applied mathematics. Dr Young has stressed applications of the theory, particularly to the solution of partial differential equations in mathematical physics and to the approximation of functions in complex analysis. Some basic familiarity with real analysis, linear algebra and metric spaces is assumed, but otherwise the book is self-contained. It is based on courses given at the University of Glasgow and contains numerous examples and exercises (many with solutions). Thus it will make an excellent first course in Hilbert space theory at either undergraduate or graduate level and will also be of interest to electrical engineers and physicists, particularly those involved in control theory and filter design.},
  isbn = {978-0-521-33717-5},
  file = {/home/thomas/Zotero/storage/Q72XTLTI/41D80B163B8E186820B5A48F3665EC22.html}
}

@book{zabczyk2008,
  title = {Mathematical {{Control Theory}}},
  author = {Zabczyk, Jerzy},
  date = {2008},
  publisher = {Birkhäuser},
  location = {Boston, MA},
  doi = {10.1007/978-0-8176-4733-9},
  url = {http://link.springer.com/10.1007/978-0-8176-4733-9},
  urldate = {2024-01-01},
  isbn = {978-0-8176-4732-2 978-0-8176-4733-9},
  langid = {english},
  keywords = {control,control system,control theory,dynamic programming,infinite dimensional linear systems,linear systems,mathematical control theory,nonlinear control,nonlinear system,observability,optimal control,programming,stability,stabilization,sys},
  file = {/home/thomas/Documents/Obsidian Vault/Sources/zabczyk2008.pdf}
}

@online{zotero-182,
  title = {{{OpenStax}} | {{Calculus Volume}} 2},
  url = {https://openstax.org/},
  abstract = {OpenStax offers free college textbooks for all types of students, making education accessible \& affordable for everyone. Browse our list of available subjects!},
  langid = {american},
  file = {/home/thomas/Zotero/storage/8HXCFBAT/calculus-volume-2.html}
}

@online{zotero-211,
  title = {Amazon.de:{{Customer Reviews}}: {{Fourier Analysis}} and {{Its Applications}} ({{Pure}} and {{Applied Undergraduate Texts}}, 4, {{Band}} 4)},
  url = {https://www.amazon.de/-/en/Gerald-B-Folland/product-reviews/0821847902/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews},
  urldate = {2024-02-26},
  file = {/home/thomas/Zotero/storage/6UVGAD4G/ref=cm_cr_dp_d_show_all_btm.html}
}

@online{zotero-256,
  title = {Applied {{Cryptography}}: {{Protocols}}, {{Algorithms}} and {{Source Code}} in {{C}}, 20th {{Anniversary Edition}} | {{Wiley}}},
  shorttitle = {Applied {{Cryptography}}},
  url = {https://www.wiley.com/en-au/Applied+Cryptography%3A+Protocols%2C+Algorithms+and+Source+Code+in+C%2C+20th+Anniversary+Edition-p-9781119096726},
  urldate = {2024-06-08},
  abstract = {Generated by create next app},
  langid = {english},
  organization = {Wiley.com},
  file = {/home/thomas/Zotero/storage/ZVVS37YF/Applied+Cryptography+Protocols,+Algorithms+and+Source+Code+in+C,+20th+Anniversary+Edition-p-978.html}
}

@online{zotero-258,
  title = {Applied {{Cryptography}}: {{Protocols}}, {{Algorithms}}, and {{Source Code}} in {{C}} : {{Schneier}}, {{Bruce}}: {{Amazon}}.de: {{Books}}},
  url = {https://www.amazon.de/Applied-Cryptography-Protocols-Algorithms-Source/dp/0471117099},
  urldate = {2024-06-08},
  file = {/home/thomas/Zotero/storage/64CQI3CL/0471117099.html}
}

@online{zotero-272,
  title = {Signals | {{Free Full-Text}} | {{Learning}} with {{Errors}}: {{A Lattice-Based Keystone}} of {{Post-Quantum Cryptography}}},
  url = {https://www.mdpi.com/2624-6120/5/2/12},
  urldate = {2024-06-17},
  file = {/home/thomas/Zotero/storage/5HJPH4KI/12.html}
}

@online{zotero-282,
  title = {Topics in {{Circular Statistics}} | {{Series}} on {{Multivariate Analysis}}},
  url = {https://www.worldscientific.com/worldscibooks/10.1142/4031},
  urldate = {2024-06-22},
  abstract = {This research monograph on circular data analysis covers some recent advances in the field, besides providing a brief introduction to, and a review of, existing methods and models. The primary focu...},
  langid = {english},
  file = {/home/thomas/Zotero/storage/PZCI3EKY/4031.html}
}

@online{zotero-316,
  title = {Digitalized {{Signatures}} and {{Public-Key Functions}} as {{Intractable}} as {{Factorization}},},
  url = {https://apps.dtic.mil/sti/citations/ADA078415},
  urldate = {2024-08-08},
  file = {/home/thomas/Zotero/storage/2PWTU4R5/ADA078415.html}
}

@online{zotero-327,
  title = {Introduction to {{Algorithms}}, Third Edition ({{Mit Press}}) : {{Cormen}}, {{Thomas H}}., {{Leiserson}}, {{Charles E}}., {{Rivest}}, {{Ronald L}}., {{Stein}}, {{Clifford}}: {{Amazon}}.de: {{Books}}},
  url = {https://www.amazon.de/Introduction-Algorithms-Press-Thomas-Cormen/dp/0262033844},
  urldate = {2024-08-12},
  file = {/home/thomas/Zotero/storage/MBNQADPU/0262033844.html}
}

@online{zotero-421,
  title = {[{{PDF}}] {{Faster}} Homomorphic Comparison Operations for {{BGV}} and {{BFV}} | {{Semantic Scholar}}},
  url = {https://www.semanticscholar.org/paper/Faster-homomorphic-comparison-operations-for-BGV-Iliashenko-Zucca/868407cd7c2491c596c416b3c287eb19596d1035},
  urldate = {2024-10-02},
  file = {/home/thomas/Zotero/storage/YYKZ5ZV9/868407cd7c2491c596c416b3c287eb19596d1035.html}
}

@online{zotero-537,
  title = {Digit Extraction Bootstrapping - {{Google Suche}}},
  url = {https://www.google.com/search?q=digit+extraction+bootstrapping&sourceid=chrome&ie=UTF-8},
  urldate = {2025-01-14},
  file = {/home/thomas/Zotero/storage/MQPB7MDU/search.html}
}

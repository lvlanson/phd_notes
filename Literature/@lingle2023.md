---
title: Transformer-VQ Linear-Time Transformers via Vector Quantization
authors: Lucas D. Lingle
year: 2023
DOI: 10.48550/arXiv.2309.16354
aliases:
  - Transformer VQ Paper
  - Transformer Vector Quantization Paper
  - Linear Time Transformer Vector Quantization Paper
---

>[!Links]-
>URL: http://arxiv.org/abs/2309.16354
>PDF: [PDF](lingle2023.pdf)
>Zotero: [Zotero-Link](zotero://select/items/@lingle2023)

>[!ABSTRACT]-
>We introduce Transformer-VQ, a decoder-only transformer computing softmax-based dense self-attention in linear time. Transformer-VQ's efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In large-scale experiments, Transformer-VQ is shown highly competitive in quality, with strong results on Enwik8 (0.99 bpb), PG-19 (26.6 ppl), and ImageNet64 (3.16 bpb). Code: https://github.com/transformer-vq/transformer_vq

